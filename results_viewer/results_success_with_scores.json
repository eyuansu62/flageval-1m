{
  "completed": 182,
  "datasets_config": [
    "mmlu",
    "mmlu_pro",
    "math_500",
    "gsm8k",
    "gpqa_diamond"
  ],
  "failed": 18,
  "hostname": "baai-sailing-3",
  "python_version": "3.11.14",
  "results": {
    "01-ai/Yi-1.5-34B-Chat": {
      "model_id": "01-ai/Yi-1.5-34B-Chat",
      "model_name": "01-ai/Yi-1.5-34B-Chat",
      "size_bytes": 34388917248,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T11:20:53.411312",
      "end_time": "2026-01-31T11:57:09.630552",
      "duration_seconds": 2176.22,
      "vllm_pid": 3259151,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5074,
          "overall": 0.5074
        },
        "mmlu_pro": {
          "score": 0.2786,
          "overall": 0.2786
        },
        "math_500": {
          "score": 0.5704,
          "overall": 0.5704
        },
        "gsm8k": {
          "score": 0.85,
          "overall": 0.85
        },
        "gpqa_diamond": {
          "score": 0.18,
          "overall": 0.18
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "01-ai/Yi-34B-Chat": {
      "model_id": "01-ai/Yi-34B-Chat",
      "model_name": "01-ai/Yi-34B-Chat",
      "size_bytes": 34388917248,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T07:22:14.254952",
      "end_time": "2026-01-31T07:43:05.562994",
      "duration_seconds": 1251.31,
      "vllm_pid": 848107,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5582,
          "overall": 0.5582
        },
        "mmlu_pro": {
          "score": 0.3343,
          "overall": 0.3343
        },
        "math_500": {
          "score": 0.224,
          "overall": 0.224
        },
        "gsm8k": {
          "score": 0.53,
          "overall": 0.53
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AI-MO/NuminaMath-7B-TIR": {
      "model_id": "AI-MO/NuminaMath-7B-TIR",
      "model_name": "AI-MO/NuminaMath-7B-TIR",
      "size_bytes": 6910365696,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T07:00:06.689335",
      "end_time": "2026-01-31T07:46:27.531989",
      "duration_seconds": 2780.84,
      "vllm_pid": 2764750,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.38,
          "overall": 0.38
        },
        "mmlu_pro": {
          "score": 0.2329,
          "overall": 0.2329
        },
        "math_500": {
          "score": 0.4041,
          "overall": 0.4041
        },
        "gsm8k": {
          "score": 0.43,
          "overall": 0.43
        },
        "gpqa_diamond": {
          "score": 0.21,
          "overall": 0.21
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AIDC-AI/Marco-o1": {
      "model_id": "AIDC-AI/Marco-o1",
      "model_name": "AIDC-AI/Marco-o1",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:41:42.797743",
      "end_time": "2026-01-31T03:19:50.304560",
      "duration_seconds": 2287.51,
      "vllm_pid": 2263689,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6565,
          "overall": 0.6565
        },
        "mmlu_pro": {
          "score": 0.4993,
          "overall": 0.4993
        },
        "math_500": {
          "score": 0.6951,
          "overall": 0.6951
        },
        "gsm8k": {
          "score": 0.91,
          "overall": 0.91
        },
        "gpqa_diamond": {
          "score": 0.38,
          "overall": 0.38
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Alibaba-NLP/Tongyi-DeepResearch-30B-A3B": {
      "model_id": "Alibaba-NLP/Tongyi-DeepResearch-30B-A3B",
      "model_name": "Alibaba-NLP/Tongyi-DeepResearch-30B-A3B",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T23:10:35.631771",
      "end_time": "2026-01-31T00:35:43.467666",
      "duration_seconds": 5107.84,
      "vllm_pid": 1868311,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6865,
          "overall": 0.6865
        },
        "mmlu_pro": {
          "score": 0.3471,
          "overall": 0.3471
        },
        "math_500": {
          "score": 0.5658,
          "overall": 0.5658
        },
        "gsm8k": {
          "score": 0.9,
          "overall": 0.9
        },
        "gpqa_diamond": {
          "score": 0.11,
          "overall": 0.11
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BioMistral/BioMistral-7B": {
      "model_id": "BioMistral/BioMistral-7B",
      "model_name": "BioMistral/BioMistral-7B",
      "size_bytes": 14486259862,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-29T13:39:27.812128",
      "end_time": "2026-01-29T13:50:23.186439",
      "duration_seconds": 655.37,
      "vllm_pid": 2251585,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.1744,
          "overall": 0.1744
        },
        "mmlu_pro": {
          "score": 0.1107,
          "overall": 0.1107
        },
        "math_500": {
          "score": 0.0508,
          "overall": 0.0508
        },
        "gsm8k": {
          "score": 0.09,
          "overall": 0.09
        },
        "gpqa_diamond": {
          "score": 0.13,
          "overall": 0.13
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/Seed-OSS-36B-Instruct": {
      "model_id": "ByteDance-Seed/Seed-OSS-36B-Instruct",
      "model_name": "ByteDance-Seed/Seed-OSS-36B-Instruct",
      "size_bytes": 36151104512,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T04:54:29.486574",
      "end_time": "2026-01-31T06:47:19.817516",
      "duration_seconds": 6770.33,
      "vllm_pid": 2525622,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7707,
          "overall": 0.7707
        },
        "mmlu_pro": {
          "score": 0.4864,
          "overall": 0.4864
        },
        "math_500": {
          "score": 0.515,
          "overall": 0.515
        },
        "gsm8k": {
          "score": 0.94,
          "overall": 0.94
        },
        "gpqa_diamond": {
          "score": 0.11,
          "overall": 0.11
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/aya-23-8B": {
      "model_id": "CohereLabs/aya-23-8B",
      "model_name": "CohereLabs/aya-23-8B",
      "size_bytes": 8028033024,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T05:55:56.770081",
      "end_time": "2026-01-31T06:02:03.311105",
      "duration_seconds": 366.54,
      "vllm_pid": 2640149,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.456,
          "overall": 0.456
        },
        "mmlu_pro": {
          "score": 0.2479,
          "overall": 0.2479
        },
        "math_500": {
          "score": 0.0739,
          "overall": 0.0739
        },
        "gsm8k": {
          "score": 0.48,
          "overall": 0.48
        },
        "gpqa_diamond": {
          "score": 0.29,
          "overall": 0.29
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FractalAIResearch/Fathom-R1-14B": {
      "model_id": "FractalAIResearch/Fathom-R1-14B",
      "model_name": "FractalAIResearch/Fathom-R1-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T11:12:53.412198",
      "end_time": "2026-01-31T13:44:21.983000",
      "duration_seconds": 9088.57,
      "vllm_pid": 3241128,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7081,
          "overall": 0.7081
        },
        "mmlu_pro": {
          "score": 0.4121,
          "overall": 0.4121
        },
        "math_500": {
          "score": 0.6143,
          "overall": 0.6143
        },
        "gsm8k": {
          "score": 0.93,
          "overall": 0.93
        },
        "gpqa_diamond": {
          "score": 0.11,
          "overall": 0.11
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Groq/Llama-3-Groq-8B-Tool-Use": {
      "model_id": "Groq/Llama-3-Groq-8B-Tool-Use",
      "model_name": "Groq/Llama-3-Groq-8B-Tool-Use",
      "size_bytes": 8030310400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T10:41:17.200956",
      "end_time": "2026-01-31T10:49:54.915821",
      "duration_seconds": 517.71,
      "vllm_pid": 3179697,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.1867,
          "overall": 0.1867
        },
        "mmlu_pro": {
          "score": 0.125,
          "overall": 0.125
        },
        "math_500": {
          "score": 0.2494,
          "overall": 0.2494
        },
        "gsm8k": {
          "score": 0.61,
          "overall": 0.61
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1": {
      "model_id": "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1",
      "model_name": "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1",
      "size_bytes": 140620634112,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-02-02T23:25:46.834369",
      "end_time": "2026-02-03T00:05:48.293509",
      "duration_seconds": 2401.46,
      "vllm_pid": 3742287,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.593,
          "overall": 0.593
        },
        "mmlu_pro": {
          "score": 0.4879,
          "overall": 0.4879
        },
        "math_500": {
          "score": 0.455,
          "overall": 0.455
        },
        "gsm8k": {
          "score": 0.67,
          "overall": 0.67
        },
        "gpqa_diamond": {
          "score": 0.31,
          "overall": 0.31
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-1.7B-Instruct": {
      "model_id": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      "model_name": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      "size_bytes": 1711376384,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:41:42.799575",
      "end_time": "2026-02-01T00:29:54.876620",
      "duration_seconds": 78492.08,
      "vllm_pid": 2263688,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.2701,
          "overall": 0.2701
        },
        "mmlu_pro": {
          "score": 0.1364,
          "overall": 0.1364
        },
        "math_500": {
          "score": 0.2494,
          "overall": 0.2494
        },
        "gsm8k": {
          "score": 0.35,
          "overall": 0.35
        },
        "gpqa_diamond": {
          "score": 0.21,
          "overall": 0.21
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-135M-Instruct": {
      "model_id": "HuggingFaceTB/SmolLM2-135M-Instruct",
      "model_name": "HuggingFaceTB/SmolLM2-135M-Instruct",
      "size_bytes": 134515008,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T11:12:53.415631",
      "end_time": "2026-01-31T11:20:53.403973",
      "duration_seconds": 479.99,
      "vllm_pid": 3241126,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.0432,
          "overall": 0.0432
        },
        "mmlu_pro": {
          "score": 0.0114,
          "overall": 0.0114
        },
        "math_500": {
          "score": 0.0254,
          "overall": 0.0254
        },
        "gsm8k": {
          "score": 0.0,
          "overall": 0.0
        },
        "gpqa_diamond": {
          "score": 0.07,
          "overall": 0.07
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM3-3B": {
      "model_id": "HuggingFaceTB/SmolLM3-3B",
      "model_name": "HuggingFaceTB/SmolLM3-3B",
      "size_bytes": 3075098624,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:43:49.617182",
      "end_time": "2026-01-30T15:30:56.990391",
      "duration_seconds": 2827.37,
      "vllm_pid": 924382,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6586,
          "overall": 0.6586
        },
        "mmlu_pro": {
          "score": 0.4471,
          "overall": 0.4471
        },
        "math_500": {
          "score": 0.5358,
          "overall": 0.5358
        },
        "gsm8k": {
          "score": 0.85,
          "overall": 0.85
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LGAI-EXAONE/EXAONE-4.0-32B": {
      "model_id": "LGAI-EXAONE/EXAONE-4.0-32B",
      "model_name": "LGAI-EXAONE/EXAONE-4.0-32B",
      "size_bytes": 32003216384,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T12:34:00.413580",
      "end_time": "2026-01-31T13:18:56.835484",
      "duration_seconds": 2696.42,
      "vllm_pid": 1440412,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8723,
          "overall": 0.8723
        },
        "mmlu_pro": {
          "score": 0.7436,
          "overall": 0.7436
        },
        "math_500": {
          "score": 0.8014,
          "overall": 0.8014
        },
        "gsm8k": {
          "score": 0.97,
          "overall": 0.97
        },
        "gpqa_diamond": {
          "score": 0.53,
          "overall": 0.53
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LGAI-EXAONE/EXAONE-Deep-32B": {
      "model_id": "LGAI-EXAONE/EXAONE-Deep-32B",
      "model_name": "LGAI-EXAONE/EXAONE-Deep-32B",
      "size_bytes": 32003200000,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T10:30:02.983029",
      "end_time": "2026-01-31T12:33:50.375292",
      "duration_seconds": 7427.39,
      "vllm_pid": 1211177,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6212,
          "overall": 0.6212
        },
        "mmlu_pro": {
          "score": 0.3143,
          "overall": 0.3143
        },
        "math_500": {
          "score": 0.4065,
          "overall": 0.4065
        },
        "gsm8k": {
          "score": 0.73,
          "overall": 0.73
        },
        "gpqa_diamond": {
          "score": 0.1,
          "overall": 0.1
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LLM360/K2-Think": {
      "model_id": "LLM360/K2-Think",
      "model_name": "LLM360/K2-Think",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T06:47:19.821991",
      "end_time": "2026-01-31T08:24:01.056001",
      "duration_seconds": 5801.23,
      "vllm_pid": 2740414,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7854,
          "overall": 0.7854
        },
        "mmlu_pro": {
          "score": 0.5079,
          "overall": 0.5079
        },
        "math_500": {
          "score": 0.6212,
          "overall": 0.6212
        },
        "gsm8k": {
          "score": 0.94,
          "overall": 0.94
        },
        "gpqa_diamond": {
          "score": 0.21,
          "overall": 0.21
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-1.2B": {
      "model_id": "LiquidAI/LFM2-1.2B",
      "model_name": "LiquidAI/LFM2-1.2B",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:40:22.894378",
      "end_time": "2026-02-04T19:46:55.500456",
      "duration_seconds": 392.61,
      "vllm_pid": 2199934,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4933,
          "overall": 0.4933
        },
        "mmlu_pro": {
          "score": 0.24,
          "overall": 0.24
        },
        "math_500": {
          "score": 0.4365,
          "overall": 0.4365
        },
        "gsm8k": {
          "score": 0.64,
          "overall": 0.64
        },
        "gpqa_diamond": {
          "score": 0.2,
          "overall": 0.2
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-8B-A1B": {
      "model_id": "LiquidAI/LFM2-8B-A1B",
      "model_name": "LiquidAI/LFM2-8B-A1B",
      "size_bytes": 8339929856,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-29T13:57:46.285727",
      "end_time": "2026-01-29T14:19:01.956962",
      "duration_seconds": 1275.67,
      "vllm_pid": 2304305,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7207,
          "overall": 0.7207
        },
        "mmlu_pro": {
          "score": 0.5064,
          "overall": 0.5064
        },
        "math_500": {
          "score": 0.7621,
          "overall": 0.7621
        },
        "gsm8k": {
          "score": 0.89,
          "overall": 0.89
        },
        "gpqa_diamond": {
          "score": 0.29,
          "overall": 0.29
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MLP-KTLim/llama-3-Korean-Bllossom-8B": {
      "model_id": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
      "model_name": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T18:47:55.184590",
      "end_time": "2026-02-04T19:05:11.841423",
      "duration_seconds": 1036.66,
      "vllm_pid": 2093571,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6265,
          "overall": 0.6265
        },
        "mmlu_pro": {
          "score": 0.3943,
          "overall": 0.3943
        },
        "math_500": {
          "score": 0.3233,
          "overall": 0.3233
        },
        "gsm8k": {
          "score": 0.8,
          "overall": 0.8
        },
        "gpqa_diamond": {
          "score": 0.35,
          "overall": 0.35
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/Jan-nano": {
      "model_id": "Menlo/Jan-nano",
      "model_name": "Menlo/Jan-nano",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T04:39:47.905935",
      "end_time": "2026-01-31T04:57:24.508985",
      "duration_seconds": 1056.6,
      "vllm_pid": 516964,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7765,
          "overall": 0.7765
        },
        "mmlu_pro": {
          "score": 0.5957,
          "overall": 0.5957
        },
        "math_500": {
          "score": 0.8268,
          "overall": 0.8268
        },
        "gsm8k": {
          "score": 0.95,
          "overall": 0.95
        },
        "gpqa_diamond": {
          "score": 0.48,
          "overall": 0.48
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NexaAI/Octopus-v2": {
      "model_id": "NexaAI/Octopus-v2",
      "model_name": "NexaAI/Octopus-v2",
      "size_bytes": 2506217472,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:21:11.052939",
      "end_time": "2026-01-30T14:26:34.911699",
      "duration_seconds": 323.86,
      "vllm_pid": 871316,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.0361,
          "overall": 0.0361
        },
        "mmlu_pro": {
          "score": 0.0071,
          "overall": 0.0071
        },
        "math_500": {
          "score": 0.0185,
          "overall": 0.0185
        },
        "gsm8k": {
          "score": 0.01,
          "overall": 0.01
        },
        "gpqa_diamond": {
          "score": 0.0,
          "overall": 0.0
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nexusflow/Starling-LM-7B-beta": {
      "model_id": "Nexusflow/Starling-LM-7B-beta",
      "model_name": "Nexusflow/Starling-LM-7B-beta",
      "size_bytes": 14485418342,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T07:13:06.267159",
      "end_time": "2026-01-31T07:34:47.348720",
      "duration_seconds": 1301.08,
      "vllm_pid": 829289,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5393,
          "overall": 0.5393
        },
        "mmlu_pro": {
          "score": 0.3121,
          "overall": 0.3121
        },
        "math_500": {
          "score": 0.3118,
          "overall": 0.3118
        },
        "gsm8k": {
          "score": 0.7,
          "overall": 0.7
        },
        "gpqa_diamond": {
          "score": 0.25,
          "overall": 0.25
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/DeepHermes-3-Llama-3-8B-Preview": {
      "model_id": "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
      "model_name": "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-29T13:42:56.658689",
      "end_time": "2026-01-29T14:00:13.711534",
      "duration_seconds": 1037.05,
      "vllm_pid": 2262702,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6447,
          "overall": 0.6447
        },
        "mmlu_pro": {
          "score": 0.4121,
          "overall": 0.4121
        },
        "math_500": {
          "score": 0.2933,
          "overall": 0.2933
        },
        "gsm8k": {
          "score": 0.76,
          "overall": 0.76
        },
        "gpqa_diamond": {
          "score": 0.31,
          "overall": 0.31
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Hermes-2-Pro-Llama-3-8B": {
      "model_id": "NousResearch/Hermes-2-Pro-Llama-3-8B",
      "model_name": "NousResearch/Hermes-2-Pro-Llama-3-8B",
      "size_bytes": 8030523392,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-29T13:41:08.917764",
      "end_time": "2026-01-29T13:56:38.195242",
      "duration_seconds": 929.28,
      "vllm_pid": 2257415,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6168,
          "overall": 0.6168
        },
        "mmlu_pro": {
          "score": 0.37,
          "overall": 0.37
        },
        "math_500": {
          "score": 0.3072,
          "overall": 0.3072
        },
        "gsm8k": {
          "score": 0.69,
          "overall": 0.69
        },
        "gpqa_diamond": {
          "score": 0.31,
          "overall": 0.31
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Hermes-2-Pro-Mistral-7B": {
      "model_id": "NousResearch/Hermes-2-Pro-Mistral-7B",
      "model_name": "NousResearch/Hermes-2-Pro-Mistral-7B",
      "size_bytes": 7241994240,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T04:39:47.337444",
      "end_time": "2026-01-31T04:57:17.327738",
      "duration_seconds": 1049.99,
      "vllm_pid": 516797,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.474,
          "overall": 0.474
        },
        "mmlu_pro": {
          "score": 0.29,
          "overall": 0.29
        },
        "math_500": {
          "score": 0.2564,
          "overall": 0.2564
        },
        "gsm8k": {
          "score": 0.67,
          "overall": 0.67
        },
        "gpqa_diamond": {
          "score": 0.22,
          "overall": 0.22
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Hermes-3-Llama-3.1-8B": {
      "model_id": "NousResearch/Hermes-3-Llama-3.1-8B",
      "model_name": "NousResearch/Hermes-3-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:05:21.878704",
      "end_time": "2026-02-04T19:19:21.261379",
      "duration_seconds": 839.38,
      "vllm_pid": 2128192,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6597,
          "overall": 0.6597
        },
        "mmlu_pro": {
          "score": 0.4114,
          "overall": 0.4114
        },
        "math_500": {
          "score": 0.3187,
          "overall": 0.3187
        },
        "gsm8k": {
          "score": 0.78,
          "overall": 0.78
        },
        "gpqa_diamond": {
          "score": 0.31,
          "overall": 0.31
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
      "model_id": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
      "model_name": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
      "size_bytes": 46702809088,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T05:55:56.765561",
      "end_time": "2026-01-31T06:34:33.644973",
      "duration_seconds": 2316.88,
      "vllm_pid": 664196,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6735,
          "overall": 0.6735
        },
        "mmlu_pro": {
          "score": 0.4421,
          "overall": 0.4421
        },
        "math_500": {
          "score": 0.4388,
          "overall": 0.4388
        },
        "gsm8k": {
          "score": 0.79,
          "overall": 0.79
        },
        "gpqa_diamond": {
          "score": 0.26,
          "overall": 0.26
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Nous-Hermes-2-Yi-34B": {
      "model_id": "NousResearch/Nous-Hermes-2-Yi-34B",
      "model_name": "NousResearch/Nous-Hermes-2-Yi-34B",
      "size_bytes": 34388917248,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-04T21:40:18.290853",
      "end_time": "2026-02-04T21:59:59.922958",
      "duration_seconds": 1181.63,
      "vllm_pid": 2435423,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4905,
          "overall": 0.4905
        },
        "mmlu_pro": {
          "score": 0.3414,
          "overall": 0.3414
        },
        "math_500": {
          "score": 0.2887,
          "overall": 0.2887
        },
        "gsm8k": {
          "score": 0.68,
          "overall": 0.68
        },
        "gpqa_diamond": {
          "score": 0.09,
          "overall": 0.09
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NovaSky-AI/Sky-T1-32B-Preview": {
      "model_id": "NovaSky-AI/Sky-T1-32B-Preview",
      "model_name": "NovaSky-AI/Sky-T1-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-04T19:45:14.585628",
      "end_time": "2026-02-04T20:31:17.800501",
      "duration_seconds": 2763.21,
      "vllm_pid": 2211228,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8486,
          "overall": 0.8486
        },
        "mmlu_pro": {
          "score": 0.7064,
          "overall": 0.7064
        },
        "math_500": {
          "score": 0.8637,
          "overall": 0.8637
        },
        "gsm8k": {
          "score": 0.98,
          "overall": 0.98
        },
        "gpqa_diamond": {
          "score": 0.51,
          "overall": 0.51
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Orenguteng/Llama-3-8B-Lexi-Uncensored": {
      "model_id": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
      "model_name": "Orenguteng/Llama-3-8B-Lexi-Uncensored",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:51:33.054398",
      "end_time": "2026-02-04T20:09:30.100148",
      "duration_seconds": 1077.05,
      "vllm_pid": 2225945,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4902,
          "overall": 0.4902
        },
        "mmlu_pro": {
          "score": 0.3236,
          "overall": 0.3236
        },
        "math_500": {
          "score": 0.2771,
          "overall": 0.2771
        },
        "gsm8k": {
          "score": 0.69,
          "overall": 0.69
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2": {
      "model_id": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
      "model_name": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T20:10:10.335665",
      "end_time": "2026-02-04T20:39:16.913568",
      "duration_seconds": 1746.58,
      "vllm_pid": 2263901,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7049,
          "overall": 0.7049
        },
        "mmlu_pro": {
          "score": 0.4736,
          "overall": 0.4736
        },
        "math_500": {
          "score": 0.4965,
          "overall": 0.4965
        },
        "gsm8k": {
          "score": 0.89,
          "overall": 0.89
        },
        "gpqa_diamond": {
          "score": 0.24,
          "overall": 0.24
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/CodeQwen1.5-7B-Chat": {
      "model_id": "Qwen/CodeQwen1.5-7B-Chat",
      "model_name": "Qwen/CodeQwen1.5-7B-Chat",
      "size_bytes": 7250284544,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:19:31.295177",
      "end_time": "2026-02-04T19:38:31.185666",
      "duration_seconds": 1139.89,
      "vllm_pid": 2157443,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3283,
          "overall": 0.3283
        },
        "mmlu_pro": {
          "score": 0.1486,
          "overall": 0.1486
        },
        "math_500": {
          "score": 0.0947,
          "overall": 0.0947
        },
        "gsm8k": {
          "score": 0.22,
          "overall": 0.22
        },
        "gpqa_diamond": {
          "score": 0.17,
          "overall": 0.17
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/QwQ-32B": {
      "model_id": "Qwen/QwQ-32B",
      "model_name": "Qwen/QwQ-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-29T22:40:26.235877",
      "end_time": "2026-01-30T00:30:57.450881",
      "duration_seconds": 6631.22,
      "vllm_pid": 3317363,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7582,
          "overall": 0.7582
        },
        "mmlu_pro": {
          "score": 0.4664,
          "overall": 0.4664
        },
        "math_500": {
          "score": 0.4873,
          "overall": 0.4873
        },
        "gsm8k": {
          "score": 0.85,
          "overall": 0.85
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/QwQ-32B-Preview": {
      "model_id": "Qwen/QwQ-32B-Preview",
      "model_name": "Qwen/QwQ-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T05:52:17.419077",
      "end_time": "2026-01-30T07:26:05.183895",
      "duration_seconds": 5627.76,
      "vllm_pid": 4116327,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7897,
          "overall": 0.7897
        },
        "mmlu_pro": {
          "score": 0.5864,
          "overall": 0.5864
        },
        "math_500": {
          "score": 0.7552,
          "overall": 0.7552
        },
        "gsm8k": {
          "score": 0.97,
          "overall": 0.97
        },
        "gpqa_diamond": {
          "score": 0.25,
          "overall": 0.25
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2-72B-Instruct": {
      "model_id": "Qwen/Qwen2-72B-Instruct",
      "model_name": "Qwen/Qwen2-72B-Instruct",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-04T17:35:20.463894",
      "end_time": "2026-02-04T18:06:10.115666",
      "duration_seconds": 1849.65,
      "vllm_pid": 1944531,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8313,
          "overall": 0.8313
        },
        "mmlu_pro": {
          "score": 0.65,
          "overall": 0.65
        },
        "math_500": {
          "score": 0.7182,
          "overall": 0.7182
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.39,
          "overall": 0.39
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2-7B-Instruct": {
      "model_id": "Qwen/Qwen2-7B-Instruct",
      "model_name": "Qwen/Qwen2-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:41:44.011384",
      "end_time": "2026-01-31T02:54:59.771763",
      "duration_seconds": 795.76,
      "vllm_pid": 281048,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4998,
          "overall": 0.4998
        },
        "mmlu_pro": {
          "score": 0.3836,
          "overall": 0.3836
        },
        "math_500": {
          "score": 0.552,
          "overall": 0.552
        },
        "gsm8k": {
          "score": 0.88,
          "overall": 0.88
        },
        "gpqa_diamond": {
          "score": 0.3,
          "overall": 0.3
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-0.5B": {
      "model_id": "Qwen/Qwen2.5-0.5B",
      "model_name": "Qwen/Qwen2.5-0.5B",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:38:41.219646",
      "end_time": "2026-02-04T19:48:45.565961",
      "duration_seconds": 604.35,
      "vllm_pid": 2196410,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.1728,
          "overall": 0.1728
        },
        "mmlu_pro": {
          "score": 0.1043,
          "overall": 0.1043
        },
        "math_500": {
          "score": 0.2448,
          "overall": 0.2448
        },
        "gsm8k": {
          "score": 0.27,
          "overall": 0.27
        },
        "gpqa_diamond": {
          "score": 0.14,
          "overall": 0.14
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-0.5B-Instruct": {
      "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
      "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T06:33:21.061786",
      "end_time": "2026-01-31T06:40:19.703811",
      "duration_seconds": 418.64,
      "vllm_pid": 740549,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3172,
          "overall": 0.3172
        },
        "mmlu_pro": {
          "score": 0.1357,
          "overall": 0.1357
        },
        "math_500": {
          "score": 0.3002,
          "overall": 0.3002
        },
        "gsm8k": {
          "score": 0.45,
          "overall": 0.45
        },
        "gpqa_diamond": {
          "score": 0.2,
          "overall": 0.2
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-1.5B-Instruct": {
      "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
      "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T03:51:37.880094",
      "end_time": "2026-01-31T04:00:16.958909",
      "duration_seconds": 519.08,
      "vllm_pid": 2401082,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5607,
          "overall": 0.5607
        },
        "mmlu_pro": {
          "score": 0.2857,
          "overall": 0.2857
        },
        "math_500": {
          "score": 0.5243,
          "overall": 0.5243
        },
        "gsm8k": {
          "score": 0.7,
          "overall": 0.7
        },
        "gpqa_diamond": {
          "score": 0.34,
          "overall": 0.34
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-14B-Instruct": {
      "model_id": "Qwen/Qwen2.5-14B-Instruct",
      "model_name": "Qwen/Qwen2.5-14B-Instruct",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T10:30:02.989163",
      "end_time": "2026-01-31T11:07:24.008088",
      "duration_seconds": 2241.02,
      "vllm_pid": 3157435,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8128,
          "overall": 0.8128
        },
        "mmlu_pro": {
          "score": 0.64,
          "overall": 0.64
        },
        "math_500": {
          "score": 0.8199,
          "overall": 0.8199
        },
        "gsm8k": {
          "score": 0.95,
          "overall": 0.95
        },
        "gpqa_diamond": {
          "score": 0.45,
          "overall": 0.45
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-32B-Instruct": {
      "model_id": "Qwen/Qwen2.5-32B-Instruct",
      "model_name": "Qwen/Qwen2.5-32B-Instruct",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T10:21:33.660659",
      "end_time": "2026-01-31T10:49:38.056190",
      "duration_seconds": 1684.4,
      "vllm_pid": 3139835,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8435,
          "overall": 0.8435
        },
        "mmlu_pro": {
          "score": 0.6871,
          "overall": 0.6871
        },
        "math_500": {
          "score": 0.843,
          "overall": 0.843
        },
        "gsm8k": {
          "score": 0.98,
          "overall": 0.98
        },
        "gpqa_diamond": {
          "score": 0.47,
          "overall": 0.47
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-3B-Instruct": {
      "model_id": "Qwen/Qwen2.5-3B-Instruct",
      "model_name": "Qwen/Qwen2.5-3B-Instruct",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:26:21.416728",
      "end_time": "2026-02-04T19:39:52.630155",
      "duration_seconds": 811.21,
      "vllm_pid": 2171208,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.673,
          "overall": 0.673
        },
        "mmlu_pro": {
          "score": 0.4314,
          "overall": 0.4314
        },
        "math_500": {
          "score": 0.672,
          "overall": 0.672
        },
        "gsm8k": {
          "score": 0.87,
          "overall": 0.87
        },
        "gpqa_diamond": {
          "score": 0.39,
          "overall": 0.39
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-72B-Instruct": {
      "model_id": "Qwen/Qwen2.5-72B-Instruct",
      "model_name": "Qwen/Qwen2.5-72B-Instruct",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T14:40:37.365351",
      "end_time": "2026-01-30T15:37:13.620832",
      "duration_seconds": 3396.26,
      "vllm_pid": 917505,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8646,
          "overall": 0.8646
        },
        "mmlu_pro": {
          "score": 0.7029,
          "overall": 0.7029
        },
        "math_500": {
          "score": 0.866,
          "overall": 0.866
        },
        "gsm8k": {
          "score": 0.97,
          "overall": 0.97
        },
        "gpqa_diamond": {
          "score": 0.55,
          "overall": 0.55
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-7B": {
      "model_id": "Qwen/Qwen2.5-7B",
      "model_name": "Qwen/Qwen2.5-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T20:35:54.511239",
      "end_time": "2026-02-04T20:54:28.426726",
      "duration_seconds": 1113.92,
      "vllm_pid": 2313013,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5251,
          "overall": 0.5251
        },
        "mmlu_pro": {
          "score": 0.4121,
          "overall": 0.4121
        },
        "math_500": {
          "score": 0.6721,
          "overall": 0.6721
        },
        "gsm8k": {
          "score": 0.83,
          "overall": 0.83
        },
        "gpqa_diamond": {
          "score": 0.32,
          "overall": 0.32
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-7B-Instruct": {
      "model_id": "Qwen/Qwen2.5-7B-Instruct",
      "model_name": "Qwen/Qwen2.5-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:21:10.532014",
      "end_time": "2026-01-30T14:40:23.024873",
      "duration_seconds": 1152.49,
      "vllm_pid": 871308,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7611,
          "overall": 0.7611
        },
        "mmlu_pro": {
          "score": 0.5457,
          "overall": 0.5457
        },
        "math_500": {
          "score": 0.7506,
          "overall": 0.7506
        },
        "gsm8k": {
          "score": 0.94,
          "overall": 0.94
        },
        "gpqa_diamond": {
          "score": 0.41,
          "overall": 0.41
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-Coder-32B-Instruct": {
      "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "model_name": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T05:20:34.112113",
      "end_time": "2026-01-30T05:52:07.388329",
      "duration_seconds": 1893.28,
      "vllm_pid": 4054894,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7942,
          "overall": 0.7942
        },
        "mmlu_pro": {
          "score": 0.6286,
          "overall": 0.6286
        },
        "math_500": {
          "score": 0.7945,
          "overall": 0.7945
        },
        "gsm8k": {
          "score": 0.98,
          "overall": 0.98
        },
        "gpqa_diamond": {
          "score": 0.49,
          "overall": 0.49
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-Coder-7B-Instruct": {
      "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "model_name": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T03:31:40.877634",
      "end_time": "2026-01-31T03:51:27.841588",
      "duration_seconds": 1186.96,
      "vllm_pid": 2362495,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6872,
          "overall": 0.6872
        },
        "mmlu_pro": {
          "score": 0.46,
          "overall": 0.46
        },
        "math_500": {
          "score": 0.6859,
          "overall": 0.6859
        },
        "gsm8k": {
          "score": 0.89,
          "overall": 0.89
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-0.6B": {
      "model_id": "Qwen/Qwen3-0.6B",
      "model_name": "Qwen/Qwen3-0.6B",
      "size_bytes": 751632384,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:21:11.052837",
      "end_time": "2026-01-30T14:44:56.945796",
      "duration_seconds": 1425.89,
      "vllm_pid": 871318,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5121,
          "overall": 0.5121
        },
        "mmlu_pro": {
          "score": 0.3114,
          "overall": 0.3114
        },
        "math_500": {
          "score": 0.3626,
          "overall": 0.3626
        },
        "gsm8k": {
          "score": 0.66,
          "overall": 0.66
        },
        "gpqa_diamond": {
          "score": 0.14,
          "overall": 0.14
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-1.7B": {
      "model_id": "Qwen/Qwen3-1.7B",
      "model_name": "Qwen/Qwen3-1.7B",
      "size_bytes": 2031739904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T06:44:55.046356",
      "end_time": "2026-01-31T07:22:14.165276",
      "duration_seconds": 2239.12,
      "vllm_pid": 765202,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6588,
          "overall": 0.6588
        },
        "mmlu_pro": {
          "score": 0.3957,
          "overall": 0.3957
        },
        "math_500": {
          "score": 0.2517,
          "overall": 0.2517
        },
        "gsm8k": {
          "score": 0.73,
          "overall": 0.73
        },
        "gpqa_diamond": {
          "score": 0.16,
          "overall": 0.16
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-14B": {
      "model_id": "Qwen/Qwen3-14B",
      "model_name": "Qwen/Qwen3-14B",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T08:24:01.694474",
      "end_time": "2026-01-31T10:10:54.001706",
      "duration_seconds": 6412.31,
      "vllm_pid": 2920586,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8079,
          "overall": 0.8079
        },
        "mmlu_pro": {
          "score": 0.5729,
          "overall": 0.5729
        },
        "math_500": {
          "score": 0.3326,
          "overall": 0.3326
        },
        "gsm8k": {
          "score": 0.78,
          "overall": 0.78
        },
        "gpqa_diamond": {
          "score": 0.19,
          "overall": 0.19
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-235B-A22B-Instruct-2507": {
      "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "model_name": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "size_bytes": 235093634560,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-01-31T01:36:39.221497",
      "end_time": "2026-01-31T02:41:42.758735",
      "duration_seconds": 3903.54,
      "vllm_pid": 159174,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8991,
          "overall": 0.8991
        },
        "mmlu_pro": {
          "score": 0.7943,
          "overall": 0.7943
        },
        "math_500": {
          "score": 0.7968,
          "overall": 0.7968
        },
        "gsm8k": {
          "score": 0.98,
          "overall": 0.98
        },
        "gpqa_diamond": {
          "score": 0.72,
          "overall": 0.72
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-30B-A3B": {
      "model_id": "Qwen/Qwen3-30B-A3B",
      "model_name": "Qwen/Qwen3-30B-A3B",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T15:37:23.695287",
      "end_time": "2026-01-30T16:44:51.665030",
      "duration_seconds": 4047.97,
      "vllm_pid": 1027232,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8121,
          "overall": 0.8121
        },
        "mmlu_pro": {
          "score": 0.5393,
          "overall": 0.5393
        },
        "math_500": {
          "score": 0.3279,
          "overall": 0.3279
        },
        "gsm8k": {
          "score": 0.82,
          "overall": 0.82
        },
        "gpqa_diamond": {
          "score": 0.18,
          "overall": 0.18
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-30B-A3B-Instruct-2507": {
      "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "model_name": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-04T18:06:10.146753",
      "end_time": "2026-02-04T18:27:51.248267",
      "duration_seconds": 1301.1,
      "vllm_pid": 2009425,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.867,
          "overall": 0.867
        },
        "mmlu_pro": {
          "score": 0.7229,
          "overall": 0.7229
        },
        "math_500": {
          "score": 0.8152,
          "overall": 0.8152
        },
        "gsm8k": {
          "score": 0.99,
          "overall": 0.99
        },
        "gpqa_diamond": {
          "score": 0.62,
          "overall": 0.62
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-30B-A3B-Thinking-2507": {
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "model_name": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-04T20:31:27.846961",
      "end_time": "2026-02-04T21:40:08.253390",
      "duration_seconds": 4120.41,
      "vllm_pid": 2304792,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7888,
          "overall": 0.7888
        },
        "mmlu_pro": {
          "score": 0.53,
          "overall": 0.53
        },
        "math_500": {
          "score": 0.4111,
          "overall": 0.4111
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.14,
          "overall": 0.14
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-32B": {
      "model_id": "Qwen/Qwen3-32B",
      "model_name": "Qwen/Qwen3-32B",
      "size_bytes": 32762123264,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-04T18:28:01.281605",
      "end_time": "2026-02-04T19:45:14.553337",
      "duration_seconds": 4633.27,
      "vllm_pid": 2054763,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8531,
          "overall": 0.8531
        },
        "mmlu_pro": {
          "score": 0.6357,
          "overall": 0.6357
        },
        "math_500": {
          "score": 0.3372,
          "overall": 0.3372
        },
        "gsm8k": {
          "score": 0.85,
          "overall": 0.85
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B": {
      "model_id": "Qwen/Qwen3-4B",
      "model_name": "Qwen/Qwen3-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T18:13:14.578059",
      "end_time": "2026-02-04T19:07:02.921596",
      "duration_seconds": 3228.34,
      "vllm_pid": 2024272,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7442,
          "overall": 0.7442
        },
        "mmlu_pro": {
          "score": 0.4886,
          "overall": 0.4886
        },
        "math_500": {
          "score": 0.2725,
          "overall": 0.2725
        },
        "gsm8k": {
          "score": 0.62,
          "overall": 0.62
        },
        "gpqa_diamond": {
          "score": 0.12,
          "overall": 0.12
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Instruct-2507": {
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "model_name": "Qwen/Qwen3-4B-Instruct-2507",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T04:26:11.272439",
      "end_time": "2026-01-31T04:42:10.032123",
      "duration_seconds": 958.76,
      "vllm_pid": 2466522,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8007,
          "overall": 0.8007
        },
        "mmlu_pro": {
          "score": 0.6236,
          "overall": 0.6236
        },
        "math_500": {
          "score": 0.8014,
          "overall": 0.8014
        },
        "gsm8k": {
          "score": 0.97,
          "overall": 0.97
        },
        "gpqa_diamond": {
          "score": 0.47,
          "overall": 0.47
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Thinking-2507": {
      "model_id": "Qwen/Qwen3-4B-Thinking-2507",
      "model_name": "Qwen/Qwen3-4B-Thinking-2507",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T18:22:59.064995",
      "end_time": "2026-02-04T19:51:02.804402",
      "duration_seconds": 5283.74,
      "vllm_pid": 2043673,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5837,
          "overall": 0.5837
        },
        "mmlu_pro": {
          "score": 0.28,
          "overall": 0.28
        },
        "math_500": {
          "score": 0.4134,
          "overall": 0.4134
        },
        "gsm8k": {
          "score": 0.86,
          "overall": 0.86
        },
        "gpqa_diamond": {
          "score": 0.04,
          "overall": 0.04
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-8B": {
      "model_id": "Qwen/Qwen3-8B",
      "model_name": "Qwen/Qwen3-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T20:00:43.359357",
      "end_time": "2026-01-30T21:27:25.567769",
      "duration_seconds": 5202.21,
      "vllm_pid": 1512971,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7384,
          "overall": 0.7384
        },
        "mmlu_pro": {
          "score": 0.4657,
          "overall": 0.4657
        },
        "math_500": {
          "score": 0.2656,
          "overall": 0.2656
        },
        "gsm8k": {
          "score": 0.65,
          "overall": 0.65
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-Coder-30B-A3B-Instruct": {
      "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
      "model_name": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T20:00:43.359489",
      "end_time": "2026-01-30T20:28:15.325263",
      "duration_seconds": 1651.97,
      "vllm_pid": 1512972,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8397,
          "overall": 0.8397
        },
        "mmlu_pro": {
          "score": 0.6886,
          "overall": 0.6886
        },
        "math_500": {
          "score": 0.8614,
          "overall": 0.8614
        },
        "gsm8k": {
          "score": 0.99,
          "overall": 0.99
        },
        "gpqa_diamond": {
          "score": 0.56,
          "overall": 0.56
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SciPhi/Triplex": {
      "model_id": "SciPhi/Triplex",
      "model_name": "SciPhi/Triplex",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T10:21:33.662640",
      "end_time": "2026-01-31T10:41:07.162005",
      "duration_seconds": 1173.5,
      "vllm_pid": 3139834,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.0035,
          "overall": 0.0035
        },
        "mmlu_pro": {
          "score": 0.0014,
          "overall": 0.0014
        },
        "math_500": {
          "score": 0.0069,
          "overall": 0.0069
        },
        "gsm8k": {
          "score": 0.0,
          "overall": 0.0
        },
        "gpqa_diamond": {
          "score": 0.0,
          "overall": 0.0
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tiiny/SmallThinker-3B-Preview": {
      "model_id": "Tiiny/SmallThinker-3B-Preview",
      "model_name": "Tiiny/SmallThinker-3B-Preview",
      "size_bytes": 3397103616,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T06:02:03.345205",
      "end_time": "2026-01-31T06:36:55.387869",
      "duration_seconds": 2092.04,
      "vllm_pid": 2654324,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6431,
          "overall": 0.6431
        },
        "mmlu_pro": {
          "score": 0.3936,
          "overall": 0.3936
        },
        "math_500": {
          "score": 0.6166,
          "overall": 0.6166
        },
        "gsm8k": {
          "score": 0.84,
          "overall": 0.84
        },
        "gpqa_diamond": {
          "score": 0.19,
          "overall": 0.19
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "WeiboAI/VibeThinker-1.5B": {
      "model_id": "WeiboAI/VibeThinker-1.5B",
      "model_name": "WeiboAI/VibeThinker-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T17:37:26.962491",
      "end_time": "2026-02-04T18:13:14.513309",
      "duration_seconds": 2147.55,
      "vllm_pid": 1950897,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4149,
          "overall": 0.4149
        },
        "mmlu_pro": {
          "score": 0.1864,
          "overall": 0.1864
        },
        "math_500": {
          "score": 0.4226,
          "overall": 0.4226
        },
        "gsm8k": {
          "score": 0.63,
          "overall": 0.63
        },
        "gpqa_diamond": {
          "score": 0.05,
          "overall": 0.05
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "XiaomiMiMo/MiMo-7B-RL": {
      "model_id": "XiaomiMiMo/MiMo-7B-RL",
      "model_name": "XiaomiMiMo/MiMo-7B-RL",
      "size_bytes": 7833409536,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T12:34:00.415309",
      "end_time": "2026-01-31T14:11:09.042340",
      "duration_seconds": 5828.63,
      "vllm_pid": 3399702,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5781,
          "overall": 0.5781
        },
        "mmlu_pro": {
          "score": 0.3114,
          "overall": 0.3114
        },
        "math_500": {
          "score": 0.3649,
          "overall": 0.3649
        },
        "gsm8k": {
          "score": 0.78,
          "overall": 0.78
        },
        "gpqa_diamond": {
          "score": 0.13,
          "overall": 0.13
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "agentica-org/DeepCoder-14B-Preview": {
      "model_id": "agentica-org/DeepCoder-14B-Preview",
      "model_name": "agentica-org/DeepCoder-14B-Preview",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:41:44.011337",
      "end_time": "2026-01-31T05:20:53.144516",
      "duration_seconds": 9549.13,
      "vllm_pid": 281050,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6583,
          "overall": 0.6583
        },
        "mmlu_pro": {
          "score": 0.3621,
          "overall": 0.3621
        },
        "math_500": {
          "score": 0.5035,
          "overall": 0.5035
        },
        "gsm8k": {
          "score": 0.85,
          "overall": 0.85
        },
        "gpqa_diamond": {
          "score": 0.12,
          "overall": 0.12
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "agentica-org/DeepScaleR-1.5B-Preview": {
      "model_id": "agentica-org/DeepScaleR-1.5B-Preview",
      "model_name": "agentica-org/DeepScaleR-1.5B-Preview",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T03:20:10.339405",
      "end_time": "2026-01-31T03:57:06.879627",
      "duration_seconds": 2216.54,
      "vllm_pid": 362134,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3056,
          "overall": 0.3056
        },
        "mmlu_pro": {
          "score": 0.16,
          "overall": 0.16
        },
        "math_500": {
          "score": 0.5635,
          "overall": 0.5635
        },
        "gsm8k": {
          "score": 0.77,
          "overall": 0.77
        },
        "gpqa_diamond": {
          "score": 0.07,
          "overall": 0.07
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "alpindale/WizardLM-2-8x22B": {
      "model_id": "alpindale/WizardLM-2-8x22B",
      "model_name": "alpindale/WizardLM-2-8x22B",
      "size_bytes": 140620634112,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-02-02T18:15:35.317507",
      "end_time": "2026-02-02T19:54:36.926194",
      "duration_seconds": 5941.61,
      "vllm_pid": 3164601,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8132,
          "overall": 0.8132
        },
        "mmlu_pro": {
          "score": 0.6007,
          "overall": 0.6007
        },
        "math_500": {
          "score": 0.6074,
          "overall": 0.6074
        },
        "gsm8k": {
          "score": 0.91,
          "overall": 0.91
        },
        "gpqa_diamond": {
          "score": 0.49,
          "overall": 0.49
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-21B-A3B-Thinking": {
      "model_id": "baidu/ERNIE-4.5-21B-A3B-Thinking",
      "model_name": "baidu/ERNIE-4.5-21B-A3B-Thinking",
      "size_bytes": 21825437888,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-30T23:10:35.631743",
      "end_time": "2026-01-31T00:45:35.271244",
      "duration_seconds": 5699.64,
      "vllm_pid": 1868312,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7058,
          "overall": 0.7058
        },
        "mmlu_pro": {
          "score": 0.4221,
          "overall": 0.4221
        },
        "math_500": {
          "score": 0.2679,
          "overall": 0.2679
        },
        "gsm8k": {
          "score": 0.69,
          "overall": 0.69
        },
        "gpqa_diamond": {
          "score": 0.11,
          "overall": 0.11
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "berkeley-nest/Starling-LM-7B-alpha": {
      "model_id": "berkeley-nest/Starling-LM-7B-alpha",
      "model_name": "berkeley-nest/Starling-LM-7B-alpha",
      "size_bytes": 7241748480,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T04:26:11.273173",
      "end_time": "2026-01-31T04:44:08.760324",
      "duration_seconds": 1077.49,
      "vllm_pid": 2466521,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5695,
          "overall": 0.5695
        },
        "mmlu_pro": {
          "score": 0.34,
          "overall": 0.34
        },
        "math_500": {
          "score": 0.3326,
          "overall": 0.3326
        },
        "gsm8k": {
          "score": 0.7,
          "overall": 0.7
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-Prover-V2-671B": {
      "model_id": "deepseek-ai/DeepSeek-Prover-V2-671B",
      "model_name": "deepseek-ai/DeepSeek-Prover-V2-671B",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-01-30T20:00:43.352077",
      "end_time": "2026-01-30T23:10:35.571948",
      "duration_seconds": 11392.22,
      "vllm_pid": 3733500,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8839,
          "overall": 0.8839
        },
        "mmlu_pro": {
          "score": 0.7579,
          "overall": 0.7579
        },
        "math_500": {
          "score": 0.8406,
          "overall": 0.8406
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.59,
          "overall": 0.59
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1": {
      "model_id": "deepseek-ai/DeepSeek-R1",
      "model_name": "deepseek-ai/DeepSeek-R1",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-01-29T22:24:02.648742",
      "end_time": "2026-01-30T04:14:13.180207",
      "duration_seconds": 21010.53,
      "vllm_pid": 1350651,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8169,
          "overall": 0.8169
        },
        "mmlu_pro": {
          "score": 0.5929,
          "overall": 0.5929
        },
        "math_500": {
          "score": 0.672,
          "overall": 0.672
        },
        "gsm8k": {
          "score": 0.95,
          "overall": 0.95
        },
        "gpqa_diamond": {
          "score": 0.21,
          "overall": 0.21
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-0528": {
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "model_name": "deepseek-ai/DeepSeek-R1-0528",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-01-30T04:14:13.815962",
      "end_time": "2026-01-30T11:04:31.188205",
      "duration_seconds": 24617.37,
      "vllm_pid": 1995128,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7709,
          "overall": 0.7709
        },
        "mmlu_pro": {
          "score": 0.4714,
          "overall": 0.4714
        },
        "math_500": {
          "score": 0.3695,
          "overall": 0.3695
        },
        "gsm8k": {
          "score": 0.76,
          "overall": 0.76
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": {
      "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "model_name": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:21:10.531318",
      "end_time": "2026-01-30T16:01:47.452736",
      "duration_seconds": 6036.92,
      "vllm_pid": 871307,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6921,
          "overall": 0.6921
        },
        "mmlu_pro": {
          "score": 0.3714,
          "overall": 0.3714
        },
        "math_500": {
          "score": 0.3326,
          "overall": 0.3326
        },
        "gsm8k": {
          "score": 0.73,
          "overall": 0.73
        },
        "gpqa_diamond": {
          "score": 0.13,
          "overall": 0.13
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T01:36:39.221678",
      "end_time": "2026-01-31T04:26:01.239843",
      "duration_seconds": 10162.02,
      "vllm_pid": 2142380,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8039,
          "overall": 0.8039
        },
        "mmlu_pro": {
          "score": 0.5579,
          "overall": 0.5579
        },
        "math_500": {
          "score": 0.6767,
          "overall": 0.6767
        },
        "gsm8k": {
          "score": 0.94,
          "overall": 0.94
        },
        "gpqa_diamond": {
          "score": 0.17,
          "overall": 0.17
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Llama-8B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:44:57.125932",
      "end_time": "2026-01-30T16:13:36.573194",
      "duration_seconds": 5319.45,
      "vllm_pid": 927313,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6088,
          "overall": 0.6088
        },
        "mmlu_pro": {
          "score": 0.3636,
          "overall": 0.3636
        },
        "math_500": {
          "score": 0.5958,
          "overall": 0.5958
        },
        "gsm8k": {
          "score": 0.83,
          "overall": 0.83
        },
        "gpqa_diamond": {
          "score": 0.08,
          "overall": 0.08
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T09:01:38.064674",
      "end_time": "2026-01-30T09:34:28.698137",
      "duration_seconds": 1970.63,
      "vllm_pid": 276457,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3842,
          "overall": 0.3842
        },
        "mmlu_pro": {
          "score": 0.2136,
          "overall": 0.2136
        },
        "math_500": {
          "score": 0.5473,
          "overall": 0.5473
        },
        "gsm8k": {
          "score": 0.76,
          "overall": 0.76
        },
        "gpqa_diamond": {
          "score": 0.08,
          "overall": 0.08
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T03:20:10.339206",
      "end_time": "2026-01-31T05:41:36.532071",
      "duration_seconds": 8486.19,
      "vllm_pid": 362133,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7493,
          "overall": 0.7493
        },
        "mmlu_pro": {
          "score": 0.4657,
          "overall": 0.4657
        },
        "math_500": {
          "score": 0.612,
          "overall": 0.612
        },
        "gsm8k": {
          "score": 0.93,
          "overall": 0.93
        },
        "gpqa_diamond": {
          "score": 0.14,
          "overall": 0.14
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T09:01:38.062901",
      "end_time": "2026-01-30T10:45:04.571946",
      "duration_seconds": 6206.51,
      "vllm_pid": 276459,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.79,
          "overall": 0.79
        },
        "mmlu_pro": {
          "score": 0.5279,
          "overall": 0.5279
        },
        "math_500": {
          "score": 0.6628,
          "overall": 0.6628
        },
        "gsm8k": {
          "score": 0.93,
          "overall": 0.93
        },
        "gpqa_diamond": {
          "score": 0.21,
          "overall": 0.21
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T23:10:35.632523",
      "end_time": "2026-01-31T00:24:05.527472",
      "duration_seconds": 4409.89,
      "vllm_pid": 1868313,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5839,
          "overall": 0.5839
        },
        "mmlu_pro": {
          "score": 0.3621,
          "overall": 0.3621
        },
        "math_500": {
          "score": 0.612,
          "overall": 0.612
        },
        "gsm8k": {
          "score": 0.86,
          "overall": 0.86
        },
        "gpqa_diamond": {
          "score": 0.13,
          "overall": 0.13
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "defog/llama-3-sqlcoder-8b": {
      "model_id": "defog/llama-3-sqlcoder-8b",
      "model_name": "defog/llama-3-sqlcoder-8b",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:49:46.066704",
      "end_time": "2026-02-04T20:35:54.480684",
      "duration_seconds": 2768.41,
      "vllm_pid": 2221934,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3842,
          "overall": 0.3842
        },
        "mmlu_pro": {
          "score": 0.2171,
          "overall": 0.2171
        },
        "math_500": {
          "score": 0.2702,
          "overall": 0.2702
        },
        "gsm8k": {
          "score": 0.72,
          "overall": 0.72
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/Dolphin-Mistral-24B-Venice-Edition": {
      "model_id": "dphn/Dolphin-Mistral-24B-Venice-Edition",
      "model_name": "dphn/Dolphin-Mistral-24B-Venice-Edition",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-31T08:11:13.075738",
      "end_time": "2026-01-31T08:40:53.986642",
      "duration_seconds": 1780.91,
      "vllm_pid": 941764,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8297,
          "overall": 0.8297
        },
        "mmlu_pro": {
          "score": 0.66,
          "overall": 0.66
        },
        "math_500": {
          "score": 0.746,
          "overall": 0.746
        },
        "gsm8k": {
          "score": 0.95,
          "overall": 0.95
        },
        "gpqa_diamond": {
          "score": 0.52,
          "overall": 0.52
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/dolphin-2.5-mixtral-8x7b": {
      "model_id": "dphn/dolphin-2.5-mixtral-8x7b",
      "model_name": "dphn/dolphin-2.5-mixtral-8x7b",
      "size_bytes": 46702809088,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T10:45:09.151196",
      "end_time": "2026-01-30T12:58:50.113511",
      "duration_seconds": 8020.96,
      "vllm_pid": 473707,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.107,
          "overall": 0.107
        },
        "mmlu_pro": {
          "score": 0.0571,
          "overall": 0.0571
        },
        "math_500": {
          "score": 0.03,
          "overall": 0.03
        },
        "gsm8k": {
          "score": 0.06,
          "overall": 0.06
        },
        "gpqa_diamond": {
          "score": 0.09,
          "overall": 0.09
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/dolphin-2.9-llama3-8b": {
      "model_id": "dphn/dolphin-2.9-llama3-8b",
      "model_name": "dphn/dolphin-2.9-llama3-8b",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T05:41:46.570064",
      "end_time": "2026-01-31T05:55:46.575260",
      "duration_seconds": 840.01,
      "vllm_pid": 2612390,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4584,
          "overall": 0.4584
        },
        "mmlu_pro": {
          "score": 0.2829,
          "overall": 0.2829
        },
        "math_500": {
          "score": 0.2933,
          "overall": 0.2933
        },
        "gsm8k": {
          "score": 0.81,
          "overall": 0.81
        },
        "gpqa_diamond": {
          "score": 0.27,
          "overall": 0.27
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-1.1-7b-it": {
      "model_id": "google/gemma-1.1-7b-it",
      "model_name": "google/gemma-1.1-7b-it",
      "size_bytes": 8537680896,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T11:12:53.413380",
      "end_time": "2026-01-31T11:24:05.071360",
      "duration_seconds": 671.66,
      "vllm_pid": 3241127,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5363,
          "overall": 0.5363
        },
        "mmlu_pro": {
          "score": 0.2829,
          "overall": 0.2829
        },
        "math_500": {
          "score": 0.224,
          "overall": 0.224
        },
        "gsm8k": {
          "score": 0.17,
          "overall": 0.17
        },
        "gpqa_diamond": {
          "score": 0.33,
          "overall": 0.33
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-3-1b-it": {
      "model_id": "google/gemma-3-1b-it",
      "model_name": "google/gemma-3-1b-it",
      "size_bytes": 999885952,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T00:24:05.805082",
      "end_time": "2026-01-31T00:33:33.230731",
      "duration_seconds": 567.43,
      "vllm_pid": 2008304,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4151,
          "overall": 0.4151
        },
        "mmlu_pro": {
          "score": 0.18,
          "overall": 0.18
        },
        "math_500": {
          "score": 0.4642,
          "overall": 0.4642
        },
        "gsm8k": {
          "score": 0.63,
          "overall": 0.63
        },
        "gpqa_diamond": {
          "score": 0.33,
          "overall": 0.33
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-3-270m-it": {
      "model_id": "google/gemma-3-270m-it",
      "model_name": "google/gemma-3-270m-it",
      "size_bytes": 268098176,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T04:42:20.064391",
      "end_time": "2026-01-31T04:46:10.860454",
      "duration_seconds": 230.8,
      "vllm_pid": 2500989,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.0428,
          "overall": 0.0428
        },
        "mmlu_pro": {
          "score": 0.0543,
          "overall": 0.0543
        },
        "math_500": {
          "score": 0.0878,
          "overall": 0.0878
        },
        "gsm8k": {
          "score": 0.15,
          "overall": 0.15
        },
        "gpqa_diamond": {
          "score": 0.12,
          "overall": 0.12
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-7b-it": {
      "model_id": "google/gemma-7b-it",
      "model_name": "google/gemma-7b-it",
      "size_bytes": 8537680896,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T10:45:09.152733",
      "end_time": "2026-01-30T10:56:44.978928",
      "duration_seconds": 695.83,
      "vllm_pid": 473706,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3272,
          "overall": 0.3272
        },
        "mmlu_pro": {
          "score": 0.095,
          "overall": 0.095
        },
        "math_500": {
          "score": 0.0462,
          "overall": 0.0462
        },
        "gsm8k": {
          "score": 0.0,
          "overall": 0.0
        },
        "gpqa_diamond": {
          "score": 0.14,
          "overall": 0.14
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "gorilla-llm/gorilla-openfunctions-v2": {
      "model_id": "gorilla-llm/gorilla-openfunctions-v2",
      "model_name": "gorilla-llm/gorilla-openfunctions-v2",
      "size_bytes": 27641655714,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-29T14:00:24.253331",
      "end_time": "2026-01-29T14:07:43.560442",
      "duration_seconds": 439.31,
      "vllm_pid": 2312643,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.2058,
          "overall": 0.2058
        },
        "mmlu_pro": {
          "score": 0.1779,
          "overall": 0.1779
        },
        "math_500": {
          "score": 0.1986,
          "overall": 0.1986
        },
        "gsm8k": {
          "score": 0.29,
          "overall": 0.29
        },
        "gpqa_diamond": {
          "score": 0.2,
          "overall": 0.2
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "gradientai/Llama-3-8B-Instruct-262k": {
      "model_id": "gradientai/Llama-3-8B-Instruct-262k",
      "model_name": "gradientai/Llama-3-8B-Instruct-262k",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T13:19:06.883534",
      "end_time": "2026-01-31T13:32:51.927883",
      "duration_seconds": 825.04,
      "vllm_pid": 3483141,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4398,
          "overall": 0.4398
        },
        "mmlu_pro": {
          "score": 0.3236,
          "overall": 0.3236
        },
        "math_500": {
          "score": 0.2217,
          "overall": 0.2217
        },
        "gsm8k": {
          "score": 0.63,
          "overall": 0.63
        },
        "gpqa_diamond": {
          "score": 0.19,
          "overall": 0.19
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-small": {
      "model_id": "ibm-granite/granite-4.0-h-small",
      "model_name": "ibm-granite/granite-4.0-h-small",
      "size_bytes": 32207337984,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T11:57:09.635274",
      "end_time": "2026-01-31T12:34:08.545840",
      "duration_seconds": 2218.91,
      "vllm_pid": 3328374,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8002,
          "overall": 0.8002
        },
        "mmlu_pro": {
          "score": 0.635,
          "overall": 0.635
        },
        "math_500": {
          "score": 0.8106,
          "overall": 0.8106
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.44,
          "overall": 0.44
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-micro": {
      "model_id": "ibm-granite/granite-4.0-micro",
      "model_name": "ibm-granite/granite-4.0-micro",
      "size_bytes": 3402836480,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T13:20:59.594970",
      "end_time": "2026-01-31T13:30:44.002385",
      "duration_seconds": 584.41,
      "vllm_pid": 1532569,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6528,
          "overall": 0.6528
        },
        "mmlu_pro": {
          "score": 0.4471,
          "overall": 0.4471
        },
        "math_500": {
          "score": 0.6351,
          "overall": 0.6351
        },
        "gsm8k": {
          "score": 0.83,
          "overall": 0.83
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "janhq/Jan-v1-4B": {
      "model_id": "janhq/Jan-v1-4B",
      "model_name": "janhq/Jan-v1-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T07:00:06.687255",
      "end_time": "2026-01-31T08:10:53.036819",
      "duration_seconds": 4246.35,
      "vllm_pid": 798897,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7205,
          "overall": 0.7205
        },
        "mmlu_pro": {
          "score": 0.4121,
          "overall": 0.4121
        },
        "math_500": {
          "score": 0.5012,
          "overall": 0.5012
        },
        "gsm8k": {
          "score": 0.93,
          "overall": 0.93
        },
        "gpqa_diamond": {
          "score": 0.12,
          "overall": 0.12
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jinaai/ReaderLM-v2": {
      "model_id": "jinaai/ReaderLM-v2",
      "model_name": "jinaai/ReaderLM-v2",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T01:36:39.222009",
      "end_time": "2026-01-31T01:45:38.690316",
      "duration_seconds": 539.47,
      "vllm_pid": 2142379,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3025,
          "overall": 0.3025
        },
        "mmlu_pro": {
          "score": 0.1371,
          "overall": 0.1371
        },
        "math_500": {
          "score": 0.127,
          "overall": 0.127
        },
        "gsm8k": {
          "score": 0.33,
          "overall": 0.33
        },
        "gpqa_diamond": {
          "score": 0.24,
          "overall": 0.24
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jinaai/reader-lm-1.5b": {
      "model_id": "jinaai/reader-lm-1.5b",
      "model_name": "jinaai/reader-lm-1.5b",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T03:20:10.337567",
      "end_time": "2026-01-31T03:31:30.846022",
      "duration_seconds": 680.51,
      "vllm_pid": 2339220,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.1181,
          "overall": 0.1181
        },
        "mmlu_pro": {
          "score": 0.0607,
          "overall": 0.0607
        },
        "math_500": {
          "score": 0.0393,
          "overall": 0.0393
        },
        "gsm8k": {
          "score": 0.26,
          "overall": 0.26
        },
        "gpqa_diamond": {
          "score": 0.13,
          "overall": 0.13
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mattshumer/Reflection-Llama-3.1-70B": {
      "model_id": "mattshumer/Reflection-Llama-3.1-70B",
      "model_name": "mattshumer/Reflection-Llama-3.1-70B",
      "size_bytes": 70553804800,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T07:26:05.218652",
      "end_time": "2026-01-30T09:01:28.029565",
      "duration_seconds": 5722.81,
      "vllm_pid": 97319,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5268,
          "overall": 0.5268
        },
        "mmlu_pro": {
          "score": 0.4279,
          "overall": 0.4279
        },
        "math_500": {
          "score": 0.4827,
          "overall": 0.4827
        },
        "gsm8k": {
          "score": 0.34,
          "overall": 0.34
        },
        "gpqa_diamond": {
          "score": 0.3,
          "overall": 0.3
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-3.1-8B-Instruct": {
      "model_id": "meta-llama/Llama-3.1-8B-Instruct",
      "model_name": "meta-llama/Llama-3.1-8B-Instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-03T15:20:18.025499",
      "end_time": "2026-02-03T15:49:54.761003",
      "duration_seconds": 1776.74,
      "vllm_pid": 3233217,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.717,
          "overall": 0.717
        },
        "mmlu_pro": {
          "score": 0.4657,
          "overall": 0.4657
        },
        "math_500": {
          "score": 0.5127,
          "overall": 0.5127
        },
        "gsm8k": {
          "score": 0.85,
          "overall": 0.85
        },
        "gpqa_diamond": {
          "score": 0.3,
          "overall": 0.3
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-3.3-70B-Instruct": {
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "model_name": "meta-llama/Llama-3.3-70B-Instruct",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-03T15:20:15.815986",
      "end_time": "2026-02-03T16:17:49.108547",
      "duration_seconds": 3453.29,
      "vllm_pid": 3233206,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8626,
          "overall": 0.8626
        },
        "mmlu_pro": {
          "score": 0.7079,
          "overall": 0.7079
        },
        "math_500": {
          "score": 0.7644,
          "overall": 0.7644
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.47,
          "overall": 0.47
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Meta-Llama-3-8B-Instruct": {
      "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
      "model_name": "meta-llama/Meta-Llama-3-8B-Instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-29T22:24:02.299651",
      "end_time": "2026-01-29T22:40:58.235467",
      "duration_seconds": 1015.94,
      "vllm_pid": 3278669,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6595,
          "overall": 0.6595
        },
        "mmlu_pro": {
          "score": 0.4293,
          "overall": 0.4293
        },
        "math_500": {
          "score": 0.2979,
          "overall": 0.2979
        },
        "gsm8k": {
          "score": 0.81,
          "overall": 0.81
        },
        "gpqa_diamond": {
          "score": 0.32,
          "overall": 0.32
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-medium-128k-instruct": {
      "model_id": "microsoft/Phi-3-medium-128k-instruct",
      "model_name": "microsoft/Phi-3-medium-128k-instruct",
      "size_bytes": 13960238080,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T06:42:42.374752",
      "end_time": "2026-01-31T07:14:05.050829",
      "duration_seconds": 1882.68,
      "vllm_pid": 759120,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7609,
          "overall": 0.7609
        },
        "mmlu_pro": {
          "score": 0.5643,
          "overall": 0.5643
        },
        "math_500": {
          "score": 0.5243,
          "overall": 0.5243
        },
        "gsm8k": {
          "score": 0.91,
          "overall": 0.91
        },
        "gpqa_diamond": {
          "score": 0.31,
          "overall": 0.31
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-mini-128k-instruct": {
      "model_id": "microsoft/Phi-3-mini-128k-instruct",
      "model_name": "microsoft/Phi-3-mini-128k-instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T07:26:05.222563",
      "end_time": "2026-01-30T07:44:30.833079",
      "duration_seconds": 1105.61,
      "vllm_pid": 97317,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.691,
          "overall": 0.691
        },
        "mmlu_pro": {
          "score": 0.48,
          "overall": 0.48
        },
        "math_500": {
          "score": 0.455,
          "overall": 0.455
        },
        "gsm8k": {
          "score": 0.7,
          "overall": 0.7
        },
        "gpqa_diamond": {
          "score": 0.32,
          "overall": 0.32
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-mini-4k-instruct": {
      "model_id": "microsoft/Phi-3-mini-4k-instruct",
      "model_name": "microsoft/Phi-3-mini-4k-instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T09:01:38.068040",
      "end_time": "2026-01-30T09:17:09.275001",
      "duration_seconds": 931.21,
      "vllm_pid": 276456,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6914,
          "overall": 0.6914
        },
        "mmlu_pro": {
          "score": 0.4814,
          "overall": 0.4814
        },
        "math_500": {
          "score": 0.4711,
          "overall": 0.4711
        },
        "gsm8k": {
          "score": 0.81,
          "overall": 0.81
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-vision-128k-instruct": {
      "model_id": "microsoft/Phi-3-vision-128k-instruct",
      "model_name": "microsoft/Phi-3-vision-128k-instruct",
      "size_bytes": 4146621440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:21:10.531569",
      "end_time": "2026-01-30T14:40:37.337908",
      "duration_seconds": 1166.81,
      "vllm_pid": 871305,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5844,
          "overall": 0.5844
        },
        "mmlu_pro": {
          "score": 0.3571,
          "overall": 0.3571
        },
        "math_500": {
          "score": 0.3164,
          "overall": 0.3164
        },
        "gsm8k": {
          "score": 0.76,
          "overall": 0.76
        },
        "gpqa_diamond": {
          "score": 0.19,
          "overall": 0.19
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3.5-MoE-instruct": {
      "model_id": "microsoft/Phi-3.5-MoE-instruct",
      "model_name": "microsoft/Phi-3.5-MoE-instruct",
      "size_bytes": 41873153344,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T04:26:11.272685",
      "end_time": "2026-01-31T04:54:19.453752",
      "duration_seconds": 1688.18,
      "vllm_pid": 2466523,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7958,
          "overall": 0.7958
        },
        "mmlu_pro": {
          "score": 0.5879,
          "overall": 0.5879
        },
        "math_500": {
          "score": 0.6628,
          "overall": 0.6628
        },
        "gsm8k": {
          "score": 0.91,
          "overall": 0.91
        },
        "gpqa_diamond": {
          "score": 0.34,
          "overall": 0.34
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3.5-mini-instruct": {
      "model_id": "microsoft/Phi-3.5-mini-instruct",
      "model_name": "microsoft/Phi-3.5-mini-instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T14:21:11.052962",
      "end_time": "2026-01-30T14:43:39.581709",
      "duration_seconds": 1348.53,
      "vllm_pid": 871317,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7084,
          "overall": 0.7084
        },
        "mmlu_pro": {
          "score": 0.4943,
          "overall": 0.4943
        },
        "math_500": {
          "score": 0.5035,
          "overall": 0.5035
        },
        "gsm8k": {
          "score": 0.87,
          "overall": 0.87
        },
        "gpqa_diamond": {
          "score": 0.34,
          "overall": 0.34
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-mini-instruct": {
      "model_id": "microsoft/Phi-4-mini-instruct",
      "model_name": "microsoft/Phi-4-mini-instruct",
      "size_bytes": 3836021760,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:43:10.723440",
      "end_time": "2026-01-31T02:55:16.845958",
      "duration_seconds": 726.12,
      "vllm_pid": 284759,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7295,
          "overall": 0.7295
        },
        "mmlu_pro": {
          "score": 0.5336,
          "overall": 0.5336
        },
        "math_500": {
          "score": 0.7067,
          "overall": 0.7067
        },
        "gsm8k": {
          "score": 0.91,
          "overall": 0.91
        },
        "gpqa_diamond": {
          "score": 0.38,
          "overall": 0.38
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-reasoning-plus": {
      "model_id": "microsoft/Phi-4-reasoning-plus",
      "model_name": "microsoft/Phi-4-reasoning-plus",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:47:45.790001",
      "end_time": "2026-02-04T23:36:56.512107",
      "duration_seconds": 13750.72,
      "vllm_pid": 2217626,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.2597,
          "overall": 0.2597
        },
        "mmlu_pro": {
          "score": 0.2857,
          "overall": 0.2857
        },
        "math_500": {
          "score": 0.5219,
          "overall": 0.5219
        },
        "gsm8k": {
          "score": 0.68,
          "overall": 0.68
        },
        "gpqa_diamond": {
          "score": 0.11,
          "overall": 0.11
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/UserLM-8b": {
      "model_id": "microsoft/UserLM-8b",
      "model_name": "microsoft/UserLM-8b",
      "size_bytes": 8030269440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T19:08:08.384105",
      "end_time": "2026-02-04T19:26:11.385255",
      "duration_seconds": 1083.0,
      "vllm_pid": 2134256,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.0156,
          "overall": 0.0156
        },
        "mmlu_pro": {
          "score": 0.0521,
          "overall": 0.0521
        },
        "math_500": {
          "score": 0.0,
          "overall": 0.0
        },
        "gsm8k": {
          "score": 0.01,
          "overall": 0.01
        },
        "gpqa_diamond": {
          "score": 0.05,
          "overall": 0.05
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/phi-4": {
      "model_id": "microsoft/phi-4",
      "model_name": "microsoft/phi-4",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T04:14:13.821506",
      "end_time": "2026-01-30T04:54:03.453398",
      "duration_seconds": 2389.63,
      "vllm_pid": 3928718,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8542,
          "overall": 0.8542
        },
        "mmlu_pro": {
          "score": 0.7236,
          "overall": 0.7236
        },
        "math_500": {
          "score": 0.8268,
          "overall": 0.8268
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.68,
          "overall": 0.68
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Ministral-8B-Instruct-2410": {
      "model_id": "mistralai/Ministral-8B-Instruct-2410",
      "model_name": "mistralai/Ministral-8B-Instruct-2410",
      "size_bytes": 8019808256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-03T15:36:08.793578",
      "end_time": "2026-02-03T15:53:16.131881",
      "duration_seconds": 1027.34,
      "vllm_pid": 3269657,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6609,
          "overall": 0.6609
        },
        "mmlu_pro": {
          "score": 0.4407,
          "overall": 0.4407
        },
        "math_500": {
          "score": 0.5912,
          "overall": 0.5912
        },
        "gsm8k": {
          "score": 0.87,
          "overall": 0.87
        },
        "gpqa_diamond": {
          "score": 0.33,
          "overall": 0.33
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-Instruct-v0.1": {
      "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
      "model_name": "mistralai/Mistral-7B-Instruct-v0.1",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T05:20:34.113268",
      "end_time": "2026-01-30T05:37:14.466691",
      "duration_seconds": 1000.35,
      "vllm_pid": 4054892,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3386,
          "overall": 0.3386
        },
        "mmlu_pro": {
          "score": 0.1864,
          "overall": 0.1864
        },
        "math_500": {
          "score": 0.0855,
          "overall": 0.0855
        },
        "gsm8k": {
          "score": 0.33,
          "overall": 0.33
        },
        "gpqa_diamond": {
          "score": 0.22,
          "overall": 0.22
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-Instruct-v0.2": {
      "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
      "model_name": "mistralai/Mistral-7B-Instruct-v0.2",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-29T22:24:02.296156",
      "end_time": "2026-01-29T22:40:16.192588",
      "duration_seconds": 973.9,
      "vllm_pid": 3278668,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.473,
          "overall": 0.473
        },
        "mmlu_pro": {
          "score": 0.2429,
          "overall": 0.2429
        },
        "math_500": {
          "score": 0.1316,
          "overall": 0.1316
        },
        "gsm8k": {
          "score": 0.37,
          "overall": 0.37
        },
        "gpqa_diamond": {
          "score": 0.22,
          "overall": 0.22
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-Instruct-v0.3": {
      "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
      "model_name": "mistralai/Mistral-7B-Instruct-v0.3",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T04:14:13.820431",
      "end_time": "2026-01-30T04:29:19.394162",
      "duration_seconds": 905.57,
      "vllm_pid": 3928720,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3186,
          "overall": 0.3186
        },
        "mmlu_pro": {
          "score": 0.2329,
          "overall": 0.2329
        },
        "math_500": {
          "score": 0.1455,
          "overall": 0.1455
        },
        "gsm8k": {
          "score": 0.48,
          "overall": 0.48
        },
        "gpqa_diamond": {
          "score": 0.12,
          "overall": 0.12
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Nemo-Instruct-2407": {
      "model_id": "mistralai/Mistral-Nemo-Instruct-2407",
      "model_name": "mistralai/Mistral-Nemo-Instruct-2407",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-30T07:26:05.640079",
      "end_time": "2026-01-30T07:50:01.079852",
      "duration_seconds": 1435.44,
      "vllm_pid": 97324,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6649,
          "overall": 0.6649
        },
        "mmlu_pro": {
          "score": 0.4443,
          "overall": 0.4443
        },
        "math_500": {
          "score": 0.4642,
          "overall": 0.4642
        },
        "gsm8k": {
          "score": 0.94,
          "overall": 0.94
        },
        "gpqa_diamond": {
          "score": 0.3,
          "overall": 0.3
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlabonne/NeuralDaredevil-8B-abliterated": {
      "model_id": "mlabonne/NeuralDaredevil-8B-abliterated",
      "model_name": "mlabonne/NeuralDaredevil-8B-abliterated",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T13:20:19.405287",
      "end_time": "2026-01-31T13:33:45.154517",
      "duration_seconds": 805.75,
      "vllm_pid": 3485810,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6803,
          "overall": 0.6803
        },
        "mmlu_pro": {
          "score": 0.4371,
          "overall": 0.4371
        },
        "math_500": {
          "score": 0.3118,
          "overall": 0.3118
        },
        "gsm8k": {
          "score": 0.77,
          "overall": 0.77
        },
        "gpqa_diamond": {
          "score": 0.34,
          "overall": 0.34
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Kimi-Dev-72B": {
      "model_id": "moonshotai/Kimi-Dev-72B",
      "model_name": "moonshotai/Kimi-Dev-72B",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-03T16:17:49.286680",
      "end_time": "2026-02-03T18:08:38.882202",
      "duration_seconds": 6649.6,
      "vllm_pid": 3350277,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8591,
          "overall": 0.8591
        },
        "mmlu_pro": {
          "score": 0.67,
          "overall": 0.67
        },
        "math_500": {
          "score": 0.8222,
          "overall": 0.8222
        },
        "gsm8k": {
          "score": 0.98,
          "overall": 0.98
        },
        "gpqa_diamond": {
          "score": 0.44,
          "overall": 0.44
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "numind/NuExtract-1.5": {
      "model_id": "numind/NuExtract-1.5",
      "model_name": "numind/NuExtract-1.5",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T20:39:42.121112",
      "end_time": "2026-02-04T21:00:15.000618",
      "duration_seconds": 1232.88,
      "vllm_pid": 2320900,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6435,
          "overall": 0.6435
        },
        "mmlu_pro": {
          "score": 0.415,
          "overall": 0.415
        },
        "math_500": {
          "score": 0.4042,
          "overall": 0.4042
        },
        "gsm8k": {
          "score": 0.82,
          "overall": 0.82
        },
        "gpqa_diamond": {
          "score": 0.32,
          "overall": 0.32
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
      "model_id": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
      "model_name": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-30T04:14:13.821813",
      "end_time": "2026-01-30T05:20:34.078821",
      "duration_seconds": 3980.26,
      "vllm_pid": 3928719,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.853,
          "overall": 0.853
        },
        "mmlu_pro": {
          "score": 0.6907,
          "overall": 0.6907
        },
        "math_500": {
          "score": 0.7252,
          "overall": 0.7252
        },
        "gsm8k": {
          "score": 0.92,
          "overall": 0.92
        },
        "gpqa_diamond": {
          "score": 0.52,
          "overall": 0.52
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3_3-Nemotron-Super-49B-v1": {
      "model_id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
      "model_name": "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
      "size_bytes": 49867145216,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T09:40:02.810590",
      "end_time": "2026-01-31T10:29:22.782789",
      "duration_seconds": 2959.97,
      "vllm_pid": 1114636,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8439,
          "overall": 0.8439
        },
        "mmlu_pro": {
          "score": 0.6771,
          "overall": 0.6771
        },
        "math_500": {
          "score": 0.7875,
          "overall": 0.7875
        },
        "gsm8k": {
          "score": 0.99,
          "overall": 0.99
        },
        "gpqa_diamond": {
          "score": 0.53,
          "overall": 0.53
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama3-ChatQA-1.5-70B": {
      "model_id": "nvidia/Llama3-ChatQA-1.5-70B",
      "model_name": "nvidia/Llama3-ChatQA-1.5-70B",
      "size_bytes": 70553711616,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T08:11:13.075693",
      "end_time": "2026-01-31T08:27:30.999464",
      "duration_seconds": 977.92,
      "vllm_pid": 941765,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3152,
          "overall": 0.3152
        },
        "mmlu_pro": {
          "score": 0.2057,
          "overall": 0.2057
        },
        "math_500": {
          "score": 0.3487,
          "overall": 0.3487
        },
        "gsm8k": {
          "score": 0.6,
          "overall": 0.6
        },
        "gpqa_diamond": {
          "score": 0.23,
          "overall": 0.23
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama3-ChatQA-1.5-8B": {
      "model_id": "nvidia/Llama3-ChatQA-1.5-8B",
      "model_name": "nvidia/Llama3-ChatQA-1.5-8B",
      "size_bytes": 8030263296,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T04:26:11.275434",
      "end_time": "2026-01-31T04:35:02.315258",
      "duration_seconds": 531.04,
      "vllm_pid": 489271,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.1937,
          "overall": 0.1937
        },
        "mmlu_pro": {
          "score": 0.2071,
          "overall": 0.2071
        },
        "math_500": {
          "score": 0.157,
          "overall": 0.157
        },
        "gsm8k": {
          "score": 0.51,
          "overall": 0.51
        },
        "gpqa_diamond": {
          "score": 0.24,
          "overall": 0.24
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/NVIDIA-Nemotron-Nano-9B-v2": {
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "model_name": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "size_bytes": 8888227328,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T05:55:56.764223",
      "end_time": "2026-01-31T08:38:37.757390",
      "duration_seconds": 9760.99,
      "vllm_pid": 2640147,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.0,
          "overall": 0.0
        },
        "mmlu_pro": {
          "score": 0.0,
          "overall": 0.0
        },
        "math_500": {
          "score": 0.0,
          "overall": 0.0
        },
        "gsm8k": {
          "score": 0.0,
          "overall": 0.0
        },
        "gpqa_diamond": {
          "score": 0.0,
          "overall": 0.0
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-Orchestrator-8B": {
      "model_id": "nvidia/Nemotron-Orchestrator-8B",
      "model_name": "nvidia/Nemotron-Orchestrator-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-03T15:36:28.933275",
      "end_time": "2026-02-03T16:56:29.483419",
      "duration_seconds": 4800.55,
      "vllm_pid": 3270296,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7633,
          "overall": 0.7633
        },
        "mmlu_pro": {
          "score": 0.5014,
          "overall": 0.5014
        },
        "math_500": {
          "score": 0.2979,
          "overall": 0.2979
        },
        "gsm8k": {
          "score": 0.68,
          "overall": 0.68
        },
        "gpqa_diamond": {
          "score": 0.19,
          "overall": 0.19
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openai/gpt-oss-20b": {
      "model_id": "openai/gpt-oss-20b",
      "model_name": "openai/gpt-oss-20b",
      "size_bytes": 21511953984,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-29T22:24:02.333251",
      "end_time": "2026-01-29T22:53:15.398092",
      "duration_seconds": 1753.06,
      "vllm_pid": 3278670,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8446,
          "overall": 0.8446
        },
        "mmlu_pro": {
          "score": 0.6714,
          "overall": 0.6714
        },
        "math_500": {
          "score": 0.7829,
          "overall": 0.7829
        },
        "gsm8k": {
          "score": 0.96,
          "overall": 0.96
        },
        "gpqa_diamond": {
          "score": 0.41,
          "overall": 0.41
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/MiniCPM-2B-sft-fp32": {
      "model_id": "openbmb/MiniCPM-2B-sft-fp32",
      "model_name": "openbmb/MiniCPM-2B-sft-fp32",
      "size_bytes": 21801203700,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-29T13:45:35.760912",
      "end_time": "2026-01-29T13:51:04.613191",
      "duration_seconds": 328.85,
      "vllm_pid": 2271330,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.443,
          "overall": 0.443
        },
        "mmlu_pro": {
          "score": 0.1586,
          "overall": 0.1586
        },
        "math_500": {
          "score": 0.1986,
          "overall": 0.1986
        },
        "gsm8k": {
          "score": 0.54,
          "overall": 0.54
        },
        "gpqa_diamond": {
          "score": 0.26,
          "overall": 0.26
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/MiniCPM3-4B": {
      "model_id": "openbmb/MiniCPM3-4B",
      "model_name": "openbmb/MiniCPM3-4B",
      "size_bytes": 16297031550,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-02-04T17:35:20.441130",
      "end_time": "2026-02-04T18:22:33.892908",
      "duration_seconds": 2833.45,
      "vllm_pid": 1944526,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6895,
          "overall": 0.6895
        },
        "mmlu_pro": {
          "score": 0.4221,
          "overall": 0.4221
        },
        "math_500": {
          "score": 0.4896,
          "overall": 0.4896
        },
        "gsm8k": {
          "score": 0.8,
          "overall": 0.8
        },
        "gpqa_diamond": {
          "score": 0.33,
          "overall": 0.33
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/MiniCPM4.1-8B": {
      "model_id": "openbmb/MiniCPM4.1-8B",
      "model_name": "openbmb/MiniCPM4.1-8B",
      "size_bytes": 8185253888,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T06:42:42.374596",
      "end_time": "2026-01-31T08:06:48.955348",
      "duration_seconds": 5046.58,
      "vllm_pid": 759124,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6963,
          "overall": 0.6963
        },
        "mmlu_pro": {
          "score": 0.3607,
          "overall": 0.3607
        },
        "math_500": {
          "score": 0.3372,
          "overall": 0.3372
        },
        "gsm8k": {
          "score": 0.78,
          "overall": 0.78
        },
        "gpqa_diamond": {
          "score": 0.11,
          "overall": 0.11
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "osmosis-ai/Osmosis-Structure-0.6B": {
      "model_id": "osmosis-ai/Osmosis-Structure-0.6B",
      "model_name": "osmosis-ai/Osmosis-Structure-0.6B",
      "size_bytes": 596049920,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T06:42:42.374033",
      "end_time": "2026-01-31T06:44:32.210774",
      "duration_seconds": 109.84,
      "vllm_pid": 2730240,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.2404,
          "overall": 0.2404
        },
        "mmlu_pro": {
          "score": 0.1129,
          "overall": 0.1129
        },
        "math_500": {
          "score": 0.0185,
          "overall": 0.0185
        },
        "gsm8k": {
          "score": 0.0,
          "overall": 0.0
        },
        "gpqa_diamond": {
          "score": 0.18,
          "overall": 0.18
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "qihoo360/TinyR1-32B-Preview": {
      "model_id": "qihoo360/TinyR1-32B-Preview",
      "model_name": "qihoo360/TinyR1-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-01-31T08:24:01.693984",
      "end_time": "2026-01-31T10:21:33.625535",
      "duration_seconds": 7051.93,
      "vllm_pid": 2920665,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.7388,
          "overall": 0.7388
        },
        "mmlu_pro": {
          "score": 0.4379,
          "overall": 0.4379
        },
        "math_500": {
          "score": 0.6212,
          "overall": 0.6212
        },
        "gsm8k": {
          "score": 0.91,
          "overall": 0.91
        },
        "gpqa_diamond": {
          "score": 0.15,
          "overall": 0.15
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sarvamai/sarvam-m": {
      "model_id": "sarvamai/sarvam-m",
      "model_name": "sarvamai/sarvam-m",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-31T09:40:02.813803",
      "end_time": "2026-01-31T11:12:33.373948",
      "duration_seconds": 5550.56,
      "vllm_pid": 1114635,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.8028,
          "overall": 0.8028
        },
        "mmlu_pro": {
          "score": 0.5614,
          "overall": 0.5614
        },
        "math_500": {
          "score": 0.6836,
          "overall": 0.6836
        },
        "gsm8k": {
          "score": 0.92,
          "overall": 0.92
        },
        "gpqa_diamond": {
          "score": 0.22,
          "overall": 0.22
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "shenzhi-wang/Llama3-8B-Chinese-Chat": {
      "model_id": "shenzhi-wang/Llama3-8B-Chinese-Chat",
      "model_name": "shenzhi-wang/Llama3-8B-Chinese-Chat",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:41:44.011467",
      "end_time": "2026-01-31T02:54:36.347535",
      "duration_seconds": 772.34,
      "vllm_pid": 281049,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.5637,
          "overall": 0.5637
        },
        "mmlu_pro": {
          "score": 0.3414,
          "overall": 0.3414
        },
        "math_500": {
          "score": 0.3049,
          "overall": 0.3049
        },
        "gsm8k": {
          "score": 0.73,
          "overall": 0.73
        },
        "gpqa_diamond": {
          "score": 0.22,
          "overall": 0.22
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "shenzhi-wang/Llama3.1-8B-Chinese-Chat": {
      "model_id": "shenzhi-wang/Llama3.1-8B-Chinese-Chat",
      "model_name": "shenzhi-wang/Llama3.1-8B-Chinese-Chat",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T13:19:06.880724",
      "end_time": "2026-01-31T13:34:13.200664",
      "duration_seconds": 906.32,
      "vllm_pid": 3483140,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6482,
          "overall": 0.6482
        },
        "mmlu_pro": {
          "score": 0.4386,
          "overall": 0.4386
        },
        "math_500": {
          "score": 0.4388,
          "overall": 0.4388
        },
        "gsm8k": {
          "score": 0.81,
          "overall": 0.81
        },
        "gpqa_diamond": {
          "score": 0.36,
          "overall": 0.36
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "stabilityai/stablelm-zephyr-3b": {
      "model_id": "stabilityai/stablelm-zephyr-3b",
      "model_name": "stabilityai/stablelm-zephyr-3b",
      "size_bytes": 2795443200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T13:19:52.154583",
      "end_time": "2026-01-31T13:45:32.651374",
      "duration_seconds": 1540.5,
      "vllm_pid": 3483995,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.2384,
          "overall": 0.2384
        },
        "mmlu_pro": {
          "score": 0.13,
          "overall": 0.13
        },
        "math_500": {
          "score": 0.1316,
          "overall": 0.1316
        },
        "gsm8k": {
          "score": 0.44,
          "overall": 0.44
        },
        "gpqa_diamond": {
          "score": 0.17,
          "overall": 0.17
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "swiss-ai/Apertus-8B-Instruct-2509": {
      "model_id": "swiss-ai/Apertus-8B-Instruct-2509",
      "model_name": "swiss-ai/Apertus-8B-Instruct-2509",
      "size_bytes": 8053338176,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-04T18:22:59.210012",
      "end_time": "2026-02-04T18:45:52.354412",
      "duration_seconds": 1373.14,
      "vllm_pid": 2043677,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.616,
          "overall": 0.616
        },
        "mmlu_pro": {
          "score": 0.3593,
          "overall": 0.3593
        },
        "math_500": {
          "score": 0.1824,
          "overall": 0.1824
        },
        "gsm8k": {
          "score": 0.52,
          "overall": 0.52
        },
        "gpqa_diamond": {
          "score": 0.26,
          "overall": 0.26
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-1.8B-Instruct": {
      "model_id": "tencent/Hunyuan-1.8B-Instruct",
      "model_name": "tencent/Hunyuan-1.8B-Instruct",
      "size_bytes": 1791080448,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T03:20:10.338141",
      "end_time": "2026-01-31T04:07:13.976507",
      "duration_seconds": 2823.64,
      "vllm_pid": 2339219,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.4048,
          "overall": 0.4048
        },
        "mmlu_pro": {
          "score": 0.2186,
          "overall": 0.2186
        },
        "math_500": {
          "score": 0.2818,
          "overall": 0.2818
        },
        "gsm8k": {
          "score": 0.66,
          "overall": 0.66
        },
        "gpqa_diamond": {
          "score": 0.07,
          "overall": 0.07
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-MT-7B": {
      "model_id": "tencent/Hunyuan-MT-7B",
      "model_name": "tencent/Hunyuan-MT-7B",
      "size_bytes": 8030269440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-31T02:41:42.798130",
      "end_time": "2026-01-31T03:04:48.930771",
      "duration_seconds": 1386.13,
      "vllm_pid": 2263690,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.3432,
          "overall": 0.3432
        },
        "mmlu_pro": {
          "score": 0.1814,
          "overall": 0.1814
        },
        "math_500": {
          "score": 0.1986,
          "overall": 0.1986
        },
        "gsm8k": {
          "score": 0.57,
          "overall": 0.57
        },
        "gpqa_diamond": {
          "score": 0.18,
          "overall": 0.18
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tngtech/DeepSeek-R1T-Chimera": {
      "model_id": "tngtech/DeepSeek-R1T-Chimera",
      "model_name": "tngtech/DeepSeek-R1T-Chimera",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "success",
      "start_time": "2026-02-03T00:05:58.339373",
      "end_time": "2026-02-03T04:42:29.264534",
      "duration_seconds": 16590.93,
      "vllm_pid": 3818078,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.9042,
          "overall": 0.9042
        },
        "mmlu_pro": {
          "score": 0.7636,
          "overall": 0.7636
        },
        "math_500": {
          "score": 0.8152,
          "overall": 0.8152
        },
        "gsm8k": {
          "score": 0.97,
          "overall": 0.97
        },
        "gpqa_diamond": {
          "score": 0.48,
          "overall": 0.48
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "upstage/SOLAR-10.7B-Instruct-v1.0": {
      "model_id": "upstage/SOLAR-10.7B-Instruct-v1.0",
      "model_name": "upstage/SOLAR-10.7B-Instruct-v1.0",
      "size_bytes": 10731524096,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-02-03T15:20:15.817950",
      "end_time": "2026-02-03T15:35:58.564767",
      "duration_seconds": 942.75,
      "vllm_pid": 3233205,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": {
        "mmlu": {
          "score": 0.6177,
          "overall": 0.6177
        },
        "mmlu_pro": {
          "score": 0.3343,
          "overall": 0.3343
        },
        "math_500": {
          "score": 0.224,
          "overall": 0.224
        },
        "gsm8k": {
          "score": 0.74,
          "overall": 0.74
        },
        "gpqa_diamond": {
          "score": 0.28,
          "overall": 0.28
        }
      },
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "1bitLLM/bitnet_b1_58-3B": {
      "model_id": "1bitLLM/bitnet_b1_58-3B",
      "model_name": "1bitLLM/bitnet_b1_58-3B",
      "size_bytes": 3324389140,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T11:52:09.170271",
      "end_time": "2026-02-08T11:52:25.957733",
      "duration_seconds": 16.79,
      "vllm_pid": 3588115,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m   Value error, Model architectures ['BitnetForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoEForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'Grok1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'IQuestCoderForCausalLM', 'IQuestLoopCoderForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'MiMoV2FlashForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguProMoEV2ForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaBidirectionalModel', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'JinaVLForRanking', 'LlamaBidirectionalForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GlmAsrForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'IsaacForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KananaVForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Lfm2VlForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'VoxtralStreamingGeneration', 'NemotronParseForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'ExaoneMoeMTP', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=3588115)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 16.79
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "BAAI/bge-reranker-v2-m3": {
      "model_id": "BAAI/bge-reranker-v2-m3",
      "model_name": "BAAI/bge-reranker-v2-m3",
      "size_bytes": 567755777,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T11:52:35.987196",
      "end_time": "2026-02-08T11:52:56.116352",
      "duration_seconds": 20.13,
      "vllm_pid": 3588919,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m   Value error, User-specified max_model_len (8194) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1. VLLM_ALLOW_LONG_MAX_MODEL_LEN must be used with extreme caution. If the model uses relative position encoding (RoPE), positions exceeding derived_max_model_len lead to nan. If the model uses absolute position encoding, positions exceeding derived_max_model_len will cause a CUDA array out-of-bounds error. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=3588919)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.13
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "Nexusflow/NexusRaven-V2-13B": {
      "model_id": "Nexusflow/NexusRaven-V2-13B",
      "model_name": "Nexusflow/NexusRaven-V2-13B",
      "size_bytes": 52068067950,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-08T11:52:09.004890",
      "end_time": "2026-02-08T12:01:13.285509",
      "duration_seconds": 544.28,
      "vllm_pid": 1594049,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.3084,
          "score": 0.3084
        },
        "mmlu_pro": {
          "overall": 0.1486,
          "score": 0.1486
        },
        "math_500": {
          "overall": 0.0162,
          "score": 0.0162
        },
        "gsm8k": {
          "overall": 0.0,
          "score": 0.0
        },
        "gpqa_diamond": {
          "overall": 0.21,
          "score": 0.21
        }
      },
      "eval_output_dir": "outputs/Nexusflow_NexusRaven-V2-13B",
      "timing": {
        "server_start_seconds": 80.04,
        "test_execution_seconds": 451.9,
        "total_seconds": 544.28
      },
      "gpu_memory_gb": 1025.99,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "ai-forever/mGPT": {
      "model_id": "ai-forever/mGPT",
      "model_name": "ai-forever/mGPT",
      "size_bytes": 16109723293,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-02-08T11:52:09.174505",
      "end_time": "2026-02-08T12:05:19.863527",
      "duration_seconds": 790.69,
      "vllm_pid": 3588104,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.0688,
          "score": 0.0688
        },
        "mmlu_pro": {
          "overall": 0.0186,
          "score": 0.0186
        },
        "math_500": {
          "overall": 0.0069,
          "score": 0.0069
        },
        "gsm8k": {
          "overall": 0.01,
          "score": 0.01
        },
        "gpqa_diamond": {
          "overall": 0.04,
          "score": 0.04
        }
      },
      "eval_output_dir": "outputs/ai-forever_mGPT",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 721.72,
        "total_seconds": 790.69
      },
      "gpu_memory_gb": 900.62,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "WhiteRabbitNeo/WhiteRabbitNeo-13B-v1": {
      "model_id": "WhiteRabbitNeo/WhiteRabbitNeo-13B-v1",
      "model_name": "WhiteRabbitNeo/WhiteRabbitNeo-13B-v1",
      "size_bytes": 52066287694,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-08T11:52:09.004962",
      "end_time": "2026-02-08T12:13:07.526938",
      "duration_seconds": 1258.52,
      "vllm_pid": 1594046,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.1828,
          "score": 0.1828
        },
        "mmlu_pro": {
          "overall": 0.1229,
          "score": 0.1229
        },
        "math_500": {
          "overall": 0.0393,
          "score": 0.0393
        },
        "gsm8k": {
          "overall": 0.02,
          "score": 0.02
        },
        "gpqa_diamond": {
          "overall": 0.09,
          "score": 0.09
        }
      },
      "eval_output_dir": "outputs/WhiteRabbitNeo_WhiteRabbitNeo-13B-v1",
      "timing": {
        "server_start_seconds": 75.03,
        "test_execution_seconds": 1168.29,
        "total_seconds": 1258.52
      },
      "gpu_memory_gb": 1029.41,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "tiiuae/falcon-40b": {
      "model_id": "tiiuae/falcon-40b",
      "model_name": "tiiuae/falcon-40b",
      "size_bytes": 41835970560,
      "tp_size": 4,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:13:17.550985",
      "end_time": "2026-02-08T12:13:37.722047",
      "duration_seconds": 20.17,
      "vllm_pid": 1637258,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m [2026-02-08 12:13:28] WARNING logging.py:328: \n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m \n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 214, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 205, in from_vllm_config\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 112, in __init__\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     tokenizer = cached_tokenizer_from_config(self.model_config)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/registry.py\", line 231, in cached_tokenizer_from_config\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return cached_get_tokenizer(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/registry.py\", line 214, in get_tokenizer\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     tokenizer = tokenizer_cls_.from_pretrained(tokenizer_name, *args, **kwargs)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/hf.py\", line 79, in from_pretrained\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 1175, in from_pretrained\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2113, in from_pretrained\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     return cls._from_pretrained(\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2359, in _from_pretrained\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     tokenizer = cls(*init_inputs, **init_kwargs)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 139, in __init__\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py\", line 1857, in convert_slow_tokenizer\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m     elif transformer_tokenizer.vocab_file.endswith(\"tekken.json\"):\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1637258)\u001b[0;0m AttributeError: 'NoneType' object has no attribute 'endswith'\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.17
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "allenai/OLMo-7B": {
      "model_id": "allenai/OLMo-7B",
      "model_name": "allenai/OLMo-7B",
      "size_bytes": 6888095744,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:13:48.655092",
      "end_time": "2026-02-08T12:14:10.318015",
      "duration_seconds": 21.66,
      "vllm_pid": 1638089,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m Encountered exception while importing hf_olmo: No module named 'hf_olmo'\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 473, in __post_init__\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 633, in get_config\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 166, in parse\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 427, in get_cached_module_file\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     modules_needed = check_imports(resolved_module_file)\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 260, in check_imports\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=1638089)\u001b[0;0m ImportError: This modeling file requires the following packages that were not found in your environment: hf_olmo. Run `pip install hf_olmo`\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 21.66
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "answerdotai/ModernBERT-base": {
      "model_id": "answerdotai/ModernBERT-base",
      "model_name": "answerdotai/ModernBERT-base",
      "size_bytes": 149655232,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:14:20.332161",
      "end_time": "2026-02-08T12:14:40.447736",
      "duration_seconds": 20.12,
      "vllm_pid": 1639500,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m   Value error, Model architectures ['ModernBertForMaskedLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoEForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'Grok1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'IQuestCoderForCausalLM', 'IQuestLoopCoderForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'MiMoV2FlashForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguProMoEV2ForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaBidirectionalModel', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'JinaVLForRanking', 'LlamaBidirectionalForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GlmAsrForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'IsaacForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KananaVForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Lfm2VlForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'VoxtralStreamingGeneration', 'NemotronParseForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'ExaoneMoeMTP', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1639500)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.12
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "answerdotai/ModernBERT-large": {
      "model_id": "answerdotai/ModernBERT-large",
      "model_name": "answerdotai/ModernBERT-large",
      "size_bytes": 395881664,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:14:50.461073",
      "end_time": "2026-02-08T12:15:05.569708",
      "duration_seconds": 15.11,
      "vllm_pid": 1640707,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m   Value error, Model architectures ['ModernBertForMaskedLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoEForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'Grok1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'IQuestCoderForCausalLM', 'IQuestLoopCoderForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'MiMoV2FlashForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguProMoEV2ForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaBidirectionalModel', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'JinaVLForRanking', 'LlamaBidirectionalForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GlmAsrForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'IsaacForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KananaVForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Lfm2VlForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'VoxtralStreamingGeneration', 'NemotronParseForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'ExaoneMoeMTP', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1640707)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.11
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "apple/DCLM-7B": {
      "model_id": "apple/DCLM-7B",
      "model_name": "apple/DCLM-7B",
      "size_bytes": 6889674752,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:15:15.581788",
      "end_time": "2026-02-08T12:15:30.762491",
      "duration_seconds": 15.18,
      "vllm_pid": 1642163,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m   Value error, The checkpoint you are trying to load has model type `openlm` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m \n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1642163)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.18
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "apple/OpenELM-3B-Instruct": {
      "model_id": "apple/OpenELM-3B-Instruct",
      "model_name": "apple/OpenELM-3B-Instruct",
      "size_bytes": 3036647424,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:15:40.774804",
      "end_time": "2026-02-08T12:15:55.892696",
      "duration_seconds": 15.12,
      "vllm_pid": 1642348,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m total_num_kv_heads\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m   Input should be a valid integer [type=int_type, input_value=[3, 3, 3, 3, 4, 4, 4, 4, ... 5, 5, 6, 6, 6, 6, 6, 6], input_type=list]\n\u001b[0;36m(APIServer pid=1642348)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/int_type\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.12
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "ByteDance-Seed/Seed-X-PPO-7B": {
      "model_id": "ByteDance-Seed/Seed-X-PPO-7B",
      "model_name": "ByteDance-Seed/Seed-X-PPO-7B",
      "size_bytes": 15028942905,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T11:53:06.143029",
      "end_time": "2026-02-08T12:16:16.317136",
      "duration_seconds": 1390.17,
      "vllm_pid": 3590546,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.3163,
          "score": 0.3163
        },
        "mmlu_pro": {
          "overall": 0.1107,
          "score": 0.1107
        },
        "math_500": {
          "overall": 0.0508,
          "score": 0.0508
        },
        "gsm8k": {
          "overall": 0.07,
          "score": 0.07
        },
        "gpqa_diamond": {
          "overall": 0.2,
          "score": 0.2
        }
      },
      "eval_output_dir": "outputs/ByteDance-Seed_Seed-X-PPO-7B",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1311.45,
        "total_seconds": 1390.17
      },
      "gpu_memory_gb": 901.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "facebook/bart-large-cnn": {
      "model_id": "facebook/bart-large-cnn",
      "model_name": "facebook/bart-large-cnn",
      "size_bytes": 406290432,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:16:26.330993",
      "end_time": "2026-02-08T12:16:46.459798",
      "duration_seconds": 20.13,
      "vllm_pid": 3645097,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m   Value error, Model architecture BartForConditionalGeneration was supported in vLLM until v0.10.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=3645097)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.13
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "abacusai/Smaug-72B-v0.1": {
      "model_id": "abacusai/Smaug-72B-v0.1",
      "model_name": "abacusai/Smaug-72B-v0.1",
      "size_bytes": 72288575488,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-08T11:52:09.144470",
      "end_time": "2026-02-08T12:18:30.112726",
      "duration_seconds": 1580.97,
      "vllm_pid": 3588116,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.5458,
          "score": 0.5458
        },
        "mmlu_pro": {
          "overall": 0.4343,
          "score": 0.4343
        },
        "math_500": {
          "overall": 0.4457,
          "score": 0.4457
        },
        "gsm8k": {
          "overall": 0.82,
          "score": 0.82
        },
        "gpqa_diamond": {
          "overall": 0.36,
          "score": 0.36
        }
      },
      "eval_output_dir": "outputs/abacusai_Smaug-72B-v0.1",
      "timing": {
        "server_start_seconds": 160.06,
        "test_execution_seconds": 1404.37,
        "total_seconds": 1580.97
      },
      "gpu_memory_gb": 901.21,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "google/gemma-2-2b": {
      "model_id": "google/gemma-2-2b",
      "model_name": "google/gemma-2-2b",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:18:30.134473",
      "end_time": "2026-02-08T12:19:30.300166",
      "duration_seconds": 60.17,
      "vllm_pid": 3649767,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:00,  2.24it/s]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:05<00:03,  3.01s/it]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:10<00:00,  3.93s/it]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:10<00:00,  3.42s/it]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m 2026-02-08 12:19:17,890 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m 2026-02-08 12:19:17,900 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|         | 1/51 [00:00<00:07,  7.13it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|         | 4/51 [00:00<00:02, 16.12it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|        | 7/51 [00:00<00:02, 20.23it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|        | 10/51 [00:00<00:01, 23.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|       | 13/51 [00:00<00:01, 24.52it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|      | 16/51 [00:00<00:01, 25.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|      | 19/51 [00:00<00:01, 26.74it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|     | 23/51 [00:00<00:00, 28.64it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|    | 27/51 [00:01<00:00, 29.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|    | 31/51 [00:01<00:00, 30.13it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|   | 35/51 [00:01<00:00, 30.17it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|  | 39/51 [00:01<00:00, 30.22it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%| | 43/51 [00:01<00:00, 30.27it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|| 47/51 [00:01<00:00, 30.11it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|| 51/51 [00:01<00:00, 30.66it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|| 51/51 [00:01<00:00, 27.57it/s]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 940, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 927, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 692, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 113, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 270, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 455, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4832, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4910, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4461, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 222, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 465, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 233, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 119, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 64, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 807, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 714, in forward\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 279, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=3650161)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W208 12:19:20.922288642 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 214, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 205, in from_vllm_config\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 132, in __init__\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 122, in make_async_mp_client\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 824, in __init__\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 479, in __init__\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 921, in launch_core_engines\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 980, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=3649767)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 60.17
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "jetmoe/jetmoe-8b": {
      "model_id": "jetmoe/jetmoe-8b",
      "model_name": "jetmoe/jetmoe-8b",
      "size_bytes": 8522237952,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:19:40.313109",
      "end_time": "2026-02-08T12:20:00.521619",
      "duration_seconds": 20.21,
      "vllm_pid": 3653043,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m   Value error, Model architectures ['JetMoEForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoEForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'Grok1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'IQuestCoderForCausalLM', 'IQuestLoopCoderForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'MiMoV2FlashForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguProMoEV2ForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaBidirectionalModel', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'JinaVLForRanking', 'LlamaBidirectionalForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GlmAsrForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'IsaacForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KananaVForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Lfm2VlForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'VoxtralStreamingGeneration', 'NemotronParseForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'ExaoneMoeMTP', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=3653043)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.21
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "HuggingFaceTB/SmolLM-135M": {
      "model_id": "HuggingFaceTB/SmolLM-135M",
      "model_name": "HuggingFaceTB/SmolLM-135M",
      "size_bytes": 134515008,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:13:47.735033",
      "end_time": "2026-02-08T12:21:58.055529",
      "duration_seconds": 490.32,
      "vllm_pid": 1637911,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.1119,
          "score": 0.1119
        },
        "mmlu_pro": {
          "overall": 0.06,
          "score": 0.06
        },
        "math_500": {
          "overall": 0.0231,
          "score": 0.0231
        },
        "gsm8k": {
          "overall": 0.02,
          "score": 0.02
        },
        "gpqa_diamond": {
          "overall": 0.09,
          "score": 0.09
        }
      },
      "eval_output_dir": "outputs/HuggingFaceTB_SmolLM-135M",
      "timing": {
        "server_start_seconds": 35.03,
        "test_execution_seconds": 446.68,
        "total_seconds": 490.32
      },
      "gpu_memory_gb": 1025.28,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "defog/sqlcoder-7b-2": {
      "model_id": "defog/sqlcoder-7b-2",
      "model_name": "defog/sqlcoder-7b-2",
      "size_bytes": 6738546688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:16:05.910835",
      "end_time": "2026-02-08T12:23:22.719790",
      "duration_seconds": 436.81,
      "vllm_pid": 1643676,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.1111,
          "score": 0.1111
        },
        "mmlu_pro": {
          "overall": 0.1693,
          "score": 0.1693
        },
        "math_500": {
          "overall": 0.0346,
          "score": 0.0346
        },
        "gsm8k": {
          "overall": 0.12,
          "score": 0.12
        },
        "gpqa_diamond": {
          "overall": 0.02,
          "score": 0.02
        }
      },
      "eval_output_dir": "outputs/defog_sqlcoder-7b-2",
      "timing": {
        "server_start_seconds": 50.03,
        "test_execution_seconds": 376.05,
        "total_seconds": 436.81
      },
      "gpu_memory_gb": 1025.76,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "MarinaraSpaghetti/NemoMix-Unleashed-12B": {
      "model_id": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
      "model_name": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:13:48.641244",
      "end_time": "2026-02-08T12:29:09.109442",
      "duration_seconds": 920.47,
      "vllm_pid": 1638090,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.4537,
          "score": 0.4537
        },
        "mmlu_pro": {
          "overall": 0.0786,
          "score": 0.0786
        },
        "math_500": {
          "overall": 0.1871,
          "score": 0.1871
        },
        "gsm8k": {
          "overall": 0.5,
          "score": 0.5
        },
        "gpqa_diamond": {
          "overall": 0.26,
          "score": 0.26
        }
      },
      "eval_output_dir": "outputs/MarinaraSpaghetti_NemoMix-Unleashed-12B",
      "timing": {
        "server_start_seconds": 65.04,
        "test_execution_seconds": 841.73,
        "total_seconds": 920.47
      },
      "gpu_memory_gb": 1025.46,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "meta-llama/Llama-3.2-1B": {
      "model_id": "meta-llama/Llama-3.2-1B",
      "model_name": "meta-llama/Llama-3.2-1B",
      "size_bytes": 1235814400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:23:32.738035",
      "end_time": "2026-02-08T12:34:25.958279",
      "duration_seconds": 653.22,
      "vllm_pid": 1660899,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.1688,
          "score": 0.1688
        },
        "mmlu_pro": {
          "overall": 0.1193,
          "score": 0.1193
        },
        "math_500": {
          "overall": 0.0254,
          "score": 0.0254
        },
        "gsm8k": {
          "overall": 0.02,
          "score": 0.02
        },
        "gpqa_diamond": {
          "overall": 0.13,
          "score": 0.13
        }
      },
      "eval_output_dir": "outputs/meta-llama_Llama-3.2-1B",
      "timing": {
        "server_start_seconds": 45.04,
        "test_execution_seconds": 599.63,
        "total_seconds": 653.22
      },
      "gpu_memory_gb": 1025.46,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "meta-llama/Llama-3.1-8B": {
      "model_id": "meta-llama/Llama-3.1-8B",
      "model_name": "meta-llama/Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:22:08.073970",
      "end_time": "2026-02-08T12:46:00.678896",
      "duration_seconds": 1432.6,
      "vllm_pid": 1657911,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.2805,
          "score": 0.2805
        },
        "mmlu_pro": {
          "overall": 0.2179,
          "score": 0.2179
        },
        "math_500": {
          "overall": 0.0208,
          "score": 0.0208
        },
        "gsm8k": {
          "overall": 0.01,
          "score": 0.01
        },
        "gpqa_diamond": {
          "overall": 0.13,
          "score": 0.13
        }
      },
      "eval_output_dir": "outputs/meta-llama_Llama-3.1-8B",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 1369.3,
        "total_seconds": 1432.6
      },
      "gpu_memory_gb": 1025.76,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "google/gemma-3-270m": {
      "model_id": "google/gemma-3-270m",
      "model_name": "google/gemma-3-270m",
      "size_bytes": 268098176,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:18:35.503333",
      "end_time": "2026-02-08T12:47:27.963011",
      "duration_seconds": 1732.46,
      "vllm_pid": 3650035,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.0961,
          "score": 0.0961
        },
        "mmlu_pro": {
          "overall": 0.0336,
          "score": 0.0336
        },
        "math_500": {
          "overall": 0.0069,
          "score": 0.0069
        },
        "gsm8k": {
          "overall": 0.01,
          "score": 0.01
        },
        "gpqa_diamond": {
          "overall": 0.12,
          "score": 0.12
        }
      },
      "eval_output_dir": "outputs/google_gemma-3-270m",
      "timing": {
        "server_start_seconds": 40.04,
        "test_execution_seconds": 1678.14,
        "total_seconds": 1732.46
      },
      "gpu_memory_gb": 892.33,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "microsoft/phi-1_5": {
      "model_id": "microsoft/phi-1_5",
      "model_name": "microsoft/phi-1_5",
      "size_bytes": 1418270720,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:34:35.973470",
      "end_time": "2026-02-08T12:49:59.707943",
      "duration_seconds": 923.73,
      "vllm_pid": 1684668,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.1658,
          "score": 0.1658
        },
        "mmlu_pro": {
          "overall": 0.0493,
          "score": 0.0493
        },
        "math_500": {
          "overall": 0.0208,
          "score": 0.0208
        },
        "gsm8k": {
          "overall": 0.12,
          "score": 0.12
        },
        "gpqa_diamond": {
          "overall": 0.15,
          "score": 0.15
        }
      },
      "eval_output_dir": "outputs/microsoft_phi-1_5",
      "timing": {
        "server_start_seconds": 40.03,
        "test_execution_seconds": 874.91,
        "total_seconds": 923.73
      },
      "gpu_memory_gb": 1025.21,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "google/gemma-2b": {
      "model_id": "google/gemma-2b",
      "model_name": "google/gemma-2b",
      "size_bytes": 2506172416,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:18:35.498371",
      "end_time": "2026-02-08T12:50:32.622167",
      "duration_seconds": 1917.12,
      "vllm_pid": 3650016,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.1667,
          "score": 0.1667
        },
        "mmlu_pro": {
          "overall": 0.0607,
          "score": 0.0607
        },
        "math_500": {
          "overall": 0.0231,
          "score": 0.0231
        },
        "gsm8k": {
          "overall": 0.03,
          "score": 0.03
        },
        "gpqa_diamond": {
          "overall": 0.08,
          "score": 0.08
        }
      },
      "eval_output_dir": "outputs/google_gemma-2b",
      "timing": {
        "server_start_seconds": 40.06,
        "test_execution_seconds": 1862.41,
        "total_seconds": 1917.12
      },
      "gpu_memory_gb": 892.93,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "openai-community/gpt2": {
      "model_id": "openai-community/gpt2",
      "model_name": "openai-community/gpt2",
      "size_bytes": 137022720,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-02-08T12:50:42.637783",
      "end_time": "2026-02-08T12:52:33.768383",
      "duration_seconds": 111.13,
      "vllm_pid": 3724922,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 717 tokens. However, your request has 1041 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=1041)\", 'type': 'BadRequestError', 'param': 'input_tokens', 'code': 400}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_base.py\", line 1038, in run_test\n    task_result = run_task(task_cfg=task_cfg)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/models/openai_compatible.py\", line 109, in generate\n    return self._generate_completion(input, tools, tool_choice, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/models/openai_compatible.py\", line 201, in _generate_completion\n    return self.handle_bad_request(ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/models/openai_compatible.py\", line 244, in handle_bad_request\n    return openai_handle_bad_request(self.model_name, ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/models/utils/openai.py\", line 727, in openai_handle_bad_request\n    raise e\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/models/openai_compatible.py\", line 180, in _generate_completion\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/evalscope-simple/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 717 tokens. However, your request has 1041 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=1041)\", 'type': 'BadRequestError', 'param': 'input_tokens', 'code': 400}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/openai-community_gpt2",
      "timing": {
        "server_start_seconds": 35.04,
        "test_execution_seconds": 67.79,
        "total_seconds": 111.13
      },
      "gpu_memory_gb": 892.53,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "philschmid/bart-large-cnn-samsum": {
      "model_id": "philschmid/bart-large-cnn-samsum",
      "model_name": "philschmid/bart-large-cnn-samsum",
      "size_bytes": 14628978483,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:52:43.781437",
      "end_time": "2026-02-08T12:52:58.880503",
      "duration_seconds": 15.1,
      "vllm_pid": 3729778,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m   Value error, Model architecture BartForConditionalGeneration was supported in vLLM until v0.10.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=3729778)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.1
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "meta-llama/Meta-Llama-3-8B": {
      "model_id": "meta-llama/Meta-Llama-3-8B",
      "model_name": "meta-llama/Meta-Llama-3-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:29:19.127237",
      "end_time": "2026-02-08T12:55:54.045492",
      "duration_seconds": 1594.92,
      "vllm_pid": 1673842,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.2397,
          "score": 0.2397
        },
        "mmlu_pro": {
          "overall": 0.1593,
          "score": 0.1593
        },
        "math_500": {
          "overall": 0.0162,
          "score": 0.0162
        },
        "gsm8k": {
          "overall": 0.01,
          "score": 0.01
        },
        "gpqa_diamond": {
          "overall": 0.19,
          "score": 0.19
        }
      },
      "eval_output_dir": "outputs/meta-llama_Meta-Llama-3-8B",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 1529.26,
        "total_seconds": 1594.92
      },
      "gpu_memory_gb": 1025.51,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "starvector/starvector-8b-im2svg": {
      "model_id": "starvector/starvector-8b-im2svg",
      "model_name": "starvector/starvector-8b-im2svg",
      "size_bytes": 7507080192,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T12:56:04.061699",
      "end_time": "2026-02-08T12:56:24.264988",
      "duration_seconds": 20.2,
      "vllm_pid": 1728782,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m Encountered exception while importing starvector: No module named 'starvector'\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 473, in __post_init__\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 633, in get_config\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 166, in parse\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 427, in get_cached_module_file\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     modules_needed = check_imports(resolved_module_file)\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 260, in check_imports\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=1728782)\u001b[0;0m ImportError: This modeling file requires the following packages that were not found in your environment: starvector. Run `pip install starvector`\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.2
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "google/gemma-7b": {
      "model_id": "google/gemma-7b",
      "model_name": "google/gemma-7b",
      "size_bytes": 8537680896,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:18:35.496649",
      "end_time": "2026-02-08T12:59:20.142128",
      "duration_seconds": 2444.65,
      "vllm_pid": 3650041,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.274,
          "score": 0.274
        },
        "mmlu_pro": {
          "overall": 0.0464,
          "score": 0.0464
        },
        "math_500": {
          "overall": 0.0208,
          "score": 0.0208
        },
        "gsm8k": {
          "overall": 0.0,
          "score": 0.0
        },
        "gpqa_diamond": {
          "overall": 0.01,
          "score": 0.01
        }
      },
      "eval_output_dir": "outputs/google_gemma-7b",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 2367.44,
        "total_seconds": 2444.65
      },
      "gpu_memory_gb": 893.09,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "fdtn-ai/Foundation-Sec-8B": {
      "model_id": "fdtn-ai/Foundation-Sec-8B",
      "model_name": "fdtn-ai/Foundation-Sec-8B",
      "size_bytes": 8031309824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:16:56.471728",
      "end_time": "2026-02-08T13:09:19.669257",
      "duration_seconds": 3143.2,
      "vllm_pid": 3645914,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.5202,
          "score": 0.5202
        },
        "mmlu_pro": {
          "overall": 0.2343,
          "score": 0.2343
        },
        "math_500": {
          "overall": 0.0277,
          "score": 0.0277
        },
        "gsm8k": {
          "overall": 0.05,
          "score": 0.05
        },
        "gpqa_diamond": {
          "overall": 0.26,
          "score": 0.26
        }
      },
      "eval_output_dir": "outputs/fdtn-ai_Foundation-Sec-8B",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 3079.03,
        "total_seconds": 3143.2
      },
      "gpu_memory_gb": 901.21,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "vectara/hallucination_evaluation_model": {
      "model_id": "vectara/hallucination_evaluation_model",
      "model_name": "vectara/hallucination_evaluation_model",
      "size_bytes": 109630082,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T13:09:29.693538",
      "end_time": "2026-02-08T13:09:49.813999",
      "duration_seconds": 20.12,
      "vllm_pid": 3767699,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 498, in __post_init__\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     self.model_arch_config = self.get_model_arch_config()\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 626, in get_model_arch_config\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return convertor.convert()\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/model_arch_config_convertor.py\", line 252, in convert\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     head_size=self.get_head_size(),\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m               ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/model_arch_config_convertor.py\", line 64, in get_head_size\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     self.hf_text_config.hidden_size // self.hf_text_config.num_attention_heads\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 207, in __getattribute__\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m     return super().__getattribute__(key)\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767699)\u001b[0;0m AttributeError: 'HHEMv2Config' object has no attribute 'hidden_size'\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.12
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "vikhyatk/moondream1": {
      "model_id": "vikhyatk/moondream1",
      "model_name": "vikhyatk/moondream1",
      "size_bytes": 1857482608,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T13:09:49.826775",
      "end_time": "2026-02-08T13:10:09.976147",
      "duration_seconds": 20.15,
      "vllm_pid": 3767940,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 498, in __post_init__\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     self.model_arch_config = self.get_model_arch_config()\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 626, in get_model_arch_config\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return convertor.convert()\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/model_arch_config_convertor.py\", line 252, in convert\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     head_size=self.get_head_size(),\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m               ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/model_arch_config_convertor.py\", line 64, in get_head_size\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     self.hf_text_config.hidden_size // self.hf_text_config.num_attention_heads\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 207, in __getattribute__\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m     return super().__getattribute__(key)\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3767940)\u001b[0;0m AttributeError: 'MoondreamConfig' object has no attribute 'hidden_size'\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.15
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "zai-org/chatglm-6b": {
      "model_id": "zai-org/chatglm-6b",
      "model_name": "zai-org/chatglm-6b",
      "size_bytes": 47452655921,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-02-08T13:10:19.997280",
      "end_time": "2026-02-08T13:10:35.114112",
      "duration_seconds": 15.12,
      "vllm_pid": 3769333,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1319, in run_server\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1338, in run_server_worker\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 199, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1369, in create_engine_config\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1223, in create_model_config\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m   Value error, User-specified max_model_len (65536) is greater than the derived max_model_len (max_position_embeddings=2048.0 or model_max_length=None in model's config.json). To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1. VLLM_ALLOW_LONG_MAX_MODEL_LEN must be used with extreme caution. If the model uses relative position encoding (RoPE), positions exceeding derived_max_model_len lead to nan. If the model uses absolute position encoding, positions exceeding derived_max_model_len will cause a CUDA array out-of-bounds error. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=3769333)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.12
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "zai-org/chatglm2-6b": {
      "model_id": "zai-org/chatglm2-6b",
      "model_name": "zai-org/chatglm2-6b",
      "size_bytes": 49949968758,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-02-08T13:10:45.128416",
      "end_time": "2026-02-08T13:12:39.341929",
      "duration_seconds": 114.21,
      "vllm_pid": 3770113,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"ChatGLMTokenizer._pad() got an unexpected keyword argument 'padding_side'\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.04,
        "test_execution_seconds": null,
        "total_seconds": 114.21
      },
      "gpu_memory_gb": null,
      "throughput": null,
      "_merged_from": "base_experiment_results"
    },
    "microsoft/phi-2": {
      "model_id": "microsoft/phi-2",
      "model_name": "microsoft/phi-2",
      "size_bytes": 2779683840,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:46:10.693992",
      "end_time": "2026-02-08T13:18:00.746270",
      "duration_seconds": 1910.05,
      "vllm_pid": 1708644,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.3202,
          "score": 0.3202
        },
        "mmlu_pro": {
          "overall": 0.1279,
          "score": 0.1279
        },
        "math_500": {
          "overall": 0.0762,
          "score": 0.0762
        },
        "gsm8k": {
          "overall": 0.34,
          "score": 0.34
        },
        "gpqa_diamond": {
          "overall": 0.17,
          "score": 0.17
        }
      },
      "eval_output_dir": "outputs/microsoft_phi-2",
      "timing": {
        "server_start_seconds": 45.03,
        "test_execution_seconds": 1854.92,
        "total_seconds": 1910.05
      },
      "gpu_memory_gb": 1026.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "tiiuae/falcon-mamba-7b": {
      "model_id": "tiiuae/falcon-mamba-7b",
      "model_name": "tiiuae/falcon-mamba-7b",
      "size_bytes": 7272665088,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:56:34.279020",
      "end_time": "2026-02-08T13:37:57.025520",
      "duration_seconds": 2482.75,
      "vllm_pid": 1729470,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.2923,
          "score": 0.2923
        },
        "mmlu_pro": {
          "overall": 0.1479,
          "score": 0.1479
        },
        "math_500": {
          "overall": 0.0346,
          "score": 0.0346
        },
        "gsm8k": {
          "overall": 0.29,
          "score": 0.29
        },
        "gpqa_diamond": {
          "overall": 0.11,
          "score": 0.11
        }
      },
      "eval_output_dir": "outputs/tiiuae_falcon-mamba-7b",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 2422.31,
        "total_seconds": 2482.75
      },
      "gpu_memory_gb": 1026.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "upstage/SOLAR-10.7B-v1.0": {
      "model_id": "upstage/SOLAR-10.7B-v1.0",
      "model_name": "upstage/SOLAR-10.7B-v1.0",
      "size_bytes": 10731524096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:59:30.160323",
      "end_time": "2026-02-08T13:51:41.514328",
      "duration_seconds": 3131.35,
      "vllm_pid": 3745253,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.6024,
          "score": 0.6024
        },
        "mmlu_pro": {
          "overall": 0.3171,
          "score": 0.3171
        },
        "math_500": {
          "overall": 0.0416,
          "score": 0.0416
        },
        "gsm8k": {
          "overall": 0.43,
          "score": 0.43
        },
        "gpqa_diamond": {
          "overall": 0.33,
          "score": 0.33
        }
      },
      "eval_output_dir": "outputs/upstage_SOLAR-10.7B-v1.0",
      "timing": {
        "server_start_seconds": 70.04,
        "test_execution_seconds": 3049.3,
        "total_seconds": 3131.35
      },
      "gpu_memory_gb": 893.21,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "mistralai/Mistral-7B-v0.1": {
      "model_id": "mistralai/Mistral-7B-v0.1",
      "model_name": "mistralai/Mistral-7B-v0.1",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:47:37.984600",
      "end_time": "2026-02-08T13:53:39.264318",
      "duration_seconds": 3961.28,
      "vllm_pid": 3717695,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.3103,
          "score": 0.3103
        },
        "mmlu_pro": {
          "overall": 0.1629,
          "score": 0.1629
        },
        "math_500": {
          "overall": 0.0,
          "score": 0.0
        },
        "gsm8k": {
          "overall": 0.03,
          "score": 0.03
        },
        "gpqa_diamond": {
          "overall": 0.09,
          "score": 0.09
        }
      },
      "eval_output_dir": "outputs/mistralai_Mistral-7B-v0.1",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 3890.7,
        "total_seconds": 3961.28
      },
      "gpu_memory_gb": 892.93,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "mistralai/Mistral-7B-v0.3": {
      "model_id": "mistralai/Mistral-7B-v0.3",
      "model_name": "mistralai/Mistral-7B-v0.3",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:50:09.725725",
      "end_time": "2026-02-08T13:54:10.898730",
      "duration_seconds": 3841.17,
      "vllm_pid": 1717016,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.2739,
          "score": 0.2739
        },
        "mmlu_pro": {
          "overall": 0.1779,
          "score": 0.1779
        },
        "math_500": {
          "overall": 0.0115,
          "score": 0.0115
        },
        "gsm8k": {
          "overall": 0.04,
          "score": 0.04
        },
        "gpqa_diamond": {
          "overall": 0.06,
          "score": 0.06
        }
      },
      "eval_output_dir": "outputs/mistralai_Mistral-7B-v0.3",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 3780.68,
        "total_seconds": 3841.17
      },
      "gpu_memory_gb": 1025.51,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "m-a-p/YuE-s1-7B-anneal-en-cot": {
      "model_id": "m-a-p/YuE-s1-7B-anneal-en-cot",
      "model_name": "m-a-p/YuE-s1-7B-anneal-en-cot",
      "size_bytes": 6224613376,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:20:10.533564",
      "end_time": "2026-02-08T14:06:13.030460",
      "duration_seconds": 6362.5,
      "vllm_pid": 3654646,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.2037,
          "score": 0.2037
        },
        "mmlu_pro": {
          "overall": 0.1243,
          "score": 0.1243
        },
        "math_500": {
          "overall": 0.0,
          "score": 0.0
        },
        "gsm8k": {
          "overall": 0.0,
          "score": 0.0
        },
        "gpqa_diamond": {
          "overall": 0.19,
          "score": 0.19
        }
      },
      "eval_output_dir": "outputs/m-a-p_YuE-s1-7B-anneal-en-cot",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 6296.82,
        "total_seconds": 6362.5
      },
      "gpu_memory_gb": 892.32,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "bigcode/starcoder2-15b": {
      "model_id": "bigcode/starcoder2-15b",
      "model_name": "bigcode/starcoder2-15b",
      "size_bytes": 15957889024,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-02-08T12:05:29.879941",
      "end_time": "2026-02-08T14:09:33.976904",
      "duration_seconds": 7444.1,
      "vllm_pid": 3620472,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.2521,
          "score": 0.2521
        },
        "mmlu_pro": {
          "overall": 0.0836,
          "score": 0.0836
        },
        "math_500": {
          "overall": 0.0808,
          "score": 0.0808
        },
        "gsm8k": {
          "overall": 0.21,
          "score": 0.21
        },
        "gpqa_diamond": {
          "overall": 0.08,
          "score": 0.08
        }
      },
      "eval_output_dir": "outputs/bigcode_starcoder2-15b",
      "timing": {
        "server_start_seconds": 105.05,
        "test_execution_seconds": 7327.61,
        "total_seconds": 7444.1
      },
      "gpu_memory_gb": 901.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "JetBrains/Mellum-4b-base": {
      "model_id": "JetBrains/Mellum-4b-base",
      "model_name": "JetBrains/Mellum-4b-base",
      "size_bytes": 4019248128,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:13:48.641800",
      "end_time": "2026-02-08T14:20:53.651661",
      "duration_seconds": 7625.01,
      "vllm_pid": 1638087,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.02,
          "score": 0.02
        },
        "mmlu_pro": {
          "overall": 0.085,
          "score": 0.085
        },
        "math_500": {
          "overall": 0.0115,
          "score": 0.0115
        },
        "gsm8k": {
          "overall": 0.01,
          "score": 0.01
        },
        "gpqa_diamond": {
          "overall": 0.02,
          "score": 0.02
        }
      },
      "eval_output_dir": "outputs/JetBrains_Mellum-4b-base",
      "timing": {
        "server_start_seconds": 45.03,
        "test_execution_seconds": 7564.8,
        "total_seconds": 7625.01
      },
      "gpu_memory_gb": 794.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "codellama/CodeLlama-70b-hf": {
      "model_id": "codellama/CodeLlama-70b-hf",
      "model_name": "codellama/CodeLlama-70b-hf",
      "size_bytes": 68976910336,
      "tp_size": 4,
      "status": "success",
      "start_time": "2026-02-08T12:01:23.307246",
      "end_time": "2026-02-08T15:18:10.691589",
      "duration_seconds": 11807.38,
      "vllm_pid": 1613989,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.4581,
          "score": 0.4581
        },
        "mmlu_pro": {
          "overall": 0.1836,
          "score": 0.1836
        },
        "math_500": {
          "overall": 0.0254,
          "score": 0.0254
        },
        "gsm8k": {
          "overall": 0.02,
          "score": 0.02
        },
        "gpqa_diamond": {
          "overall": 0.23,
          "score": 0.23
        }
      },
      "eval_output_dir": "outputs/codellama_CodeLlama-70b-hf",
      "timing": {
        "server_start_seconds": 130.06,
        "test_execution_seconds": 11662.31,
        "total_seconds": 11807.38
      },
      "gpu_memory_gb": 1029.4,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    },
    "stabilityai/stable-code-3b": {
      "model_id": "stabilityai/stable-code-3b",
      "model_name": "stabilityai/stable-code-3b",
      "size_bytes": 2795443200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-02-08T12:53:08.893796",
      "end_time": "2026-02-08T17:33:07.078706",
      "duration_seconds": 16798.18,
      "vllm_pid": 3731056,
      "vllm_version": "0.14.1",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "overall": 0.27,
          "score": 0.27
        },
        "mmlu_pro": {
          "overall": 0.1286,
          "score": 0.1286
        },
        "math_500": {
          "overall": 0.0208,
          "score": 0.0208
        },
        "gsm8k": {
          "overall": 0.04,
          "score": 0.04
        },
        "gpqa_diamond": {
          "overall": 0.14,
          "score": 0.14
        }
      },
      "eval_output_dir": "outputs/stabilityai_stable-code-3b",
      "timing": {
        "server_start_seconds": 35.06,
        "test_execution_seconds": 16751.84,
        "total_seconds": 16798.18
      },
      "gpu_memory_gb": 893.09,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      },
      "_merged_from": "base_experiment_results"
    }
  },
  "skipped": 0,
  "started_at": "2026-01-26T16:51:33.294826",
  "success": 164,
  "total_models": 138,
  "vllm_version": "0.14.1",
  "_extracted_at": "2026-02-09T13:41:14.200973",
  "_source_file": "experiment_results/results.json",
  "_scores_filled": 138,
  "_merge_info": {
    "merged_at": "2026-02-09T13:41:26.067035",
    "chat_source": "experiment_results/results_success_with_scores.json",
    "base_source": "base_experiment_results/results.json",
    "added": 44,
    "updated": 0,
    "skipped": 0,
    "total_models": 182
  }
}