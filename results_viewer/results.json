{
  "started_at": "2026-01-20T17:08:09.555425",
  "vllm_version": "0.13.0",
  "python_version": "3.11.14",
  "hostname": "baai-sailing-3",
  "datasets_config": [
    "mmlu",
    "mmlu_pro",
    "math_500",
    "gsm8k",
    "gpqa_diamond"
  ],
  "total_models": 1763,
  "completed": 1520,
  "success": 344,
  "failed": 314,
  "skipped": 862,
  "results": {
    "flashresearch/FlashResearch-4B-Thinking": {
      "model_id": "flashresearch/FlashResearch-4B-Thinking",
      "model_name": "flashresearch/FlashResearch-4B-Thinking",
      "size_bytes": 196096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.742107",
      "end_time": "2026-01-21T17:10:18.742116",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FutureMa/Qwen3-8B-Drama-Thinking": {
      "model_id": "FutureMa/Qwen3-8B-Drama-Thinking",
      "model_name": "FutureMa/Qwen3-8B-Drama-Thinking",
      "size_bytes": 308224,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.763227",
      "end_time": "2026-01-21T17:10:18.763236",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Impish_Nemo_12B": {
      "model_id": "SicariusSicariiStuff/Impish_Nemo_12B",
      "model_name": "SicariusSicariiStuff/Impish_Nemo_12B",
      "size_bytes": 414720,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.781545",
      "end_time": "2026-01-21T17:10:18.781553",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Precog-24B-v1": {
      "model_id": "TheDrummer/Precog-24B-v1",
      "model_name": "TheDrummer/Precog-24B-v1",
      "size_bytes": 414720,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.799833",
      "end_time": "2026-01-21T17:10:18.799840",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Olmo-3-7B-Instruct": {
      "model_id": "allenai/Olmo-3-7B-Instruct",
      "model_name": "allenai/Olmo-3-7B-Instruct",
      "size_bytes": 528384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.818152",
      "end_time": "2026-01-21T17:10:18.818160",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Olmo-3-7B-Think": {
      "model_id": "allenai/Olmo-3-7B-Think",
      "model_name": "allenai/Olmo-3-7B-Think",
      "size_bytes": 528384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.836224",
      "end_time": "2026-01-21T17:10:18.836231",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Olmo-3-32B-Think": {
      "model_id": "allenai/Olmo-3-32B-Think",
      "model_name": "allenai/Olmo-3-32B-Think",
      "size_bytes": 1053696,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.854752",
      "end_time": "2026-01-21T17:10:18.854759",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Maykeye/TinyLLama-v0": {
      "model_id": "Maykeye/TinyLLama-v0",
      "model_name": "Maykeye/TinyLLama-v0",
      "size_bytes": 4621392,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.873173",
      "end_time": "2026-01-21T17:10:18.873180",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "agarkovv/CryptoTrader-LM": {
      "model_id": "agarkovv/CryptoTrader-LM",
      "model_name": "agarkovv/CryptoTrader-LM",
      "size_bytes": 24764916,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.891522",
      "end_time": "2026-01-21T17:10:18.891530",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "codelion/gpt-2-70m": {
      "model_id": "codelion/gpt-2-70m",
      "model_name": "codelion/gpt-2-70m",
      "size_bytes": 64085504,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.909748",
      "end_time": "2026-01-21T17:10:18.909754",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "madhurjindal/autonlp-Gibberish-Detector-492513457": {
      "model_id": "madhurjindal/autonlp-Gibberish-Detector-492513457",
      "model_name": "madhurjindal/autonlp-Gibberish-Detector-492513457",
      "size_bytes": 66956548,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.927719",
      "end_time": "2026-01-21T17:10:18.927726",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "joeddav/distilbert-base-uncased-go-emotions-student": {
      "model_id": "joeddav/distilbert-base-uncased-go-emotions-student",
      "model_name": "joeddav/distilbert-base-uncased-go-emotions-student",
      "size_bytes": 66975004,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.945845",
      "end_time": "2026-01-21T17:10:18.945852",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Helsinki-NLP/opus-mt-fr-en": {
      "model_id": "Helsinki-NLP/opus-mt-fr-en",
      "model_name": "Helsinki-NLP/opus-mt-fr-en",
      "size_bytes": 75193466,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.964091",
      "end_time": "2026-01-21T17:10:18.964097",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AdamCodd/distilroberta-nsfw-prompt-stable-diffusion": {
      "model_id": "AdamCodd/distilroberta-nsfw-prompt-stable-diffusion",
      "model_name": "AdamCodd/distilroberta-nsfw-prompt-stable-diffusion",
      "size_bytes": 82119938,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:18.982140",
      "end_time": "2026-01-21T17:10:18.982152",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kz-transformers/kaz-roberta-conversational": {
      "model_id": "kz-transformers/kaz-roberta-conversational",
      "model_name": "kz-transformers/kaz-roberta-conversational",
      "size_bytes": 83504930,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.001629",
      "end_time": "2026-01-21T17:10:19.001637",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "raynardj/classical-chinese-punctuation-guwen-biaodian": {
      "model_id": "raynardj/classical-chinese-punctuation-guwen-biaodian",
      "model_name": "raynardj/classical-chinese-punctuation-guwen-biaodian",
      "size_bytes": 101693717,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.020183",
      "end_time": "2026-01-21T17:10:19.020191",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "shibing624/macbert4csc-base-chinese": {
      "model_id": "shibing624/macbert4csc-base-chinese",
      "model_name": "shibing624/macbert4csc-base-chinese",
      "size_bytes": 102290824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.038610",
      "end_time": "2026-01-21T17:10:19.038618",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google-bert/bert-base-chinese": {
      "model_id": "google-bert/bert-base-chinese",
      "model_name": "google-bert/bert-base-chinese",
      "size_bytes": 102882442,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.056917",
      "end_time": "2026-01-21T17:10:19.056926",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jingyaogong/MiniMind2": {
      "model_id": "jingyaogong/MiniMind2",
      "model_name": "jingyaogong/MiniMind2",
      "size_bytes": 104030976,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.075121",
      "end_time": "2026-01-21T17:10:19.075128",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "bucketresearch/politicalBiasBERT": {
      "model_id": "bucketresearch/politicalBiasBERT",
      "model_name": "bucketresearch/politicalBiasBERT",
      "size_bytes": 108313091,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.093525",
      "end_time": "2026-01-21T17:10:19.093532",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "bvanaken/clinical-assertion-negation-bert": {
      "model_id": "bvanaken/clinical-assertion-negation-bert",
      "model_name": "bvanaken/clinical-assertion-negation-bert",
      "size_bytes": 108313859,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.111734",
      "end_time": "2026-01-21T17:10:19.111741",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jjzha/jobbert-base-cased": {
      "model_id": "jjzha/jobbert-base-cased",
      "model_name": "jjzha/jobbert-base-cased",
      "size_bytes": 108341316,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.130224",
      "end_time": "2026-01-21T17:10:19.130231",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "oliverguhr/german-sentiment-bert": {
      "model_id": "oliverguhr/german-sentiment-bert",
      "model_name": "oliverguhr/german-sentiment-bert",
      "size_bytes": 109083651,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.148499",
      "end_time": "2026-01-21T17:10:19.148507",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/BiomedVLP-CXR-BERT-specialized": {
      "model_id": "microsoft/BiomedVLP-CXR-BERT-specialized",
      "model_name": "microsoft/BiomedVLP-CXR-BERT-specialized",
      "size_bytes": 109630010,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.166630",
      "end_time": "2026-01-21T17:10:19.166636",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "vectara/hallucination_evaluation_model": {
      "model_id": "vectara/hallucination_evaluation_model",
      "model_name": "vectara/hallucination_evaluation_model",
      "size_bytes": 109630082,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.184959",
      "end_time": "2026-01-21T17:10:19.184965",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "hatmimoha/arabic-ner": {
      "model_id": "hatmimoha/arabic-ner",
      "model_name": "hatmimoha/arabic-ner",
      "size_bytes": 110041875,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.203049",
      "end_time": "2026-01-21T17:10:19.203056",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sychonix/sychonix": {
      "model_id": "sychonix/sychonix",
      "model_name": "sychonix/sychonix",
      "size_bytes": 110106428,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.221332",
      "end_time": "2026-01-21T17:10:19.221338",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/japanese-roberta-base": {
      "model_id": "rinna/japanese-roberta-base",
      "model_name": "rinna/japanese-roberta-base",
      "size_bytes": 110652930,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.239569",
      "end_time": "2026-01-21T17:10:19.239576",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "almanach/camembert-bio-base": {
      "model_id": "almanach/camembert-bio-base",
      "model_name": "almanach/camembert-bio-base",
      "size_bytes": 110656007,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.257808",
      "end_time": "2026-01-21T17:10:19.257815",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dbmdz/bert-base-turkish-cased": {
      "model_id": "dbmdz/bert-base-turkish-cased",
      "model_name": "dbmdz/bert-base-turkish-cased",
      "size_bytes": 111243010,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.276046",
      "end_time": "2026-01-21T17:10:19.276053",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "haizelabs/j1-micro-1.7B": {
      "model_id": "haizelabs/j1-micro-1.7B",
      "model_name": "haizelabs/j1-micro-1.7B",
      "size_bytes": 117099276,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.294170",
      "end_time": "2026-01-21T17:10:19.294177",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "salakash/SamKash-Tolstoy": {
      "model_id": "salakash/SamKash-Tolstoy",
      "model_name": "salakash/SamKash-Tolstoy",
      "size_bytes": 122603718,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.312394",
      "end_time": "2026-01-21T17:10:19.312401",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "knowledgator/gliclass-edge-v3.0": {
      "model_id": "knowledgator/gliclass-edge-v3.0",
      "model_name": "knowledgator/gliclass-edge-v3.0",
      "size_bytes": 32705153,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.350022",
      "end_time": "2026-01-20T17:08:15.972966",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lazy-guy12/chess-llama": {
      "model_id": "lazy-guy12/chess-llama",
      "model_name": "lazy-guy12/chess-llama",
      "size_bytes": 23001600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.352040",
      "end_time": "2026-01-20T17:08:16.014428",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DeepMount00/Italian_NER_XXL": {
      "model_id": "DeepMount00/Italian_NER_XXL",
      "model_name": "DeepMount00/Italian_NER_XXL",
      "size_bytes": 110195061,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:15.981431",
      "end_time": "2026-01-20T17:08:16.024696",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "boltuix/bert-emotion": {
      "model_id": "boltuix/bert-emotion",
      "model_name": "boltuix/bert-emotion",
      "size_bytes": 11173901,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.352517",
      "end_time": "2026-01-20T17:08:16.024848",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tesslate/UIGEN-X-32B-0727": {
      "model_id": "Tesslate/UIGEN-X-32B-0727",
      "model_name": "Tesslate/UIGEN-X-32B-0727",
      "size_bytes": 676864,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.351590",
      "end_time": "2026-01-20T17:08:16.027424",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/qbw/models--Tesslate--UIGEN-X-32B-0727/snapshots/43ba0da3fde6ff16a6ec12a4e830afff38c52d6b/model-00002-of-00014.safetensors (SafetensorError: Error while deserializing header: invalid JSON in header: EOF while parsing a value at line 1 column 0)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DeepMount00/Italian_NER_XXL_v2": {
      "model_id": "DeepMount00/Italian_NER_XXL_v2",
      "model_name": "DeepMount00/Italian_NER_XXL_v2",
      "size_bytes": 110195061,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:16.021087",
      "end_time": "2026-01-20T17:08:16.064999",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "insilicomedicine/precious3-gpt-multi-modal": {
      "model_id": "insilicomedicine/precious3-gpt-multi-modal",
      "model_name": "insilicomedicine/precious3-gpt-multi-modal",
      "size_bytes": 89408520,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.799487",
      "end_time": "2026-01-20T17:08:16.250148",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-Prompt-Guard-2-22M": {
      "model_id": "meta-llama/Llama-Prompt-Guard-2-22M",
      "model_name": "meta-llama/Llama-Prompt-Guard-2-22M",
      "size_bytes": 70830722,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.803489",
      "end_time": "2026-01-20T17:08:16.250867",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "eliasalbouzidi/distilbert-nsfw-text-classifier": {
      "model_id": "eliasalbouzidi/distilbert-nsfw-text-classifier",
      "model_name": "eliasalbouzidi/distilbert-nsfw-text-classifier",
      "size_bytes": 66955010,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.788431",
      "end_time": "2026-01-20T17:08:16.251219",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Isotonic/distilbert_finetuned_ai4privacy_v2": {
      "model_id": "Isotonic/distilbert_finetuned_ai4privacy_v2",
      "model_name": "Isotonic/distilbert_finetuned_ai4privacy_v2",
      "size_bytes": 66448239,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.828908",
      "end_time": "2026-01-20T17:08:16.252971",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tabularisai/robust-sentiment-analysis": {
      "model_id": "tabularisai/robust-sentiment-analysis",
      "model_name": "tabularisai/robust-sentiment-analysis",
      "size_bytes": 66957317,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T17:08:14.803974",
      "end_time": "2026-01-20T17:08:16.250009",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Rijgersberg/GEITje-7B-chat-v2": {
      "model_id": "Rijgersberg/GEITje-7B-chat-v2",
      "model_name": "Rijgersberg/GEITje-7B-chat-v2",
      "size_bytes": 3803625,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T17:08:14.394953",
      "end_time": "2026-01-20T17:08:44.511775",
      "duration_seconds": 30.12,
      "vllm_pid": 2179489,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 640, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     return loader.load_weights(\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 279, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     for child_prefix, child_weights in self._groupby_prefix(weights):\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 163, in _groupby_prefix\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 158, in <genexpr>\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     weights_by_parts = (\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                        ^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 331, in <genexpr>\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     weights = (\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m               ^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 640, in <genexpr>\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     return loader.load_weights(\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                               ^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 277, in get_all_weights\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     yield from self._get_weights_iterator(primary_weights)\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 190, in _get_weights_iterator\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m                                                    ^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 179, in _prepare_weights\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(EngineCore_DP0 pid=2179569)\u001b[0;0m RuntimeError: Cannot find any model weights with `/mnt/baai_cp_perf/hf_models/models--Rijgersberg--GEITje-7B-chat-v2/snapshots/a5129be22534225cfee627837a7bafe100edcdb2`\n[rank0]:[W120 17:08:35.323602040 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2179489)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dousery/medical-reasoning-gpt-oss-20b": {
      "model_id": "dousery/medical-reasoning-gpt-oss-20b",
      "model_name": "dousery/medical-reasoning-gpt-oss-20b",
      "size_bytes": 88057350,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T17:08:14.802418",
      "end_time": "2026-01-20T17:08:56.346695",
      "duration_seconds": 41.54,
      "vllm_pid": 1545457,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--dousery--medical-reasoning-gpt-oss-20b/snapshots/69ee22df6ed1ab8f886ecc1cd7b101ce5b71ab53' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'rope_theta'}\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'rope_theta'}\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3658, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/adapters.py\", line 173, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     super().__init__(vllm_config=vllm_config, prefix=prefix, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gpt_oss.py\", line 691, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.model = GptOssModel(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                  ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gpt_oss.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gpt_oss.py\", line 260, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     lambda prefix: TransformerBlock(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gpt_oss.py\", line 217, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.mlp = MLPBlock(vllm_config, self.layer_idx, prefix=f\"{prefix}.mlp\")\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gpt_oss.py\", line 164, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.experts = FusedMoE(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m                    ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 651, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     self.quant_method.create_weights(layer=self, **moe_quant_params)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py\", line 154, in create_weights\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     torch.empty(\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_device.py\", line 103, in __torch_function__\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545641)\u001b[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.96 GiB. GPU 0 has a total capacity of 139.81 GiB of which 407.94 MiB is free. Including non-PyTorch memory, this process has 139.40 GiB memory in use. Of the allocated memory 138.91 GiB is allocated by PyTorch, and 61.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W120 17:08:44.149808274 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1545457)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-guardian-3.2-5b-lora-harm-correction": {
      "model_id": "ibm-granite/granite-guardian-3.2-5b-lora-harm-correction",
      "model_name": "ibm-granite/granite-guardian-3.2-5b-lora-harm-correction",
      "size_bytes": 66082888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T17:08:14.829549",
      "end_time": "2026-01-20T17:08:56.347628",
      "duration_seconds": 41.52,
      "vllm_pid": 1545456,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/granite.py\", line 475, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m ValueError: There is no module or parameter named 'base_model' in GraniteForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1545635)\u001b[0;0m \n[rank0]:[W120 17:08:44.197555500 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1545456)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Felladrin/Minueza-32M-Base": {
      "model_id": "Felladrin/Minueza-32M-Base",
      "model_name": "Felladrin/Minueza-32M-Base",
      "size_bytes": 32792760,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T17:08:14.338023",
      "end_time": "2026-01-20T17:14:03.412286",
      "duration_seconds": 349.07,
      "vllm_pid": 2179506,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Connection error.\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 982, in request\n    response = self._client.send(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 845, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1014, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PleIAs/Monad": {
      "model_id": "PleIAs/Monad",
      "model_name": "PleIAs/Monad",
      "size_bytes": 56656128,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T17:08:14.803097",
      "end_time": "2026-01-20T17:17:12.878671",
      "duration_seconds": 538.08,
      "vllm_pid": 1545455,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1378 input tokens (1536 > 2048 - 1378). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 845, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 110, in generate\n    return self.handle_bad_request(ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 150, in handle_bad_request\n    return openai_handle_bad_request(self.model_name, ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 569, in openai_handle_bad_request\n    raise e\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1378 input tokens (1536 > 2048 - 1378). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Hermes-4-14B": {
      "model_id": "NousResearch/Hermes-4-14B",
      "model_name": "NousResearch/Hermes-4-14B",
      "size_bytes": 424960,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T17:08:14.394483",
      "end_time": "2026-01-20T17:42:51.620084",
      "duration_seconds": 2077.23,
      "vllm_pid": 2179507,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "janhq/Jan-v1-2509": {
      "model_id": "janhq/Jan-v1-2509",
      "model_name": "janhq/Jan-v1-2509",
      "size_bytes": 196096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T17:08:14.336822",
      "end_time": "2026-01-20T18:25:40.967606",
      "duration_seconds": 4646.63,
      "vllm_pid": 2179509,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/japanese-gpt2-small": {
      "model_id": "rinna/japanese-gpt2-small",
      "model_name": "rinna/japanese-gpt2-small",
      "size_bytes": 123001344,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.330677",
      "end_time": "2026-01-21T17:10:19.330683",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mshenoda/roberta-spam": {
      "model_id": "mshenoda/roberta-spam",
      "model_name": "mshenoda/roberta-spam",
      "size_bytes": 124647684,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.348855",
      "end_time": "2026-01-21T17:10:19.348863",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ElKulako/cryptobert": {
      "model_id": "ElKulako/cryptobert",
      "model_name": "ElKulako/cryptobert",
      "size_bytes": 124648453,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.367255",
      "end_time": "2026-01-21T17:10:19.367262",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "neulab/codebert-python": {
      "model_id": "neulab/codebert-python",
      "model_name": "neulab/codebert-python",
      "size_bytes": 124697947,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.387803",
      "end_time": "2026-01-21T17:10:19.387819",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KB/bert-base-swedish-cased": {
      "model_id": "KB/bert-base-swedish-cased",
      "model_name": "KB/bert-base-swedish-cased",
      "size_bytes": 125333397,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.406708",
      "end_time": "2026-01-21T17:10:19.406715",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Shekswess/trlm-135m": {
      "model_id": "Shekswess/trlm-135m",
      "model_name": "Shekswess/trlm-135m",
      "size_bytes": 134516160,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.425003",
      "end_time": "2026-01-21T17:10:19.425011",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "roneneldan/TinyStories-1M": {
      "model_id": "roneneldan/TinyStories-1M",
      "model_name": "roneneldan/TinyStories-1M",
      "size_bytes": 135262298,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.443392",
      "end_time": "2026-01-21T17:10:19.443400",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lxyuan/distilbert-base-multilingual-cased-sentiments-student": {
      "model_id": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
      "model_name": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
      "size_bytes": 135326979,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.461870",
      "end_time": "2026-01-21T17:10:19.461877",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cross-encoder/nli-deberta-base": {
      "model_id": "cross-encoder/nli-deberta-base",
      "model_name": "cross-encoder/nli-deberta-base",
      "size_bytes": 139195139,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.480357",
      "end_time": "2026-01-21T17:10:19.480364",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KoalaAI/Text-Moderation": {
      "model_id": "KoalaAI/Text-Moderation",
      "model_name": "KoalaAI/Text-Moderation",
      "size_bytes": 139199753,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.498489",
      "end_time": "2026-01-21T17:10:19.498496",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenMOSS-Team/bart-base-chinese": {
      "model_id": "OpenMOSS-Team/bart-base-chinese",
      "model_name": "OpenMOSS-Team/bart-base-chinese",
      "size_bytes": 140244295,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.516770",
      "end_time": "2026-01-21T17:10:19.516778",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "m-a-p/YuE-upsampler": {
      "model_id": "m-a-p/YuE-upsampler",
      "model_name": "m-a-p/YuE-upsampler",
      "size_bytes": 145222508,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.535031",
      "end_time": "2026-01-21T17:10:19.535037",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cross-encoder/nli-deberta-v3-base": {
      "model_id": "cross-encoder/nli-deberta-v3-base",
      "model_name": "cross-encoder/nli-deberta-v3-base",
      "size_bytes": 184424963,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.553439",
      "end_time": "2026-01-21T17:10:19.553446",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yangheng/deberta-v3-base-absa-v1.1": {
      "model_id": "yangheng/deberta-v3-base-absa-v1.1",
      "model_name": "yangheng/deberta-v3-base-absa-v1.1",
      "size_bytes": 184424963,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.571633",
      "end_time": "2026-01-21T17:10:19.571640",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Roblox/Llama-3.1-8B-Instruct-RobloxGuard-1.0": {
      "model_id": "Roblox/Llama-3.1-8B-Instruct-RobloxGuard-1.0",
      "model_name": "Roblox/Llama-3.1-8B-Instruct-RobloxGuard-1.0",
      "size_bytes": 185047867,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.589937",
      "end_time": "2026-01-21T17:10:19.589944",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/gpt-oss-120b-Eagle3-long-context": {
      "model_id": "nvidia/gpt-oss-120b-Eagle3-long-context",
      "model_name": "nvidia/gpt-oss-120b-Eagle3-long-context",
      "size_bytes": 215481600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.608251",
      "end_time": "2026-01-21T17:10:19.608258",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Ateeqq/Text-Rewriter-Paraphraser": {
      "model_id": "Ateeqq/Text-Rewriter-Paraphraser",
      "model_name": "Ateeqq/Text-Rewriter-Paraphraser",
      "size_bytes": 222903552,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.626597",
      "end_time": "2026-01-21T17:10:19.626603",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "recogna-nlp/ptt5-base-summ-xlsum": {
      "model_id": "recogna-nlp/ptt5-base-summ-xlsum",
      "model_name": "recogna-nlp/ptt5-base-summ-xlsum",
      "size_bytes": 222903552,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.644815",
      "end_time": "2026-01-21T17:10:19.644822",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "protonx-models/protonx-legal-tc": {
      "model_id": "protonx-models/protonx-legal-tc",
      "model_name": "protonx-models/protonx-legal-tc",
      "size_bytes": 225950976,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.663080",
      "end_time": "2026-01-21T17:10:19.663087",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mxmax/Chinese_Chat_T5_Base": {
      "model_id": "mxmax/Chinese_Chat_T5_Base",
      "model_name": "mxmax/Chinese_Chat_T5_Base",
      "size_bytes": 247577856,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.681419",
      "end_time": "2026-01-21T17:10:19.681429",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pszemraj/long-t5-tglobal-base-16384-book-summary": {
      "model_id": "pszemraj/long-t5-tglobal-base-16384-book-summary",
      "model_name": "pszemraj/long-t5-tglobal-base-16384-book-summary",
      "size_bytes": 247587456,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.699872",
      "end_time": "2026-01-21T17:10:19.699879",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ross-dev/sexyGPT-Uncensored": {
      "model_id": "ross-dev/sexyGPT-Uncensored",
      "model_name": "ross-dev/sexyGPT-Uncensored",
      "size_bytes": 124442880,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:25.384454",
      "end_time": "2026-01-20T18:48:27.020405",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank": {
      "model_id": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
      "model_name": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
      "size_bytes": 177341186,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.034018",
      "end_time": "2026-01-20T18:48:27.051564",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tabularisai/multilingual-sentiment-analysis": {
      "model_id": "tabularisai/multilingual-sentiment-analysis",
      "model_name": "tabularisai/multilingual-sentiment-analysis",
      "size_bytes": 135328517,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:25.376147",
      "end_time": "2026-01-20T18:48:27.020457",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nomic-ai/nomic-bert-2048": {
      "model_id": "nomic-ai/nomic-bert-2048",
      "model_name": "nomic-ai/nomic-bert-2048",
      "size_bytes": 137323008,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:25.398622",
      "end_time": "2026-01-20T18:48:27.021462",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EbanLee/kobart-summary-v3": {
      "model_id": "EbanLee/kobart-summary-v3",
      "model_name": "EbanLee/kobart-summary-v3",
      "size_bytes": 123889968,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:25.374075",
      "end_time": "2026-01-20T18:48:27.023902",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "answerdotai/ModernBERT-base": {
      "model_id": "answerdotai/ModernBERT-base",
      "model_name": "answerdotai/ModernBERT-base",
      "size_bytes": 149655232,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.032670",
      "end_time": "2026-01-20T18:48:27.056630",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KBlueLeaf/TIPO-200M-ft2": {
      "model_id": "KBlueLeaf/TIPO-200M-ft2",
      "model_name": "KBlueLeaf/TIPO-200M-ft2",
      "size_bytes": 202557696,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.056870",
      "end_time": "2026-01-20T18:48:27.075012",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sbintuitions/modernbert-ja-130m": {
      "model_id": "sbintuitions/modernbert-ja-130m",
      "model_name": "sbintuitions/modernbert-ja-130m",
      "size_bytes": 132505600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:25.374012",
      "end_time": "2026-01-20T18:48:27.021440",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-135M": {
      "model_id": "HuggingFaceTB/SmolLM2-135M",
      "model_name": "HuggingFaceTB/SmolLM2-135M",
      "size_bytes": 134515008,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:25.397754",
      "end_time": "2026-01-20T18:48:27.025649",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "knowledgator/gliclass-modern-base-v2.0-init": {
      "model_id": "knowledgator/gliclass-modern-base-v2.0-init",
      "model_name": "knowledgator/gliclass-modern-base-v2.0-init",
      "size_bytes": 151378176,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.032241",
      "end_time": "2026-01-20T18:48:27.052848",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepvk/RuModernBERT-base": {
      "model_id": "deepvk/RuModernBERT-base",
      "model_name": "deepvk/RuModernBERT-base",
      "size_bytes": 149655232,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.032070",
      "end_time": "2026-01-20T18:48:27.066355",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepvk/GeRaCl-USER2-base": {
      "model_id": "deepvk/GeRaCl-USER2-base",
      "model_name": "deepvk/GeRaCl-USER2-base",
      "size_bytes": 155312896,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.033063",
      "end_time": "2026-01-20T18:48:27.047348",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "neavo/modern_bert_multilingual": {
      "model_id": "neavo/modern_bert_multilingual",
      "model_name": "neavo/modern_bert_multilingual",
      "size_bytes": 227564160,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.058238",
      "end_time": "2026-01-20T18:48:27.080243",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "charent/ChatLM-mini-Chinese": {
      "model_id": "charent/ChatLM-mini-Chinese",
      "model_name": "charent/ChatLM-mini-Chinese",
      "size_bytes": 187692288,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.055888",
      "end_time": "2026-01-20T18:48:27.077214",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-3-270m": {
      "model_id": "google/gemma-3-270m",
      "model_name": "google/gemma-3-270m",
      "size_bytes": 268098176,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.070687",
      "end_time": "2026-01-20T18:48:27.092884",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "protectai/deberta-v3-base-prompt-injection": {
      "model_id": "protectai/deberta-v3-base-prompt-injection",
      "model_name": "protectai/deberta-v3-base-prompt-injection",
      "size_bytes": 184423682,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.052753",
      "end_time": "2026-01-20T18:48:27.079332",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai4privacy/llama-ai4privacy-multilingual-categorical-anonymiser-openpii": {
      "model_id": "ai4privacy/llama-ai4privacy-multilingual-categorical-anonymiser-openpii",
      "model_name": "ai4privacy/llama-ai4privacy-multilingual-categorical-anonymiser-openpii",
      "size_bytes": 149635624,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:27.031278",
      "end_time": "2026-01-20T18:48:27.051482",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KRLabsOrg/lettucedect-base-modernbert-en-v1": {
      "model_id": "KRLabsOrg/lettucedect-base-modernbert-en-v1",
      "model_name": "KRLabsOrg/lettucedect-base-modernbert-en-v1",
      "size_bytes": 149606402,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.085455",
      "end_time": "2026-01-20T18:48:27.803277",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PleIAs/celadon": {
      "model_id": "PleIAs/celadon",
      "model_name": "PleIAs/celadon",
      "size_bytes": 141319700,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.093857",
      "end_time": "2026-01-20T18:48:27.802835",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tasksource/ModernBERT-base-nli": {
      "model_id": "tasksource/ModernBERT-base-nli",
      "model_name": "tasksource/ModernBERT-base-nli",
      "size_bytes": 149607171,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.092571",
      "end_time": "2026-01-20T18:48:27.818725",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "skt/A.X-Encoder-base": {
      "model_id": "skt/A.X-Encoder-base",
      "model_name": "skt/A.X-Encoder-base",
      "size_bytes": 149372240,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.090986",
      "end_time": "2026-01-20T18:48:27.820381",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MoritzLaurer/ModernBERT-base-zeroshot-v2.0": {
      "model_id": "MoritzLaurer/ModernBERT-base-zeroshot-v2.0",
      "model_name": "MoritzLaurer/ModernBERT-base-zeroshot-v2.0",
      "size_bytes": 149606402,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.087974",
      "end_time": "2026-01-20T18:48:27.818284",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai4privacy/llama-ai4privacy-english-anonymiser-openpii": {
      "model_id": "ai4privacy/llama-ai4privacy-english-anonymiser-openpii",
      "model_name": "ai4privacy/llama-ai4privacy-english-anonymiser-openpii",
      "size_bytes": 149607171,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.081304",
      "end_time": "2026-01-20T18:48:27.832537",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mirth/chonky_mmbert_small_multilingual_1": {
      "model_id": "mirth/chonky_mmbert_small_multilingual_1",
      "model_name": "mirth/chonky_mmbert_small_multilingual_1",
      "size_bytes": 140642306,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.082444",
      "end_time": "2026-01-20T18:48:27.834565",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "clapAI/modernBERT-base-multilingual-sentiment": {
      "model_id": "clapAI/modernBERT-base-multilingual-sentiment",
      "model_name": "clapAI/modernBERT-base-multilingual-sentiment",
      "size_bytes": 149607171,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:48:26.091723",
      "end_time": "2026-01-20T18:48:27.847504",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nikity/lille-130m-instruct": {
      "model_id": "Nikity/lille-130m-instruct",
      "model_name": "Nikity/lille-130m-instruct",
      "size_bytes": 127236768,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:48:25.373998",
      "end_time": "2026-01-20T18:48:47.128106",
      "duration_seconds": 21.75,
      "vllm_pid": 2387565,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m   Value error, The checkpoint you are trying to load has model type `lille-130m` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m \n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2387565)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-3-270m-it": {
      "model_id": "google/gemma-3-270m-it",
      "model_name": "google/gemma-3-270m-it",
      "size_bytes": 268098176,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:48:27.078943",
      "end_time": "2026-01-20T18:49:58.829363",
      "duration_seconds": 91.75,
      "vllm_pid": 2387568,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--google--gemma-3-270m-it/snapshots/ac82b4e820549b854eebf28ce6dedaf9fdfa17b3` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-135M-Instruct": {
      "model_id": "HuggingFaceTB/SmolLM2-135M-Instruct",
      "model_name": "HuggingFaceTB/SmolLM2-135M-Instruct",
      "size_bytes": 134515008,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:48:25.377791",
      "end_time": "2026-01-20T18:49:58.830293",
      "duration_seconds": 93.45,
      "vllm_pid": 2387566,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Felladrin/TinyMistral-248M-Chat-v4": {
      "model_id": "Felladrin/TinyMistral-248M-Chat-v4",
      "model_name": "Felladrin/TinyMistral-248M-Chat-v4",
      "size_bytes": 248024064,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:48:27.060969",
      "end_time": "2026-01-20T18:50:15.177793",
      "duration_seconds": 108.12,
      "vllm_pid": 2387567,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Felladrin--TinyMistral-248M-Chat-v4/snapshots/dcd3a7c4d80c2f8c338eea58d8067460c06a027f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Felladrin--TinyMistral-248M-Chat-v4/snapshots/dcd3a7c4d80c2f8c338eea58d8067460c06a027f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "classla/xlm-roberta-base-multilingual-text-genre-classifier": {
      "model_id": "classla/xlm-roberta-base-multilingual-text-genre-classifier",
      "model_name": "classla/xlm-roberta-base-multilingual-text-genre-classifier",
      "size_bytes": 278051083,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.718074",
      "end_time": "2026-01-21T17:10:19.718081",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "haykgrigorian/v2mini-eval1": {
      "model_id": "haykgrigorian/v2mini-eval1",
      "model_name": "haykgrigorian/v2mini-eval1",
      "size_bytes": 297575424,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.736301",
      "end_time": "2026-01-21T17:10:19.736308",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "naver/efficient-splade-VI-BT-large-query": {
      "model_id": "naver/efficient-splade-VI-BT-large-query",
      "model_name": "naver/efficient-splade-VI-BT-large-query",
      "size_bytes": 303317786,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.754471",
      "end_time": "2026-01-21T17:10:19.754478",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ctheodoris/Geneformer": {
      "model_id": "ctheodoris/Geneformer",
      "model_name": "ctheodoris/Geneformer",
      "size_bytes": 316354867,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.772852",
      "end_time": "2026-01-21T17:10:19.772860",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PleIAs/Pleias-350m-Preview": {
      "model_id": "PleIAs/Pleias-350m-Preview",
      "model_name": "PleIAs/Pleias-350m-Preview",
      "size_bytes": 353424384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.791159",
      "end_time": "2026-01-21T17:10:19.791167",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "obi/deid_roberta_i2b2": {
      "model_id": "obi/deid_roberta_i2b2",
      "model_name": "obi/deid_roberta_i2b2",
      "size_bytes": 354356783,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.809578",
      "end_time": "2026-01-21T17:10:19.809585",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/japanese-gpt2-medium": {
      "model_id": "rinna/japanese-gpt2-medium",
      "model_name": "rinna/japanese-gpt2-medium",
      "size_bytes": 361293824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.827907",
      "end_time": "2026-01-21T17:10:19.827914",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "manifestai/powercoder-3b": {
      "model_id": "manifestai/powercoder-3b",
      "model_name": "manifestai/powercoder-3b",
      "size_bytes": 378819426,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.846065",
      "end_time": "2026-01-21T17:10:19.846071",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenMOSS-Team/bart-large-chinese": {
      "model_id": "OpenMOSS-Team/bart-large-chinese",
      "model_name": "OpenMOSS-Team/bart-large-chinese",
      "size_bytes": 407372871,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.864186",
      "end_time": "2026-01-21T17:10:19.864193",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openai/circuit-sparsity": {
      "model_id": "openai/circuit-sparsity",
      "model_name": "openai/circuit-sparsity",
      "size_bytes": 419124736,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.882420",
      "end_time": "2026-01-21T17:10:19.882427",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cross-encoder/nli-deberta-v3-large": {
      "model_id": "cross-encoder/nli-deberta-v3-large",
      "model_name": "cross-encoder/nli-deberta-v3-large",
      "size_bytes": 435065347,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.900634",
      "end_time": "2026-01-21T17:10:19.900641",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yangheng/deberta-v3-large-absa-v1.1": {
      "model_id": "yangheng/deberta-v3-large-absa-v1.1",
      "model_name": "yangheng/deberta-v3-large-absa-v1.1",
      "size_bytes": 435065347,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.918912",
      "end_time": "2026-01-21T17:10:19.918919",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PKOBP/polish-roberta-8k": {
      "model_id": "PKOBP/polish-roberta-8k",
      "model_name": "PKOBP/polish-roberta-8k",
      "size_bytes": 442890240,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.937217",
      "end_time": "2026-01-21T17:10:19.937224",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/Dolphin3.0-Qwen2.5-0.5B": {
      "model_id": "dphn/Dolphin3.0-Qwen2.5-0.5B",
      "model_name": "dphn/Dolphin3.0-Qwen2.5-0.5B",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.955352",
      "end_time": "2026-01-21T17:10:19.955358",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "InstaDeepAI/nucleotide-transformer-v2-500m-multi-species": {
      "model_id": "InstaDeepAI/nucleotide-transformer-v2-500m-multi-species",
      "model_name": "InstaDeepAI/nucleotide-transformer-v2-500m-multi-species",
      "size_bytes": 498346364,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.973674",
      "end_time": "2026-01-21T17:10:19.973681",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TurboPascal/ChineseModernBert": {
      "model_id": "TurboPascal/ChineseModernBert",
      "model_name": "TurboPascal/ChineseModernBert",
      "size_bytes": 499988864,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:19.991932",
      "end_time": "2026-01-21T17:10:19.991939",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jjzha/esco-xlm-roberta-large": {
      "model_id": "jjzha/esco-xlm-roberta-large",
      "model_name": "jjzha/esco-xlm-roberta-large",
      "size_bytes": 561195671,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.010245",
      "end_time": "2026-01-21T17:10:20.010252",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "osmosis-ai/Osmosis-Structure-0.6B": {
      "model_id": "osmosis-ai/Osmosis-Structure-0.6B",
      "model_name": "osmosis-ai/Osmosis-Structure-0.6B",
      "size_bytes": 596049920,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.028616",
      "end_time": "2026-01-21T17:10:20.028623",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "qualifire/prompt-injection-jailbreak-sentinel-v2": {
      "model_id": "qualifire/prompt-injection-jailbreak-sentinel-v2",
      "model_name": "qualifire/prompt-injection-jailbreak-sentinel-v2",
      "size_bytes": 596051968,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.046665",
      "end_time": "2026-01-21T17:10:20.046672",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EssentialAI/eai-distill-0.5b": {
      "model_id": "EssentialAI/eai-distill-0.5b",
      "model_name": "EssentialAI/eai-distill-0.5b",
      "size_bytes": 630167424,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.064963",
      "end_time": "2026-01-21T17:10:20.064969",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5-small": {
      "model_id": "Salesforce/codet5-small",
      "model_name": "Salesforce/codet5-small",
      "size_bytes": 725995651,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.083040",
      "end_time": "2026-01-21T17:10:20.083046",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "opendatalab/MinerU-HTML": {
      "model_id": "opendatalab/MinerU-HTML",
      "model_name": "opendatalab/MinerU-HTML",
      "size_bytes": 751632384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.101349",
      "end_time": "2026-01-21T17:10:20.101356",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MoritzLaurer/xlm-v-base-mnli-xnli": {
      "model_id": "MoritzLaurer/xlm-v-base-mnli-xnli",
      "model_name": "MoritzLaurer/xlm-v-base-mnli-xnli",
      "size_bytes": 778496005,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.119894",
      "end_time": "2026-01-21T17:10:20.119901",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "gaussalgo/T5-LM-Large-text2sql-spider": {
      "model_id": "gaussalgo/T5-LM-Large-text2sql-spider",
      "model_name": "gaussalgo/T5-LM-Large-text2sql-spider",
      "size_bytes": 783150080,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.138419",
      "end_time": "2026-01-21T17:10:20.138426",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "line-corporation/line-distilbert-base-japanese": {
      "model_id": "line-corporation/line-distilbert-base-japanese",
      "model_name": "line-corporation/line-distilbert-base-japanese",
      "size_bytes": 825061423,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.156695",
      "end_time": "2026-01-21T17:10:20.156701",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/jinaai-ReaderLM-v2": {
      "model_id": "mlx-community/jinaai-ReaderLM-v2",
      "model_name": "mlx-community/jinaai-ReaderLM-v2",
      "size_bytes": 880050443,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.174746",
      "end_time": "2026-01-21T17:10:20.174753",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/VibeThinker-1.5B-mlx-4bit": {
      "model_id": "mlx-community/VibeThinker-1.5B-mlx-4bit",
      "model_name": "mlx-community/VibeThinker-1.5B-mlx-4bit",
      "size_bytes": 880051349,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.192917",
      "end_time": "2026-01-21T17:10:20.192924",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PORTULAN/albertina-900m-portuguese-ptbr-encoder-brwac": {
      "model_id": "PORTULAN/albertina-900m-portuguese-ptbr-encoder-brwac",
      "model_name": "PORTULAN/albertina-900m-portuguese-ptbr-encoder-brwac",
      "size_bytes": 887085668,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.210999",
      "end_time": "2026-01-21T17:10:20.211005",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PORTULAN/albertina-900m-portuguese-ptpt-encoder": {
      "model_id": "PORTULAN/albertina-900m-portuguese-ptpt-encoder",
      "model_name": "PORTULAN/albertina-900m-portuguese-ptpt-encoder",
      "size_bytes": 887085668,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.229266",
      "end_time": "2026-01-21T17:10:20.229273",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5p-220m": {
      "model_id": "Salesforce/codet5p-220m",
      "model_name": "Salesforce/codet5p-220m",
      "size_bytes": 891595477,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.247366",
      "end_time": "2026-01-21T17:10:20.247372",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-Flash-1B": {
      "model_id": "nvidia/Nemotron-Flash-1B",
      "model_name": "nvidia/Nemotron-Flash-1B",
      "size_bytes": 965389440,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.265847",
      "end_time": "2026-01-21T17:10:20.265854",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Lamapi/next-1b": {
      "model_id": "Lamapi/next-1b",
      "model_name": "Lamapi/next-1b",
      "size_bytes": 999885952,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.284255",
      "end_time": "2026-01-21T17:10:20.284262",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "skt/ko-gpt-trinity-1.2B-v0.5": {
      "model_id": "skt/ko-gpt-trinity-1.2B-v0.5",
      "model_name": "skt/ko-gpt-trinity-1.2B-v0.5",
      "size_bytes": 1187721984,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.302387",
      "end_time": "2026-01-21T17:10:20.302393",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit": {
      "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit",
      "model_name": "mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit",
      "size_bytes": 1190221312,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.320520",
      "end_time": "2026-01-21T17:10:20.320526",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/Zamba2-1.2B": {
      "model_id": "Zyphra/Zamba2-1.2B",
      "model_name": "Zyphra/Zamba2-1.2B",
      "size_bytes": 1215064704,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.338635",
      "end_time": "2026-01-21T17:10:20.338642",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "amazon/bort": {
      "model_id": "amazon/bort",
      "model_name": "amazon/bort",
      "size_bytes": 1223293596,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.356866",
      "end_time": "2026-01-21T17:10:20.356873",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ContactDoctor/Bio-Medical-Llama-3-2-1B-CoT-012025": {
      "model_id": "ContactDoctor/Bio-Medical-Llama-3-2-1B-CoT-012025",
      "model_name": "ContactDoctor/Bio-Medical-Llama-3-2-1B-CoT-012025",
      "size_bytes": 1235814400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.375138",
      "end_time": "2026-01-21T17:10:20.375145",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/Dolphin3.0-Llama3.2-1B": {
      "model_id": "dphn/Dolphin3.0-Llama3.2-1B",
      "model_name": "dphn/Dolphin3.0-Llama3.2-1B",
      "size_bytes": 1235818496,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.393270",
      "end_time": "2026-01-21T17:10:20.393276",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "fastino/gliner2-multi-v1": {
      "model_id": "fastino/gliner2-multi-v1",
      "model_name": "fastino/gliner2-multi-v1",
      "size_bytes": 1249064342,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.411519",
      "end_time": "2026-01-21T17:10:20.411526",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/phi-1": {
      "model_id": "microsoft/phi-1",
      "model_name": "microsoft/phi-1",
      "size_bytes": 1418270720,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.429954",
      "end_time": "2026-01-21T17:10:20.429961",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/phi-1_5": {
      "model_id": "microsoft/phi-1_5",
      "model_name": "microsoft/phi-1_5",
      "size_bytes": 1418270720,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.448210",
      "end_time": "2026-01-21T17:10:20.448217",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Nano_Imp_1B": {
      "model_id": "SicariusSicariiStuff/Nano_Imp_1B",
      "model_name": "SicariusSicariiStuff/Nano_Imp_1B",
      "size_bytes": 1498482688,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.466581",
      "end_time": "2026-01-21T17:10:20.466588",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/gpt-oss-120b-Eagle3-throughput": {
      "model_id": "nvidia/gpt-oss-120b-Eagle3-throughput",
      "model_name": "nvidia/gpt-oss-120b-Eagle3-throughput",
      "size_bytes": 1523982032,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.484955",
      "end_time": "2026-01-21T17:10:20.484962",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Rakuten/RakutenAI-2.0-mini-instruct": {
      "model_id": "Rakuten/RakutenAI-2.0-mini-instruct",
      "model_name": "Rakuten/RakutenAI-2.0-mini-instruct",
      "size_bytes": 1534683136,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.503071",
      "end_time": "2026-01-21T17:10:20.503078",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PKU-DS-LAB/Fairy-plus-minus-i-700M": {
      "model_id": "PKU-DS-LAB/Fairy-plus-minus-i-700M",
      "model_name": "PKU-DS-LAB/Fairy-plus-minus-i-700M",
      "size_bytes": 1555983360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.521148",
      "end_time": "2026-01-21T17:10:20.521155",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BAAI/OpenSeek-Small-v1": {
      "model_id": "BAAI/OpenSeek-Small-v1",
      "model_name": "BAAI/OpenSeek-Small-v1",
      "size_bytes": 1588968512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.539472",
      "end_time": "2026-01-21T17:10:20.539480",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tengyunw/qwen3_8b_eagle3": {
      "model_id": "Tengyunw/qwen3_8b_eagle3",
      "model_name": "Tengyunw/qwen3_8b_eagle3",
      "size_bytes": 1598950374,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.557742",
      "end_time": "2026-01-21T17:10:20.557750",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "neo4j/text2cypher-gemma-2-9b-it-finetuned-2024v1": {
      "model_id": "neo4j/text2cypher-gemma-2-9b-it-finetuned-2024v1",
      "model_name": "neo4j/text2cypher-gemma-2-9b-it-finetuned-2024v1",
      "size_bytes": 1648094015,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.576127",
      "end_time": "2026-01-21T17:10:20.576136",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "fastino/gliner2-base-v1": {
      "model_id": "fastino/gliner2-base-v1",
      "model_name": "fastino/gliner2-base-v1",
      "size_bytes": 1670340832,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.594168",
      "end_time": "2026-01-21T17:10:20.594175",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa": {
      "model_id": "ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa",
      "model_name": "ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa",
      "size_bytes": 1770141117,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.612248",
      "end_time": "2026-01-21T17:10:20.612255",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lightblue/DeepSeek-R1-Distill-Qwen-1.5B-Multilingual": {
      "model_id": "lightblue/DeepSeek-R1-Distill-Qwen-1.5B-Multilingual",
      "model_name": "lightblue/DeepSeek-R1-Distill-Qwen-1.5B-Multilingual",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.630482",
      "end_time": "2026-01-21T17:10:20.630489",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/DLER-R1-1.5B-Research": {
      "model_id": "nvidia/DLER-R1-1.5B-Research",
      "model_name": "nvidia/DLER-R1-1.5B-Research",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.648746",
      "end_time": "2026-01-21T17:10:20.648753",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "fastino/gliner2-large-v1": {
      "model_id": "fastino/gliner2-large-v1",
      "model_name": "fastino/gliner2-large-v1",
      "size_bytes": 1948292756,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.667288",
      "end_time": "2026-01-21T17:10:20.667295",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cardiffnlp/twitter-roberta-base-sentiment-latest": {
      "model_id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
      "model_name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
      "size_bytes": 1999747098,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.685776",
      "end_time": "2026-01-21T17:10:20.685786",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "osmosis-ai/Osmosis-Apply-1.7B": {
      "model_id": "osmosis-ai/Osmosis-Apply-1.7B",
      "model_name": "osmosis-ai/Osmosis-Apply-1.7B",
      "size_bytes": 2031739904,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.703915",
      "end_time": "2026-01-21T17:10:20.703922",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "thu-pacman/PCMind-2.1-Kaiyuan-2B": {
      "model_id": "thu-pacman/PCMind-2.1-Kaiyuan-2B",
      "model_name": "thu-pacman/PCMind-2.1-Kaiyuan-2B",
      "size_bytes": 2031854592,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.722384",
      "end_time": "2026-01-21T17:10:20.722391",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sdobson/nanochat": {
      "model_id": "sdobson/nanochat",
      "model_name": "sdobson/nanochat",
      "size_bytes": 2077340196,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.740525",
      "end_time": "2026-01-21T17:10:20.740532",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Unbabel/wmt22-cometkiwi-da": {
      "model_id": "Unbabel/wmt22-cometkiwi-da",
      "model_name": "Unbabel/wmt22-cometkiwi-da",
      "size_bytes": 2265803756,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.758935",
      "end_time": "2026-01-21T17:10:20.758942",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-350M-multi": {
      "model_id": "Salesforce/codegen-350M-multi",
      "model_name": "Salesforce/codegen-350M-multi",
      "size_bytes": 2312383274,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.777290",
      "end_time": "2026-01-21T17:10:20.777297",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ehsanaghaei/SecureBERT": {
      "model_id": "ehsanaghaei/SecureBERT",
      "model_name": "ehsanaghaei/SecureBERT",
      "size_bytes": 2494249430,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.795993",
      "end_time": "2026-01-21T17:10:20.796000",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/2B_or_not_2B": {
      "model_id": "SicariusSicariiStuff/2B_or_not_2B",
      "model_name": "SicariusSicariiStuff/2B_or_not_2B",
      "size_bytes": 2506172416,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.815006",
      "end_time": "2026-01-21T17:10:20.815014",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nsi319/legal-led-base-16384": {
      "model_id": "nsi319/legal-led-base-16384",
      "model_name": "nsi319/legal-led-base-16384",
      "size_bytes": 2590620983,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.833941",
      "end_time": "2026-01-21T17:10:20.833948",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/CLaRa-7B-Instruct": {
      "model_id": "apple/CLaRa-7B-Instruct",
      "model_name": "apple/CLaRa-7B-Instruct",
      "size_bytes": 2617964591,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.853322",
      "end_time": "2026-01-21T17:10:20.853329",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PKU-DS-LAB/Fairy-plus-minus-i-1.3B": {
      "model_id": "PKU-DS-LAB/Fairy-plus-minus-i-1.3B",
      "model_name": "PKU-DS-LAB/Fairy-plus-minus-i-1.3B",
      "size_bytes": 2678230976,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.872319",
      "end_time": "2026-01-21T17:10:20.872327",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-Flash-3B-Instruct": {
      "model_id": "nvidia/Nemotron-Flash-3B-Instruct",
      "model_name": "nvidia/Nemotron-Flash-3B-Instruct",
      "size_bytes": 2750007840,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.891038",
      "end_time": "2026-01-21T17:10:20.891045",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "xiuyul/mamba-2.8b-zephyr": {
      "model_id": "xiuyul/mamba-2.8b-zephyr",
      "model_name": "xiuyul/mamba-2.8b-zephyr",
      "size_bytes": 2768345600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.909443",
      "end_time": "2026-01-21T17:10:20.909450",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pszemraj/long-t5-tglobal-xl-16384-book-summary": {
      "model_id": "pszemraj/long-t5-tglobal-xl-16384-book-summary",
      "model_name": "pszemraj/long-t5-tglobal-xl-16384-book-summary",
      "size_bytes": 2849807360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.928222",
      "end_time": "2026-01-21T17:10:20.928230",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5p-770m-py": {
      "model_id": "Salesforce/codet5p-770m-py",
      "model_name": "Salesforce/codet5p-770m-py",
      "size_bytes": 2950692323,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.946703",
      "end_time": "2026-01-21T17:10:20.946710",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5-large": {
      "model_id": "Salesforce/codet5-large",
      "model_name": "Salesforce/codet5-large",
      "size_bytes": 2950763504,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.964903",
      "end_time": "2026-01-21T17:10:20.964911",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5-large-ntp-py": {
      "model_id": "Salesforce/codet5-large-ntp-py",
      "model_name": "Salesforce/codet5-large-ntp-py",
      "size_bytes": 2950763504,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:20.983332",
      "end_time": "2026-01-21T17:10:20.983339",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nunchaku-tech/nunchaku-t5": {
      "model_id": "nunchaku-tech/nunchaku-t5",
      "model_name": "nunchaku-tech/nunchaku-t5",
      "size_bytes": 2986819952,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.001665",
      "end_time": "2026-01-21T17:10:21.001671",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rasbt/qwen3-from-scratch": {
      "model_id": "rasbt/qwen3-from-scratch",
      "model_name": "rasbt/qwen3-from-scratch",
      "size_bytes": 3018181967,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.019897",
      "end_time": "2026-01-21T17:10:21.019905",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MadeAgents/Hammer2.1-3b": {
      "model_id": "MadeAgents/Hammer2.1-3b",
      "model_name": "MadeAgents/Hammer2.1-3b",
      "size_bytes": 3085383680,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.038517",
      "end_time": "2026-01-21T17:10:21.038525",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/Dolphin3.0-Qwen2.5-3b": {
      "model_id": "dphn/Dolphin3.0-Qwen2.5-3b",
      "model_name": "dphn/Dolphin3.0-Qwen2.5-3b",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.057187",
      "end_time": "2026-01-21T17:10:21.057196",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Alibaba-NLP/WebSailor-3B": {
      "model_id": "Alibaba-NLP/WebSailor-3B",
      "model_name": "Alibaba-NLP/WebSailor-3B",
      "size_bytes": 3086200832,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.075376",
      "end_time": "2026-01-21T17:10:21.075383",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mobiuslabsgmbh/Llama-2-7b-chat-hf_1bitgs8_hqq": {
      "model_id": "mobiuslabsgmbh/Llama-2-7b-chat-hf_1bitgs8_hqq",
      "model_name": "mobiuslabsgmbh/Llama-2-7b-chat-hf_1bitgs8_hqq",
      "size_bytes": 3098128425,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.093670",
      "end_time": "2026-01-21T17:10:21.093677",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "etri-lirs/eagle-3b-preview": {
      "model_id": "etri-lirs/eagle-3b-preview",
      "model_name": "etri-lirs/eagle-3b-preview",
      "size_bytes": 3103869440,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.111963",
      "end_time": "2026-01-21T17:10:21.111970",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "aisingapore/SEA-LION-v1-3B": {
      "model_id": "aisingapore/SEA-LION-v1-3B",
      "model_name": "aisingapore/SEA-LION-v1-3B",
      "size_bytes": 3178255360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.130312",
      "end_time": "2026-01-21T17:10:21.130319",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/FineMath-Llama-3B": {
      "model_id": "HuggingFaceTB/FineMath-Llama-3B",
      "model_name": "HuggingFaceTB/FineMath-Llama-3B",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.148573",
      "end_time": "2026-01-21T17:10:21.148581",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Impish_LLAMA_3B": {
      "model_id": "SicariusSicariiStuff/Impish_LLAMA_3B",
      "model_name": "SicariusSicariiStuff/Impish_LLAMA_3B",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.167799",
      "end_time": "2026-01-21T17:10:21.167807",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TroyDoesAI/BlackSheep-Llama3.2-3B": {
      "model_id": "TroyDoesAI/BlackSheep-Llama3.2-3B",
      "model_name": "TroyDoesAI/BlackSheep-Llama3.2-3B",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.186232",
      "end_time": "2026-01-21T17:10:21.186240",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/Dolphin3.0-Llama3.2-3B": {
      "model_id": "dphn/Dolphin3.0-Llama3.2-3B",
      "model_name": "dphn/Dolphin3.0-Llama3.2-3B",
      "size_bytes": 3212755968,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.204352",
      "end_time": "2026-01-21T17:10:21.204360",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tiiny/SmallThinker-3B-Preview": {
      "model_id": "Tiiny/SmallThinker-3B-Preview",
      "model_name": "Tiiny/SmallThinker-3B-Preview",
      "size_bytes": 3397103616,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.222574",
      "end_time": "2026-01-21T17:10:21.222581",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/orca_mini_3b": {
      "model_id": "pankajmathur/orca_mini_3b",
      "model_name": "pankajmathur/orca_mini_3b",
      "size_bytes": 3426474900,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.240872",
      "end_time": "2026-01-21T17:10:21.240880",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "urchade/gliner_multi-v2.1": {
      "model_id": "urchade/gliner_multi-v2.1",
      "model_name": "urchade/gliner_multi-v2.1",
      "size_bytes": 3467560618,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.259360",
      "end_time": "2026-01-21T17:10:21.259368",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/AReaL-1.5B-Preview-Stage-1": {
      "model_id": "inclusionAI/AReaL-1.5B-Preview-Stage-1",
      "model_name": "inclusionAI/AReaL-1.5B-Preview-Stage-1",
      "size_bytes": 3554301510,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.277644",
      "end_time": "2026-01-21T17:10:21.277652",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "t-tech/T-pro-it-2.0-eagle": {
      "model_id": "t-tech/T-pro-it-2.0-eagle",
      "model_name": "t-tech/T-pro-it-2.0-eagle",
      "size_bytes": 3587360912,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.296137",
      "end_time": "2026-01-21T17:10:21.296145",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lianghsun/Llama-3.2-Taiwan-3B": {
      "model_id": "lianghsun/Llama-3.2-Taiwan-3B",
      "model_name": "lianghsun/Llama-3.2-Taiwan-3B",
      "size_bytes": 3606752256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.315275",
      "end_time": "2026-01-21T17:10:21.315283",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "twinkle-ai/Llama-3.2-3B-F1-Instruct": {
      "model_id": "twinkle-ai/Llama-3.2-3B-F1-Instruct",
      "model_name": "twinkle-ai/Llama-3.2-3B-F1-Instruct",
      "size_bytes": 3606752256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.333784",
      "end_time": "2026-01-21T17:10:21.333791",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "twinkle-ai/Llama-3.2-3B-F1-Reasoning-Instruct": {
      "model_id": "twinkle-ai/Llama-3.2-3B-F1-Reasoning-Instruct",
      "model_name": "twinkle-ai/Llama-3.2-3B-F1-Reasoning-Instruct",
      "size_bytes": 3606752256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.352302",
      "end_time": "2026-01-21T17:10:21.352311",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jackboyla/glirel-large-v0": {
      "model_id": "jackboyla/glirel-large-v0",
      "model_name": "jackboyla/glirel-large-v0",
      "size_bytes": 3732878510,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.370688",
      "end_time": "2026-01-21T17:10:21.370696",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-350M-mono": {
      "model_id": "Salesforce/codegen-350M-mono",
      "model_name": "Salesforce/codegen-350M-mono",
      "size_bytes": 3740254794,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.389408",
      "end_time": "2026-01-21T17:10:21.389415",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5-base": {
      "model_id": "Salesforce/codet5-base",
      "model_name": "Salesforce/codet5-base",
      "size_bytes": 3764044789,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.407769",
      "end_time": "2026-01-21T17:10:21.407777",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mobiuslabsgmbh/Llama-2-7b-chat-hf_2bitgs8_hqq": {
      "model_id": "mobiuslabsgmbh/Llama-2-7b-chat-hf_2bitgs8_hqq",
      "model_name": "mobiuslabsgmbh/Llama-2-7b-chat-hf_2bitgs8_hqq",
      "size_bytes": 3907629161,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.425827",
      "end_time": "2026-01-21T17:10:21.425834",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nanbeige/Nanbeige4-3B-Thinking-2511": {
      "model_id": "Nanbeige/Nanbeige4-3B-Thinking-2511",
      "model_name": "Nanbeige/Nanbeige4-3B-Thinking-2511",
      "size_bytes": 3933637120,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.444283",
      "end_time": "2026-01-21T17:10:21.444290",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/bilingual-gpt-neox-4b-8k": {
      "model_id": "rinna/bilingual-gpt-neox-4b-8k",
      "model_name": "rinna/bilingual-gpt-neox-4b-8k",
      "size_bytes": 3947117312,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.462546",
      "end_time": "2026-01-21T17:10:21.462553",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cardiffnlp/twitter-roberta-base-hate-latest": {
      "model_id": "cardiffnlp/twitter-roberta-base-hate-latest",
      "model_name": "cardiffnlp/twitter-roberta-base-hate-latest",
      "size_bytes": 3989585503,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.480721",
      "end_time": "2026-01-21T17:10:21.480727",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AllThingsIntel/Apollo-V0.1-4B-Thinking": {
      "model_id": "AllThingsIntel/Apollo-V0.1-4B-Thinking",
      "model_name": "AllThingsIntel/Apollo-V0.1-4B-Thinking",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.499009",
      "end_time": "2026-01-21T17:10:21.499016",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "POLARIS-Project/Polaris-4B-Preview": {
      "model_id": "POLARIS-Project/Polaris-4B-Preview",
      "model_name": "POLARIS-Project/Polaris-4B-Preview",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.517176",
      "end_time": "2026-01-21T17:10:21.517183",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "bralynn/pydevmini1": {
      "model_id": "bralynn/pydevmini1",
      "model_name": "bralynn/pydevmini1",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.536705",
      "end_time": "2026-01-21T17:10:21.536711",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "p-e-w/Qwen3-4B-Instruct-2507-heretic": {
      "model_id": "p-e-w/Qwen3-4B-Instruct-2507-heretic",
      "model_name": "p-e-w/Qwen3-4B-Instruct-2507-heretic",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.554980",
      "end_time": "2026-01-21T17:10:21.554988",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tiiny/SmallThinker-4BA0.6B-Instruct": {
      "model_id": "Tiiny/SmallThinker-4BA0.6B-Instruct",
      "model_name": "Tiiny/SmallThinker-4BA0.6B-Instruct",
      "size_bytes": 4268459520,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.573327",
      "end_time": "2026-01-21T17:10:21.573334",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Fallen-Gemma3-4B-v1": {
      "model_id": "TheDrummer/Fallen-Gemma3-4B-v1",
      "model_name": "TheDrummer/Fallen-Gemma3-4B-v1",
      "size_bytes": 4300079472,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.592980",
      "end_time": "2026-01-21T17:10:21.592988",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "osmosis-ai/osmosis-mcp-4b": {
      "model_id": "osmosis-ai/osmosis-mcp-4b",
      "model_name": "osmosis-ai/osmosis-mcp-4b",
      "size_bytes": 4411424256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.611148",
      "end_time": "2026-01-21T17:10:21.611156",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-270M-Instruct": {
      "model_id": "apple/OpenELM-270M-Instruct",
      "model_name": "apple/OpenELM-270M-Instruct",
      "size_bytes": 271527168,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.828805",
      "end_time": "2026-01-20T18:51:58.537669",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-270M": {
      "model_id": "apple/OpenELM-270M",
      "model_name": "apple/OpenELM-270M",
      "size_bytes": 271527168,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.830504",
      "end_time": "2026-01-20T18:51:58.565832",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "katanemo/Arch-Guard": {
      "model_id": "katanemo/Arch-Guard",
      "model_name": "katanemo/Arch-Guard",
      "size_bytes": 278811651,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.836689",
      "end_time": "2026-01-20T18:51:58.579492",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "iiiorg/piiranha-v1-detect-personal-information": {
      "model_id": "iiiorg/piiranha-v1-detect-personal-information",
      "model_name": "iiiorg/piiranha-v1-detect-personal-information",
      "size_bytes": 278232594,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.854855",
      "end_time": "2026-01-20T18:51:58.585744",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "5CD-AI/visocial-T5-base": {
      "model_id": "5CD-AI/visocial-T5-base",
      "model_name": "5CD-AI/visocial-T5-base",
      "size_bytes": 275933952,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.855098",
      "end_time": "2026-01-20T18:51:58.583336",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-Prompt-Guard-2-86M": {
      "model_id": "meta-llama/Llama-Prompt-Guard-2-86M",
      "model_name": "meta-llama/Llama-Prompt-Guard-2-86M",
      "size_bytes": 278810882,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.838845",
      "end_time": "2026-01-20T18:51:58.594908",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PleIAs/Pleias-RAG-350M": {
      "model_id": "PleIAs/Pleias-RAG-350M",
      "model_name": "PleIAs/Pleias-RAG-350M",
      "size_bytes": 353424384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:58.586174",
      "end_time": "2026-01-20T18:51:58.610974",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Prompt-Guard-86M": {
      "model_id": "meta-llama/Prompt-Guard-86M",
      "model_name": "meta-llama/Prompt-Guard-86M",
      "size_bytes": 278811651,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:56.835304",
      "end_time": "2026-01-20T18:51:58.589426",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "hantian/layoutreader": {
      "model_id": "hantian/layoutreader",
      "model_name": "hantian/layoutreader",
      "size_bytes": 356583422,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:58.616601",
      "end_time": "2026-01-20T18:51:58.628806",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-350m-base": {
      "model_id": "ibm-granite/granite-4.0-350m-base",
      "model_name": "ibm-granite/granite-4.0-350m-base",
      "size_bytes": 352379904,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:58.573000",
      "end_time": "2026-01-20T18:51:58.610782",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-360M": {
      "model_id": "HuggingFaceTB/SmolLM2-360M",
      "model_name": "HuggingFaceTB/SmolLM2-360M",
      "size_bytes": 361821120,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:58.633453",
      "end_time": "2026-01-20T18:51:58.645269",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "keeeeenw/MicroLlama": {
      "model_id": "keeeeenw/MicroLlama",
      "model_name": "keeeeenw/MicroLlama",
      "size_bytes": 304636928,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:57.239261",
      "end_time": "2026-01-20T18:51:58.920021",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Synthyra/ESMplusplus_small": {
      "model_id": "Synthyra/ESMplusplus_small",
      "model_name": "Synthyra/ESMplusplus_small",
      "size_bytes": 332997184,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:57.256534",
      "end_time": "2026-01-20T18:51:58.933214",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "utrobinmv/t5_summary_en_ru_zh_base_2048": {
      "model_id": "utrobinmv/t5_summary_en_ru_zh_base_2048",
      "model_name": "utrobinmv/t5_summary_en_ru_zh_base_2048",
      "size_bytes": 298222848,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:57.241700",
      "end_time": "2026-01-20T18:51:58.938347",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sbintuitions/modernbert-ja-310m": {
      "model_id": "sbintuitions/modernbert-ja-310m",
      "model_name": "sbintuitions/modernbert-ja-310m",
      "size_bytes": 315304960,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:57.265867",
      "end_time": "2026-01-20T18:51:58.949453",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-350m-base": {
      "model_id": "ibm-granite/granite-4.0-h-350m-base",
      "model_name": "ibm-granite/granite-4.0-h-350m-base",
      "size_bytes": 340332224,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:57.258861",
      "end_time": "2026-01-20T18:51:58.949697",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EuroBERT/EuroBERT-210m": {
      "model_id": "EuroBERT/EuroBERT-210m",
      "model_name": "EuroBERT/EuroBERT-210m",
      "size_bytes": 310266624,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:57.259962",
      "end_time": "2026-01-20T18:51:58.944727",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Minos-v1": {
      "model_id": "NousResearch/Minos-v1",
      "model_name": "NousResearch/Minos-v1",
      "size_bytes": 395833346,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.654401",
      "end_time": "2026-01-20T18:51:59.676850",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MoritzLaurer/ModernBERT-large-zeroshot-v2.0": {
      "model_id": "MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
      "model_name": "MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
      "size_bytes": 395833346,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.654293",
      "end_time": "2026-01-20T18:51:59.679089",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "answerdotai/ModernBERT-large": {
      "model_id": "answerdotai/ModernBERT-large",
      "model_name": "answerdotai/ModernBERT-large",
      "size_bytes": 395881664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.654814",
      "end_time": "2026-01-20T18:51:59.678800",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KRLabsOrg/lettucedect-large-modernbert-en-v1": {
      "model_id": "KRLabsOrg/lettucedect-large-modernbert-en-v1",
      "model_name": "KRLabsOrg/lettucedect-large-modernbert-en-v1",
      "size_bytes": 395833346,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.653528",
      "end_time": "2026-01-20T18:51:59.676872",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "hbseong/HarmAug-Guard": {
      "model_id": "hbseong/HarmAug-Guard",
      "model_name": "hbseong/HarmAug-Guard",
      "size_bytes": 435063810,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.689488",
      "end_time": "2026-01-20T18:51:59.712400",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M": {
      "model_id": "OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M",
      "model_name": "OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M",
      "size_bytes": 434015235,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.686526",
      "end_time": "2026-01-20T18:51:59.708919",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "desklib/ai-text-detector-v1.01": {
      "model_id": "desklib/ai-text-detector-v1.01",
      "model_name": "desklib/ai-text-detector-v1.01",
      "size_bytes": 434013185,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.685128",
      "end_time": "2026-01-20T18:51:59.705062",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-450M": {
      "model_id": "apple/OpenELM-450M",
      "model_name": "apple/OpenELM-450M",
      "size_bytes": 457179136,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.710435",
      "end_time": "2026-01-20T18:51:59.732314",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "answerdotai/ModernBERT-Large-Instruct": {
      "model_id": "answerdotai/ModernBERT-Large-Instruct",
      "model_name": "answerdotai/ModernBERT-Large-Instruct",
      "size_bytes": 395881664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.654547",
      "end_time": "2026-01-20T18:51:59.683943",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-450M-Instruct": {
      "model_id": "apple/OpenELM-450M-Instruct",
      "model_name": "apple/OpenELM-450M-Instruct",
      "size_bytes": 457179136,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:51:59.716401",
      "end_time": "2026-01-20T18:51:59.722042",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gemma-3-270m-it": {
      "model_id": "unsloth/gemma-3-270m-it",
      "model_name": "unsloth/gemma-3-270m-it",
      "size_bytes": 268098176,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:56.828750",
      "end_time": "2026-01-20T18:53:34.911083",
      "duration_seconds": 98.08,
      "vllm_pid": 2398337,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--unsloth--gemma-3-270m-it/snapshots/23cf460f6bb16954176b3ddcc8d4f250501458a9` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-350M-PII-Extract-JP": {
      "model_id": "LiquidAI/LFM2-350M-PII-Extract-JP",
      "model_name": "LiquidAI/LFM2-350M-PII-Extract-JP",
      "size_bytes": 354483968,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.615664",
      "end_time": "2026-01-20T18:53:35.460662",
      "duration_seconds": 96.84,
      "vllm_pid": 2398342,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-350M-PII-Extract-JP/snapshots/ce51280fea1c9426c127bb54047de09ec28e4376` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-350M-Extract": {
      "model_id": "LiquidAI/LFM2-350M-Extract",
      "model_name": "LiquidAI/LFM2-350M-Extract",
      "size_bytes": 354483968,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.595567",
      "end_time": "2026-01-20T18:53:35.460740",
      "duration_seconds": 96.87,
      "vllm_pid": 2398339,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-350M-Extract/snapshots/d7c90e1819d7df5415ab1b766f4a9e6d9ef01919` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-350M": {
      "model_id": "LiquidAI/LFM2-350M",
      "model_name": "LiquidAI/LFM2-350M",
      "size_bytes": 354483968,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.589405",
      "end_time": "2026-01-20T18:53:35.460759",
      "duration_seconds": 96.87,
      "vllm_pid": 2398338,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-350M/snapshots/28a2c07ba67e1a153f151fd132636514d7f1752a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-350m": {
      "model_id": "ibm-granite/granite-4.0-350m",
      "model_name": "ibm-granite/granite-4.0-350m",
      "size_bytes": 352379904,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.546071",
      "end_time": "2026-01-20T18:53:35.460718",
      "duration_seconds": 96.91,
      "vllm_pid": 2398336,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-350m/snapshots/bd8a1497065c0d6ba1ef19af6b0d2b14bacf71c2` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-350M-ENJP-MT": {
      "model_id": "LiquidAI/LFM2-350M-ENJP-MT",
      "model_name": "LiquidAI/LFM2-350M-ENJP-MT",
      "size_bytes": 354483968,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.591990",
      "end_time": "2026-01-20T18:53:35.461582",
      "duration_seconds": 96.87,
      "vllm_pid": 2398340,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-350M-ENJP-MT/snapshots/02fcc56f146ca4320c4c2217085fcfb4ac7a7a09` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KBlueLeaf/TIPO-500M-ft": {
      "model_id": "KBlueLeaf/TIPO-500M-ft",
      "model_name": "KBlueLeaf/TIPO-500M-ft",
      "size_bytes": 507989760,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:53:35.464310",
      "end_time": "2026-01-20T18:53:35.479421",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-360M-Instruct": {
      "model_id": "HuggingFaceTB/SmolLM2-360M-Instruct",
      "model_name": "HuggingFaceTB/SmolLM2-360M-Instruct",
      "size_bytes": 361821120,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.649606",
      "end_time": "2026-01-20T18:53:35.657872",
      "duration_seconds": 97.01,
      "vllm_pid": 2398343,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/a10cc1512eabd3dde888204e902eca88bddb4951` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-350M-Math": {
      "model_id": "LiquidAI/LFM2-350M-Math",
      "model_name": "LiquidAI/LFM2-350M-Math",
      "size_bytes": 354483968,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:58.601135",
      "end_time": "2026-01-20T18:53:35.968645",
      "duration_seconds": 97.37,
      "vllm_pid": 2398341,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-350M-Math/snapshots/c690cd3413a05178a2a84618ad2416cbf6953138` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/llmlingua-2-xlm-roberta-large-meetingbank": {
      "model_id": "microsoft/llmlingua-2-xlm-roberta-large-meetingbank",
      "model_name": "microsoft/llmlingua-2-xlm-roberta-large-meetingbank",
      "size_bytes": 558945282,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:53:35.971581",
      "end_time": "2026-01-20T18:53:36.023255",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Dongjin-kr/ko-reranker": {
      "model_id": "Dongjin-kr/ko-reranker",
      "model_name": "Dongjin-kr/ko-reranker",
      "size_bytes": 559891457,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:53:36.028321",
      "end_time": "2026-01-20T18:53:36.067774",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "classla/multilingual-IPTC-news-topic-classifier": {
      "model_id": "classla/multilingual-IPTC-news-topic-classifier",
      "model_name": "classla/multilingual-IPTC-news-topic-classifier",
      "size_bytes": 559907857,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:53:36.072907",
      "end_time": "2026-01-20T18:53:36.085084",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Ihor/Text2Graph-R1-Qwen2.5-0.5b": {
      "model_id": "Ihor/Text2Graph-R1-Qwen2.5-0.5b",
      "model_name": "Ihor/Text2Graph-R1-Qwen2.5-0.5b",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:59.727106",
      "end_time": "2026-01-20T18:53:42.830811",
      "duration_seconds": 103.1,
      "vllm_pid": 1762731,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Ihor--Text2Graph-R1-Qwen2.5-0.5b/snapshots/a1727ba4577443b01b63c276ef208f0893be38da` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KingNish/Reasoning-0.5b": {
      "model_id": "KingNish/Reasoning-0.5b",
      "model_name": "KingNish/Reasoning-0.5b",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:59.741707",
      "end_time": "2026-01-20T18:53:43.077610",
      "duration_seconds": 103.34,
      "vllm_pid": 1762990,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--KingNish--Reasoning-0.5b/snapshots/ec3c31a7907fee84c792b95ab3de597ee1e6521d` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Gensyn/Qwen2.5-0.5B-Instruct": {
      "model_id": "Gensyn/Qwen2.5-0.5B-Instruct",
      "model_name": "Gensyn/Qwen2.5-0.5B-Instruct",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:59.717857",
      "end_time": "2026-01-20T18:53:43.077625",
      "duration_seconds": 103.36,
      "vllm_pid": 1762632,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Gensyn--Qwen2.5-0.5B-Instruct/snapshots/317b7eb96312eda0c431d1dab1af958a308cb35e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/MiniCPM4-0.5B": {
      "model_id": "openbmb/MiniCPM4-0.5B",
      "model_name": "openbmb/MiniCPM4-0.5B",
      "size_bytes": 433873920,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:59.683990",
      "end_time": "2026-01-20T18:53:43.077561",
      "duration_seconds": 103.39,
      "vllm_pid": 1762480,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--openbmb--MiniCPM4-0.5B/snapshots/5253c7fcc5e29e1cf3eacb59a58adf1ba4df8630` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/BitCPM4-0.5B": {
      "model_id": "openbmb/BitCPM4-0.5B",
      "model_name": "openbmb/BitCPM4-0.5B",
      "size_bytes": 433873920,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:59.681998",
      "end_time": "2026-01-20T18:53:43.577353",
      "duration_seconds": 103.9,
      "vllm_pid": 1762450,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--openbmb--BitCPM4-0.5B/snapshots/4d0e4a9e2257ceb39d99b85bb946083467e8476f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "instruction-pretrain/InstructLM-500M": {
      "model_id": "instruction-pretrain/InstructLM-500M",
      "model_name": "instruction-pretrain/InstructLM-500M",
      "size_bytes": 567854592,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:53:43.171690",
      "end_time": "2026-01-20T18:53:43.184268",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DeepMount00/Alireo-400m-instruct-v0.1": {
      "model_id": "DeepMount00/Alireo-400m-instruct-v0.1",
      "model_name": "DeepMount00/Alireo-400m-instruct-v0.1",
      "size_bytes": 403777792,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:59.655089",
      "end_time": "2026-01-20T18:53:44.736677",
      "duration_seconds": 105.08,
      "vllm_pid": 1761989,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--DeepMount00--Alireo-400m-instruct-v0.1/snapshots/4457428d3201286a03112b51f6c04f85771dc1ef` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PleIAs/Baguettotron": {
      "model_id": "PleIAs/Baguettotron",
      "model_name": "PleIAs/Baguettotron",
      "size_bytes": 320956992,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:57.264487",
      "end_time": "2026-01-20T18:53:44.784690",
      "duration_seconds": 107.52,
      "vllm_pid": 1759884,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--PleIAs--Baguettotron/snapshots/88f73c15f72fcb8472484331c176905c507d724c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-350m": {
      "model_id": "ibm-granite/granite-4.0-h-350m",
      "model_name": "ibm-granite/granite-4.0-h-350m",
      "size_bytes": 340332224,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:51:57.255386",
      "end_time": "2026-01-20T18:53:55.193545",
      "duration_seconds": 117.94,
      "vllm_pid": 1759891,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-h-350m/snapshots/3b17b717b8f2f5d305b0a92c1491e239aeda19c8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "manycore-research/SpatialLM-Qwen-0.5B": {
      "model_id": "manycore-research/SpatialLM-Qwen-0.5B",
      "model_name": "manycore-research/SpatialLM-Qwen-0.5B",
      "size_bytes": 504983232,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:53:35.464263",
      "end_time": "2026-01-20T18:53:55.605657",
      "duration_seconds": 20.14,
      "vllm_pid": 2405830,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m   Value error, The checkpoint you are trying to load has model type `spatiallm_qwen` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m \n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2405830)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "manycore-research/SpatialLM1.1-Qwen-0.5B": {
      "model_id": "manycore-research/SpatialLM1.1-Qwen-0.5B",
      "model_name": "manycore-research/SpatialLM1.1-Qwen-0.5B",
      "size_bytes": 603511168,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:53:43.971793",
      "end_time": "2026-01-20T18:53:59.102891",
      "duration_seconds": 15.13,
      "vllm_pid": 1776122,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m   Value error, The checkpoint you are trying to load has model type `spatiallm_qwen` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m \n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1776122)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/Llama-3.2-3B-Instruct-4bit": {
      "model_id": "mlx-community/Llama-3.2-3B-Instruct-4bit",
      "model_name": "mlx-community/Llama-3.2-3B-Instruct-4bit",
      "size_bytes": 502139904,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:53:35.463916",
      "end_time": "2026-01-20T18:54:10.712296",
      "duration_seconds": 35.25,
      "vllm_pid": 2405833,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 640, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     return loader.load_weights(\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 261, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     loaded_params = module_load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 507, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m     param = params_dict[name]\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m             ~~~~~~~~~~~^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m KeyError: 'embed_tokens.biases'\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2406044)\u001b[0;0m \n[rank0]:[W120 18:54:00.152370280 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2405833)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-MLX-4bit": {
      "model_id": "Qwen/Qwen3-4B-MLX-4bit",
      "model_name": "Qwen/Qwen3-4B-MLX-4bit",
      "size_bytes": 565828096,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:53:36.089509",
      "end_time": "2026-01-20T18:54:11.218844",
      "duration_seconds": 35.13,
      "vllm_pid": 2405837,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen3.py\", line 331, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 261, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     loaded_params = module_load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 517, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     weight_loader(param, loaded_weight)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 458, in weight_loader\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m     param[: loaded_weight.shape[0]].data.copy_(loaded_weight)\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m RuntimeError: The size of tensor a (2560) must match the size of tensor b (320) at non-singleton dimension 1\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2406040)\u001b[0;0m \n[rank0]:[W120 18:54:00.075156470 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2405837)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/t5gemma-b-b-prefixlm-it": {
      "model_id": "google/t5gemma-b-b-prefixlm-it",
      "model_name": "google/t5gemma-b-b-prefixlm-it",
      "size_bytes": 591490560,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:53:43.171790",
      "end_time": "2026-01-20T18:54:23.327897",
      "duration_seconds": 40.16,
      "vllm_pid": 1775966,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self.driver_worker.init_device()\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py\", line 326, in init_device\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self.worker.init_device()  # type: ignore\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 278, in init_device\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     self.model_runner = GPUModelRunnerV1(self.vllm_config, self.device)\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     MultiModalBudget(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/utils.py\", line 46, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     max_tokens_by_modality = mm_registry.get_max_tokens_per_item_by_modality(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/multimodal/registry.py\", line 167, in get_max_tokens_per_item_by_modality\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     return profiler.get_mm_max_tokens(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/multimodal/profiling.py\", line 339, in get_mm_max_tokens\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     max_tokens_per_item = self.processing_info.get_mm_max_tokens_per_item(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/transformers/multimodal.py\", line 61, in get_mm_max_tokens_per_item\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     return {\"image\": self.get_max_image_tokens()}\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/transformers/multimodal.py\", line 65, in get_max_image_tokens\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     processor = self.get_hf_processor()\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/multimodal/processing.py\", line 1186, in get_hf_processor\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     return self.ctx.get_hf_processor(**kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/multimodal/processing.py\", line 1049, in get_hf_processor\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     return cached_processor_from_config(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/processor.py\", line 251, in cached_processor_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     return cached_get_processor_without_dynamic_kwargs(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/processor.py\", line 210, in cached_get_processor_without_dynamic_kwargs\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     processor = cached_get_processor(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/processor.py\", line 155, in get_processor\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m     raise TypeError(\n\u001b[0;36m(EngineCore_DP0 pid=1776734)\u001b[0;0m TypeError: Invalid type of HuggingFace processor. Expected type: <class 'transformers.processing_utils.ProcessorMixin'>, but found type: <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n[rank0]:[W120 18:54:13.389134138 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1775966)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EuroBERT/EuroBERT-610m": {
      "model_id": "EuroBERT/EuroBERT-610m",
      "model_name": "EuroBERT/EuroBERT-610m",
      "size_bytes": 755625600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:54:23.376260",
      "end_time": "2026-01-20T18:54:23.408827",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Qwen3-0.6B-unsloth-bnb-4bit": {
      "model_id": "unsloth/Qwen3-0.6B-unsloth-bnb-4bit",
      "model_name": "unsloth/Qwen3-0.6B-unsloth-bnb-4bit",
      "size_bytes": 609164690,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:53:44.772305",
      "end_time": "2026-01-20T18:54:24.947332",
      "duration_seconds": 40.18,
      "vllm_pid": 1776537,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen3.py\", line 274, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.model = Qwen3Model(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen3.py\", line 248, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen3.py\", line 196, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.mlp = Qwen3MLP(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 85, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.gate_up_proj = MergedColumnParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 631, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=1777439)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W120 18:54:13.170702655 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1776537)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "teapotai/teapotllm": {
      "model_id": "teapotai/teapotllm",
      "model_name": "teapotai/teapotllm",
      "size_bytes": 783150080,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:54:24.976415",
      "end_time": "2026-01-20T18:54:25.045295",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/FastVLM-0.5B": {
      "model_id": "apple/FastVLM-0.5B",
      "model_name": "apple/FastVLM-0.5B",
      "size_bytes": 758833760,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:54:23.413049",
      "end_time": "2026-01-20T18:54:38.541190",
      "duration_seconds": 15.13,
      "vllm_pid": 1779215,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m Encountered exception while importing timm: No module named 'timm'\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 427, in get_cached_module_file\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     modules_needed = check_imports(resolved_module_file)\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 260, in check_imports\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=1779215)\u001b[0;0m ImportError: This modeling file requires the following packages that were not found in your environment: timm. Run `pip install timm`\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon-E-3B-Instruct": {
      "model_id": "tiiuae/Falcon-E-3B-Instruct",
      "model_name": "tiiuae/Falcon-E-3B-Instruct",
      "size_bytes": 864159968,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:54:25.050460",
      "end_time": "2026-01-20T18:54:40.177532",
      "duration_seconds": 15.13,
      "vllm_pid": 1779801,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m   Value error, Unknown quantization method: bitnet. Must be one of ['awq', 'deepspeedfp', 'tpu_int8', 'fp8', 'ptpc_fp8', 'fbgemm_fp8', 'fp_quant', 'modelopt', 'modelopt_fp4', 'bitblas', 'gguf', 'gptq_marlin_24', 'gptq_marlin', 'gptq_bitblas', 'awq_marlin', 'gptq', 'compressed-tensors', 'bitsandbytes', 'hqq', 'experts_int8', 'ipex', 'quark', 'moe_wna16', 'torchao', 'auto-round', 'rtn', 'inc', 'mxfp4', 'petit_nvfp4', 'cpu_gptq', 'cpu_awq']. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1779801)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jinaai/reader-lm-0.5b": {
      "model_id": "jinaai/reader-lm-0.5b",
      "model_name": "jinaai/reader-lm-0.5b",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:34.915083",
      "end_time": "2026-01-20T18:55:07.671974",
      "duration_seconds": 92.76,
      "vllm_pid": 2405818,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--jinaai--reader-lm-0.5b/snapshots/46cb69fff9d100f9c3c2a135d59f365fb99a0121` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-3-1b-pt": {
      "model_id": "google/gemma-3-1b-pt",
      "model_name": "google/gemma-3-1b-pt",
      "size_bytes": 999885952,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:07.674828",
      "end_time": "2026-01-20T18:55:07.715934",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon-H1-0.5B-Instruct": {
      "model_id": "tiiuae/Falcon-H1-0.5B-Instruct",
      "model_name": "tiiuae/Falcon-H1-0.5B-Instruct",
      "size_bytes": 521411104,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:35.506241",
      "end_time": "2026-01-20T18:55:08.368751",
      "duration_seconds": 92.86,
      "vllm_pid": 2405834,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--tiiuae--Falcon-H1-0.5B-Instruct/snapshots/8f2587ca06bff78d8fa1adfccbe8c24d5f86b368` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/vaultgemma-1b": {
      "model_id": "google/vaultgemma-1b",
      "model_name": "google/vaultgemma-1b",
      "size_bytes": 1038741120,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:08.371450",
      "end_time": "2026-01-20T18:55:08.395078",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-1_1B": {
      "model_id": "apple/OpenELM-1_1B",
      "model_name": "apple/OpenELM-1_1B",
      "size_bytes": 1079891456,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:08.400650",
      "end_time": "2026-01-20T18:55:08.425052",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-700M": {
      "model_id": "LiquidAI/LFM2-700M",
      "model_name": "LiquidAI/LFM2-700M",
      "size_bytes": 742489344,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:55.608972",
      "end_time": "2026-01-20T18:55:08.445687",
      "duration_seconds": 72.84,
      "vllm_pid": 2406187,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-700M/snapshots/0f3cc1cb20ccf2867e9e9053514d74f85ea17ea0` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-1_1B-Instruct": {
      "model_id": "apple/OpenELM-1_1B-Instruct",
      "model_name": "apple/OpenELM-1_1B-Instruct",
      "size_bytes": 1079891456,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:08.431332",
      "end_time": "2026-01-20T18:55:08.455896",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-0.5B-Instruct": {
      "model_id": "tencent/Hunyuan-0.5B-Instruct",
      "model_name": "tencent/Hunyuan-0.5B-Instruct",
      "size_bytes": 539010048,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:35.660549",
      "end_time": "2026-01-20T18:55:08.795502",
      "duration_seconds": 93.13,
      "vllm_pid": 2405836,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--tencent--Hunyuan-0.5B-Instruct/snapshots/2359fb220c010e9d6d62c62d466f0eda179c2cf3` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "quotientai/limbic-tool-use-0.5B-32K": {
      "model_id": "quotientai/limbic-tool-use-0.5B-32K",
      "model_name": "quotientai/limbic-tool-use-0.5B-32K",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:35.463707",
      "end_time": "2026-01-20T18:55:09.153165",
      "duration_seconds": 93.69,
      "vllm_pid": 2405832,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--quotientai--limbic-tool-use-0.5B-32K/snapshots/a27c2487431facd0c81a1024247fcdb364336970` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kz919/QwQ-0.5B-Distilled-SFT": {
      "model_id": "kz919/QwQ-0.5B-Distilled-SFT",
      "model_name": "kz919/QwQ-0.5B-Distilled-SFT",
      "size_bytes": 494032768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:35.463424",
      "end_time": "2026-01-20T18:55:09.315347",
      "duration_seconds": 93.85,
      "vllm_pid": 2405831,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--kz919--QwQ-0.5B-Distilled-SFT/snapshots/b732fdd3ae4d4d0327b2040b143631342a747f8e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kurakurai/Luth-LFM2-1.2B": {
      "model_id": "kurakurai/Luth-LFM2-1.2B",
      "model_name": "kurakurai/Luth-LFM2-1.2B",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:09.318109",
      "end_time": "2026-01-20T18:55:09.339303",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--kurakurai--Luth-LFM2-1.2B/snapshots/6c529d6c85fe05fedd3f22397a5211b1aa9b6aa0/model.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3Guard-Gen-0.6B": {
      "model_id": "Qwen/Qwen3Guard-Gen-0.6B",
      "model_name": "Qwen/Qwen3Guard-Gen-0.6B",
      "size_bytes": 751632384,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:54:10.715653",
      "end_time": "2026-01-20T18:55:10.467027",
      "duration_seconds": 59.75,
      "vllm_pid": 2408247,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen3Guard-Gen-0.6B/snapshots/fada3b2f655b89601929198343c94cd2f64d93cc` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "amd/AMD-OLMo-1B": {
      "model_id": "amd/AMD-OLMo-1B",
      "model_name": "amd/AMD-OLMo-1B",
      "size_bytes": 1176764416,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:10.470289",
      "end_time": "2026-01-20T18:55:10.499697",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "amd/AMD-OLMo-1B-SFT": {
      "model_id": "amd/AMD-OLMo-1B-SFT",
      "model_name": "amd/AMD-OLMo-1B-SFT",
      "size_bytes": 1176764416,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:10.504936",
      "end_time": "2026-01-20T18:55:10.527754",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B": {
      "model_id": "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B",
      "model_name": "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B",
      "size_bytes": 566281216,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:42.872268",
      "end_time": "2026-01-20T18:55:10.601335",
      "duration_seconds": 87.73,
      "vllm_pid": 1775963,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--naver-hyperclovax--HyperCLOVAX-SEED-Text-Instruct-0.5B/snapshots/3da5046fb0195d14f2497de198136987d35fd644` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-0.6B-FP8": {
      "model_id": "Qwen/Qwen3-0.6B-FP8",
      "model_name": "Qwen/Qwen3-0.6B-FP8",
      "size_bytes": 751659264,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:54:11.222320",
      "end_time": "2026-01-20T18:55:10.768974",
      "duration_seconds": 59.55,
      "vllm_pid": 2408272,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen3-0.6B-FP8/snapshots/e5be08033360965ceca7b0ffd72d521a51331ce0` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-0.6B-Base": {
      "model_id": "Qwen/Qwen3-0.6B-Base",
      "model_name": "Qwen/Qwen3-0.6B-Base",
      "size_bytes": 596049920,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:43.671704",
      "end_time": "2026-01-20T18:55:12.368780",
      "duration_seconds": 88.7,
      "vllm_pid": 1775976,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--Qwen--Qwen3-0.6B-Base/snapshots/da87bfb608c14b7cf20ba1ce41287e8de496c0cd` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-1B": {
      "model_id": "unsloth/Llama-3.2-1B",
      "model_name": "unsloth/Llama-3.2-1B",
      "size_bytes": 1235814400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:55:12.380559",
      "end_time": "2026-01-20T18:55:12.408860",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-0.3B-Base-PT": {
      "model_id": "baidu/ERNIE-4.5-0.3B-Base-PT",
      "model_name": "baidu/ERNIE-4.5-0.3B-Base-PT",
      "size_bytes": 734314983,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:44.872293",
      "end_time": "2026-01-20T18:55:13.159556",
      "duration_seconds": 88.29,
      "vllm_pid": 1776538,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--baidu--ERNIE-4.5-0.3B-Base-PT/snapshots/2d64320bf1b5b97f19a7eccf221fedb57e7ec02a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "manycore-research/SpatialLM-Llama-1B": {
      "model_id": "manycore-research/SpatialLM-Llama-1B",
      "model_name": "manycore-research/SpatialLM-Llama-1B",
      "size_bytes": 1247355840,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:55:13.181033",
      "end_time": "2026-01-20T18:55:28.302645",
      "duration_seconds": 15.12,
      "vllm_pid": 1782663,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m   Value error, The checkpoint you are trying to load has model type `spatiallm_llama` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m \n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1782663)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit": {
      "model_id": "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
      "model_name": "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
      "size_bytes": 1266351458,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:28.382100",
      "end_time": "2026-01-20T18:56:24.031696",
      "duration_seconds": 55.65,
      "vllm_pid": 1784191,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--Llama-3.2-1B-Instruct-bnb-4bit/snapshots/fcecd050a91025151bbf518e41a5577c356efdce` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/Zamba2-1.2B-instruct": {
      "model_id": "Zyphra/Zamba2-1.2B-instruct",
      "model_name": "Zyphra/Zamba2-1.2B-instruct",
      "size_bytes": 1215064704,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:10.680204",
      "end_time": "2026-01-20T18:56:28.177045",
      "duration_seconds": 77.5,
      "vllm_pid": 1782621,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Zyphra--Zamba2-1.2B-instruct/snapshots/c06b789753995762b3c11b3eeb39e38634f897e9` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pfnet/plamo-2-1b": {
      "model_id": "pfnet/plamo-2-1b",
      "model_name": "pfnet/plamo-2-1b",
      "size_bytes": 1291441920,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:56:28.186314",
      "end_time": "2026-01-20T18:56:28.253605",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gemma-3-1b-it": {
      "model_id": "unsloth/gemma-3-1b-it",
      "model_name": "unsloth/gemma-3-1b-it",
      "size_bytes": 999885952,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:07.721991",
      "end_time": "2026-01-20T18:56:35.551011",
      "duration_seconds": 87.83,
      "vllm_pid": 2412891,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--unsloth--gemma-3-1b-it/snapshots/5b11413a10db4e486ef16a20101fd028f8f2499c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-1.2B": {
      "model_id": "LiquidAI/LFM2-1.2B",
      "model_name": "LiquidAI/LFM2-1.2B",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:08.448931",
      "end_time": "2026-01-20T18:56:36.068651",
      "duration_seconds": 87.62,
      "vllm_pid": 2412897,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-1.2B/snapshots/c97ace0b31ef13f116c70bc537ee6a1a8c501bc8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-1.2B-Extract": {
      "model_id": "LiquidAI/LFM2-1.2B-Extract",
      "model_name": "LiquidAI/LFM2-1.2B-Extract",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:08.460818",
      "end_time": "2026-01-20T18:56:36.533153",
      "duration_seconds": 88.07,
      "vllm_pid": 2412898,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-1.2B-Extract/snapshots/1ff3a10d7db876c5cae2146e551157954f3daa1c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-1.2B-RAG": {
      "model_id": "LiquidAI/LFM2-1.2B-RAG",
      "model_name": "LiquidAI/LFM2-1.2B-RAG",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:08.798332",
      "end_time": "2026-01-20T18:56:36.588336",
      "duration_seconds": 87.79,
      "vllm_pid": 2412900,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-1.2B-RAG/snapshots/f990742f828efec94cd997ad873d881359638613` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "instruction-pretrain/InstructLM-1.3B": {
      "model_id": "instruction-pretrain/InstructLM-1.3B",
      "model_name": "instruction-pretrain/InstructLM-1.3B",
      "size_bytes": 1347504128,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:56:36.536547",
      "end_time": "2026-01-20T18:56:36.622860",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "starvector/starvector-1b-im2svg": {
      "model_id": "starvector/starvector-1b-im2svg",
      "model_name": "starvector/starvector-1b-im2svg",
      "size_bytes": 1434095620,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:56:36.631370",
      "end_time": "2026-01-20T18:56:36.651565",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-1.2B-Tool": {
      "model_id": "LiquidAI/LFM2-1.2B-Tool",
      "model_name": "LiquidAI/LFM2-1.2B-Tool",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:09.155847",
      "end_time": "2026-01-20T18:56:38.063989",
      "duration_seconds": 88.91,
      "vllm_pid": 2412901,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--LiquidAI--LFM2-1.2B-Tool/snapshots/e9fa2567f8a598b00359b487fae71dcc9f4fedad` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-1b-base": {
      "model_id": "ibm-granite/granite-4.0-h-1b-base",
      "model_name": "ibm-granite/granite-4.0-h-1b-base",
      "size_bytes": 1461538368,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:56:38.067111",
      "end_time": "2026-01-20T18:56:38.100124",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "knowledgator/comprehend_it-base": {
      "model_id": "knowledgator/comprehend_it-base",
      "model_name": "knowledgator/comprehend_it-base",
      "size_bytes": 1477954385,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:56:38.104739",
      "end_time": "2026-01-20T18:56:38.109881",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yasserrmd/DentaInstruct-1.2B": {
      "model_id": "yasserrmd/DentaInstruct-1.2B",
      "model_name": "yasserrmd/DentaInstruct-1.2B",
      "size_bytes": 1170340608,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:09.344711",
      "end_time": "2026-01-20T18:56:38.173212",
      "duration_seconds": 88.83,
      "vllm_pid": 2412906,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--yasserrmd--DentaInstruct-1.2B/snapshots/8a7b38c1fa12ae8ac357213bc4fb33d7b0d72f52` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-0425-1B": {
      "model_id": "allenai/OLMo-2-0425-1B",
      "model_name": "allenai/OLMo-2-0425-1B",
      "size_bytes": 1484916736,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:56:38.113959",
      "end_time": "2026-01-20T18:56:38.172048",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ngxson/MiniThinky-v2-1B-Llama-3.2": {
      "model_id": "ngxson/MiniThinky-v2-1B-Llama-3.2",
      "model_name": "ngxson/MiniThinky-v2-1B-Llama-3.2",
      "size_bytes": 1235814400,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:10.771989",
      "end_time": "2026-01-20T18:56:38.933596",
      "duration_seconds": 88.16,
      "vllm_pid": 2412950,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ngxson--MiniThinky-v2-1B-Llama-3.2/snapshots/6a53b31cb0ea592e65195de157bc2a59cd6c3dc9` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Hymba-1.5B-Base": {
      "model_id": "nvidia/Hymba-1.5B-Base",
      "model_name": "nvidia/Hymba-1.5B-Base",
      "size_bytes": 1522797824,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:56:38.176292",
      "end_time": "2026-01-20T18:56:53.311129",
      "duration_seconds": 15.13,
      "vllm_pid": 2419613,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m   Value error, Model architectures ['HymbaForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2419613)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Hymba-1.5B-Instruct": {
      "model_id": "nvidia/Hymba-1.5B-Instruct",
      "model_name": "nvidia/Hymba-1.5B-Instruct",
      "size_bytes": 1522797824,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T18:56:38.936961",
      "end_time": "2026-01-20T18:56:54.069329",
      "duration_seconds": 15.13,
      "vllm_pid": 2419618,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m   Value error, Model architectures ['HymbaForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2419618)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "amd/AMD-OLMo-1B-SFT-DPO": {
      "model_id": "amd/AMD-OLMo-1B-SFT-DPO",
      "model_name": "amd/AMD-OLMo-1B-SFT-DPO",
      "size_bytes": 1176764416,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:10.533247",
      "end_time": "2026-01-20T18:57:10.946597",
      "duration_seconds": 120.41,
      "vllm_pid": 2412932,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1048 input tokens (1536 > 2048 - 1048). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 110, in generate\n    return self.handle_bad_request(ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 150, in handle_bad_request\n    return openai_handle_bad_request(self.model_name, ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 569, in openai_handle_bad_request\n    raise e\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1048 input tokens (1536 > 2048 - 1048). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/gemma-3-1b-it": {
      "model_id": "google/gemma-3-1b-it",
      "model_name": "google/gemma-3-1b-it",
      "size_bytes": 999885952,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:54:40.278079",
      "end_time": "2026-01-20T18:57:29.670173",
      "duration_seconds": 169.39,
      "vllm_pid": 1780669,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-1B-Instruct": {
      "model_id": "unsloth/Llama-3.2-1B-Instruct",
      "model_name": "unsloth/Llama-3.2-1B-Instruct",
      "size_bytes": 1235814400,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:55:12.414747",
      "end_time": "2026-01-20T18:57:36.124529",
      "duration_seconds": 143.71,
      "vllm_pid": 1782661,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--Llama-3.2-1B-Instruct/snapshots/5a8abab4a5d6f164389b1079fb721cfab8d7126c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--Llama-3.2-1B-Instruct/snapshots/5a8abab4a5d6f164389b1079fb721cfab8d7126c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KurmaAI/AQUA-1B": {
      "model_id": "KurmaAI/AQUA-1B",
      "model_name": "KurmaAI/AQUA-1B",
      "size_bytes": 999885952,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:54:38.577710",
      "end_time": "2026-01-20T18:57:43.050019",
      "duration_seconds": 184.47,
      "vllm_pid": 1780656,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--KurmaAI--AQUA-1B/snapshots/7a1d8c5041121adcd8d8c02d13702344c55d2c62` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--KurmaAI--AQUA-1B/snapshots/7a1d8c5041121adcd8d8c02d13702344c55d2c62` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-1b": {
      "model_id": "ibm-granite/granite-4.0-h-1b",
      "model_name": "ibm-granite/granite-4.0-h-1b",
      "size_bytes": 1461538368,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:36.656180",
      "end_time": "2026-01-20T18:58:04.367434",
      "duration_seconds": 87.71,
      "vllm_pid": 2419610,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-h-1b/snapshots/d18cca4c121edb87d022116d281ce212c9136f57` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-0425-1B-Instruct": {
      "model_id": "allenai/OLMo-2-0425-1B-Instruct",
      "model_name": "allenai/OLMo-2-0425-1B-Instruct",
      "size_bytes": 1484916736,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:38.176189",
      "end_time": "2026-01-20T18:58:05.960762",
      "duration_seconds": 87.78,
      "vllm_pid": 2419614,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--allenai--OLMo-2-0425-1B-Instruct/snapshots/48d788eca847d4d7548f375ad03d3c9312f6139e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.1-1b-a400m-instruct": {
      "model_id": "ibm-granite/granite-3.1-1b-a400m-instruct",
      "model_name": "ibm-granite/granite-3.1-1b-a400m-instruct",
      "size_bytes": 1334628352,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:28.260733",
      "end_time": "2026-01-20T18:58:06.019406",
      "duration_seconds": 97.76,
      "vllm_pid": 1787969,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-3.1-1b-a400m-instruct/snapshots/b0e4fd07be563ba8bb7689c47dc9bebdff5471ab` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Vikhrmodels/QVikhr-2.5-1.5B-Instruct-r": {
      "model_id": "Vikhrmodels/QVikhr-2.5-1.5B-Instruct-r",
      "model_name": "Vikhrmodels/QVikhr-2.5-1.5B-Instruct-r",
      "size_bytes": 1543298048,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:53.314679",
      "end_time": "2026-01-20T18:58:06.190497",
      "duration_seconds": 72.88,
      "vllm_pid": 2419873,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Vikhrmodels--QVikhr-2.5-1.5B-Instruct-r/snapshots/75879cfb701a5c185c07724171a619932c5ee13f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DeepMount00/Qwen2-1.5B-Ita": {
      "model_id": "DeepMount00/Qwen2-1.5B-Ita",
      "model_name": "DeepMount00/Qwen2-1.5B-Ita",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:54.072996",
      "end_time": "2026-01-20T18:58:07.286320",
      "duration_seconds": 73.21,
      "vllm_pid": 2419930,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--DeepMount00--Qwen2-1.5B-Ita/snapshots/681e6db531df0cc3d7806251659b973ed4ff8c8f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/BitCPM4-1B": {
      "model_id": "openbmb/BitCPM4-1B",
      "model_name": "openbmb/BitCPM4-1B",
      "size_bytes": 1360258560,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:36.592054",
      "end_time": "2026-01-20T18:58:08.068758",
      "duration_seconds": 91.48,
      "vllm_pid": 2419609,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--openbmb--BitCPM4-1B/snapshots/70d989c4972efa636e8a2211975515d188e000b0` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "prem-research/prem-1B-SQL": {
      "model_id": "prem-research/prem-1B-SQL",
      "model_name": "prem-research/prem-1B-SQL",
      "size_bytes": 1346471936,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:56:36.072567",
      "end_time": "2026-01-20T18:58:08.951082",
      "duration_seconds": 92.88,
      "vllm_pid": 2419602,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--prem-research--prem-1B-SQL/snapshots/44dd7fcf9227af4efed936bf29323c61bf66aad1` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-0.3B-PT": {
      "model_id": "baidu/ERNIE-4.5-0.3B-PT",
      "model_name": "baidu/ERNIE-4.5-0.3B-PT",
      "size_bytes": 734314983,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:55.273507",
      "end_time": "2026-01-20T18:58:53.982436",
      "duration_seconds": 298.71,
      "vllm_pid": 1776679,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--baidu--ERNIE-4.5-0.3B-PT/snapshots/b565cf6caebdb7a1eadf00100857b1ed5e044f12` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--baidu--ERNIE-4.5-0.3B-PT/snapshots/b565cf6caebdb7a1eadf00100857b1ed5e044f12` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-0.6B": {
      "model_id": "Qwen/Qwen3-0.6B",
      "model_name": "Qwen/Qwen3-0.6B",
      "size_bytes": 751632384,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:53:59.174258",
      "end_time": "2026-01-20T18:59:04.704715",
      "duration_seconds": 305.53,
      "vllm_pid": 1776714,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yasserrmd/MedScholar-1.5B": {
      "model_id": "yasserrmd/MedScholar-1.5B",
      "model_name": "yasserrmd/MedScholar-1.5B",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:59:04.798529",
      "end_time": "2026-01-20T18:59:04.816386",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jinaai/ReaderLM-v2": {
      "model_id": "jinaai/ReaderLM-v2",
      "model_name": "jinaai/ReaderLM-v2",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:58:06.094076",
      "end_time": "2026-01-20T18:59:09.104650",
      "duration_seconds": 63.01,
      "vllm_pid": 1794010,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--jinaai--ReaderLM-v2/snapshots/1d07078459ee1e880a22d67387b5e683d50a6e4b` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TIGER-Lab/general-verifier": {
      "model_id": "TIGER-Lab/general-verifier",
      "model_name": "TIGER-Lab/general-verifier",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:58:05.964670",
      "end_time": "2026-01-20T18:59:34.806696",
      "duration_seconds": 88.84,
      "vllm_pid": 2426833,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--TIGER-Lab--general-verifier/snapshots/dc59c0c82360170cab0be91c91ae545ff32ea896` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "katanemo/Arch-Router-1.5B": {
      "model_id": "katanemo/Arch-Router-1.5B",
      "model_name": "katanemo/Arch-Router-1.5B",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:58:07.290473",
      "end_time": "2026-01-20T18:59:35.265740",
      "duration_seconds": 87.98,
      "vllm_pid": 2426846,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--katanemo--Arch-Router-1.5B/snapshots/0490c1af10854396a13682fb97b59eb9de2cb445` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-1b-base": {
      "model_id": "ibm-granite/granite-4.0-1b-base",
      "model_name": "ibm-granite/granite-4.0-1b-base",
      "size_bytes": 1631750144,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:59:35.268880",
      "end_time": "2026-01-20T18:59:35.291517",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-1B-Base": {
      "model_id": "tiiuae/Falcon3-1B-Base",
      "model_name": "tiiuae/Falcon3-1B-Base",
      "size_bytes": 1669408768,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:59:35.297674",
      "end_time": "2026-01-20T18:59:35.315861",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/OpenReasoning-Nemotron-1.5B": {
      "model_id": "nvidia/OpenReasoning-Nemotron-1.5B",
      "model_name": "nvidia/OpenReasoning-Nemotron-1.5B",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:58:08.955115",
      "end_time": "2026-01-20T18:59:36.457853",
      "duration_seconds": 87.5,
      "vllm_pid": 2426880,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--nvidia--OpenReasoning-Nemotron-1.5B/snapshots/f7e9c457f57387ce4eefa856795a5afa0622ec56` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-1.7B": {
      "model_id": "HuggingFaceTB/SmolLM2-1.7B",
      "model_name": "HuggingFaceTB/SmolLM2-1.7B",
      "size_bytes": 1711376384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T18:59:36.461115",
      "end_time": "2026-01-20T18:59:36.485956",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/OpenMath-Nemotron-1.5B": {
      "model_id": "nvidia/OpenMath-Nemotron-1.5B",
      "model_name": "nvidia/OpenMath-Nemotron-1.5B",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:58:08.072603",
      "end_time": "2026-01-20T18:59:36.918092",
      "duration_seconds": 88.85,
      "vllm_pid": 2426865,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--nvidia--OpenMath-Nemotron-1.5B/snapshots/59280da0e5508b98f4ae90e8886442f28ed561cb` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Kortix/FastApply-1.5B-v1.0": {
      "model_id": "Kortix/FastApply-1.5B-v1.0",
      "model_name": "Kortix/FastApply-1.5B-v1.0",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:57:29.691684",
      "end_time": "2026-01-20T19:00:26.832745",
      "duration_seconds": 177.14,
      "vllm_pid": 1791687,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Kortix--FastApply-1.5B-v1.0/snapshots/fe5f9b2811fad59badb19a16ebc0a10b23d1b8ea` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Kortix--FastApply-1.5B-v1.0/snapshots/fe5f9b2811fad59badb19a16ebc0a10b23d1b8ea` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jinaai/reader-lm-1.5b": {
      "model_id": "jinaai/reader-lm-1.5b",
      "model_name": "jinaai/reader-lm-1.5b",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:58:06.194214",
      "end_time": "2026-01-20T19:00:37.393341",
      "duration_seconds": 151.2,
      "vllm_pid": 2426834,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--jinaai--reader-lm-1.5b/snapshots/274dc4fe6f5ac66793cd921fd0bb07dc4a3094f4` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--jinaai--reader-lm-1.5b/snapshots/274dc4fe6f5ac66793cd921fd0bb07dc4a3094f4` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-Coder-1.5B-Instruct": {
      "model_id": "Qwen/Qwen2.5-Coder-1.5B-Instruct",
      "model_name": "Qwen/Qwen2.5-Coder-1.5B-Instruct",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:57:36.191935",
      "end_time": "2026-01-20T19:00:41.838609",
      "duration_seconds": 185.65,
      "vllm_pid": 1791821,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-1.7B-Base": {
      "model_id": "Qwen/Qwen3-1.7B-Base",
      "model_name": "Qwen/Qwen3-1.7B-Base",
      "size_bytes": 1720574976,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:00:41.906437",
      "end_time": "2026-01-20T19:00:41.941277",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--Qwen--Qwen3-1.7B-Base/snapshots/ea980cb0a6c2ae4b936e82123acc929f1cec04c1/model.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon-H1-1.5B-Deep-Instruct": {
      "model_id": "tiiuae/Falcon-H1-1.5B-Deep-Instruct",
      "model_name": "tiiuae/Falcon-H1-1.5B-Deep-Instruct",
      "size_bytes": 1554874768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:59:04.823223",
      "end_time": "2026-01-20T19:00:54.738447",
      "duration_seconds": 109.92,
      "vllm_pid": 1797658,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--tiiuae--Falcon-H1-1.5B-Deep-Instruct/snapshots/b6648636ddc906688974282de6e7a243395f5423` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-1b": {
      "model_id": "ibm-granite/granite-4.0-1b",
      "model_name": "ibm-granite/granite-4.0-1b",
      "size_bytes": 1631750144,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:59:34.811515",
      "end_time": "2026-01-20T19:01:03.065456",
      "duration_seconds": 88.25,
      "vllm_pid": 2432959,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-1b/snapshots/6a7381ba1f54d684ff508d991aeb7dc580157103` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dleemiller/Penny-1.7B": {
      "model_id": "dleemiller/Penny-1.7B",
      "model_name": "dleemiller/Penny-1.7B",
      "size_bytes": 1711376384,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:59:36.921569",
      "end_time": "2026-01-20T19:01:04.655993",
      "duration_seconds": 87.73,
      "vllm_pid": 2433019,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--dleemiller--Penny-1.7B/snapshots/451ad8e59107cb4afd399b7b69fd362fa667ff80` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "alamios/Mistral-Small-3.1-DRAFT-0.5B": {
      "model_id": "alamios/Mistral-Small-3.1-DRAFT-0.5B",
      "model_name": "alamios/Mistral-Small-3.1-DRAFT-0.5B",
      "size_bytes": 592779136,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:53:43.171945",
      "end_time": "2026-01-20T19:01:37.230664",
      "duration_seconds": 474.06,
      "vllm_pid": 1775965,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-1B-Instruct": {
      "model_id": "tiiuae/Falcon3-1B-Instruct",
      "model_name": "tiiuae/Falcon3-1B-Instruct",
      "size_bytes": 1669408768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:59:35.320521",
      "end_time": "2026-01-20T19:01:44.345529",
      "duration_seconds": 129.03,
      "vllm_pid": 2432965,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--tiiuae--Falcon3-1B-Instruct/snapshots/28ba2251970a01dd1edc7ba7dad2eb71216ccfdf` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--tiiuae--Falcon3-1B-Instruct/snapshots/28ba2251970a01dd1edc7ba7dad2eb71216ccfdf` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pints-ai/1.5-Pints-2K-v0.1": {
      "model_id": "pints-ai/1.5-Pints-2K-v0.1",
      "model_name": "pints-ai/1.5-Pints-2K-v0.1",
      "size_bytes": 1565886464,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T18:59:09.199025",
      "end_time": "2026-01-20T19:02:01.242934",
      "duration_seconds": 172.04,
      "vllm_pid": 1797684,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1184 input tokens (1536 > 2048 - 1184). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 110, in generate\n    return self.handle_bad_request(ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 150, in handle_bad_request\n    return openai_handle_bad_request(self.model_name, ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 569, in openai_handle_bad_request\n    raise e\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1184 input tokens (1536 > 2048 - 1184). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "WeiboAI/VibeThinker-1.5B": {
      "model_id": "WeiboAI/VibeThinker-1.5B",
      "model_name": "WeiboAI/VibeThinker-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:01:04.658756",
      "end_time": "2026-01-20T19:02:13.429278",
      "duration_seconds": 68.77,
      "vllm_pid": 2438514,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--WeiboAI--VibeThinker-1.5B/snapshots/dbc405deea53a29597ac31a044943472af6f7e08` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/AlphaMaze-v0.2-1.5B": {
      "model_id": "Menlo/AlphaMaze-v0.2-1.5B",
      "model_name": "Menlo/AlphaMaze-v0.2-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:01:03.068390",
      "end_time": "2026-01-20T19:02:16.855487",
      "duration_seconds": 73.79,
      "vllm_pid": 2438507,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Menlo--AlphaMaze-v0.2-1.5B/snapshots/2a7c08f3614fa672bb1be573b210b3b5e575cd30` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:02:16.858698",
      "end_time": "2026-01-20T19:03:14.596688",
      "duration_seconds": 57.74,
      "vllm_pid": 2442705,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "agentica-org/DeepScaleR-1.5B-Preview": {
      "model_id": "agentica-org/DeepScaleR-1.5B-Preview",
      "model_name": "agentica-org/DeepScaleR-1.5B-Preview",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:02:13.432420",
      "end_time": "2026-01-20T19:03:17.320521",
      "duration_seconds": 63.89,
      "vllm_pid": 2442572,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--agentica-org--DeepScaleR-1.5B-Preview/snapshots/e3f524ce413a296b4d388e7560dd5c82c1c56725` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "winninghealth/WiNGPT-Babel": {
      "model_id": "winninghealth/WiNGPT-Babel",
      "model_name": "winninghealth/WiNGPT-Babel",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:58:53.997779",
      "end_time": "2026-01-20T19:05:30.811005",
      "duration_seconds": 396.81,
      "vllm_pid": 1796738,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SakanaAI/TinySwallow-1.5B-Instruct": {
      "model_id": "SakanaAI/TinySwallow-1.5B-Instruct",
      "model_name": "SakanaAI/TinySwallow-1.5B-Instruct",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:58:04.372376",
      "end_time": "2026-01-20T19:05:48.160277",
      "duration_seconds": 463.79,
      "vllm_pid": 2426814,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "UnfilteredAI/DAN-Qwen3-1.7B": {
      "model_id": "UnfilteredAI/DAN-Qwen3-1.7B",
      "model_name": "UnfilteredAI/DAN-Qwen3-1.7B",
      "size_bytes": 1720574976,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:00:41.949576",
      "end_time": "2026-01-20T19:05:48.227909",
      "duration_seconds": 306.28,
      "vllm_pid": 1802859,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--UnfilteredAI--DAN-Qwen3-1.7B/snapshots/1c158077ff02ff70cb4837898c15cadd47d4f158` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--UnfilteredAI--DAN-Qwen3-1.7B/snapshots/1c158077ff02ff70cb4837898c15cadd47d4f158` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "janhq/Jan-v1-edge": {
      "model_id": "janhq/Jan-v1-edge",
      "model_name": "janhq/Jan-v1-edge",
      "size_bytes": 1720574976,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:00:54.807756",
      "end_time": "2026-01-20T19:05:57.337563",
      "duration_seconds": 302.53,
      "vllm_pid": 1803706,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--janhq--Jan-v1-edge/snapshots/f4ed4d61df6fa0bb4724101b4ae1ab81dd3aa299` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--janhq--Jan-v1-edge/snapshots/f4ed4d61df6fa0bb4724101b4ae1ab81dd3aa299` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "thomas-sounack/BioClinical-ModernBERT-base": {
      "model_id": "thomas-sounack/BioClinical-ModernBERT-base",
      "model_name": "thomas-sounack/BioClinical-ModernBERT-base",
      "size_bytes": 1793771613,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:05:57.431209",
      "end_time": "2026-01-20T19:05:57.460486",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/AceInstruct-1.5B": {
      "model_id": "nvidia/AceInstruct-1.5B",
      "model_name": "nvidia/AceInstruct-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:03:17.323432",
      "end_time": "2026-01-20T19:06:10.275947",
      "duration_seconds": 172.95,
      "vllm_pid": 2446602,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--nvidia--AceInstruct-1.5B/snapshots/1e3d02075fcf988407b436eb5c10a407be86c71f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--nvidia--AceInstruct-1.5B/snapshots/1e3d02075fcf988407b436eb5c10a407be86c71f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "knoveleng/Open-RS3": {
      "model_id": "knoveleng/Open-RS3",
      "model_name": "knoveleng/Open-RS3",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:03:14.600666",
      "end_time": "2026-01-20T19:06:30.780447",
      "duration_seconds": 196.18,
      "vllm_pid": 2446482,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--knoveleng--Open-RS3/snapshots/94a8bd9e63e2f4d9ef60955c7297a1f6c97251bb` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--knoveleng--Open-RS3/snapshots/94a8bd9e63e2f4d9ef60955c7297a1f6c97251bb` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM2-1.7B-Instruct": {
      "model_id": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      "model_name": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      "size_bytes": 1711376384,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:59:36.491110",
      "end_time": "2026-01-20T19:06:33.950860",
      "duration_seconds": 417.46,
      "vllm_pid": 2432984,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-1.8B-Instruct": {
      "model_id": "tencent/Hunyuan-1.8B-Instruct",
      "model_name": "tencent/Hunyuan-1.8B-Instruct",
      "size_bytes": 1791080448,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:05:48.330293",
      "end_time": "2026-01-20T19:07:02.841042",
      "duration_seconds": 74.51,
      "vllm_pid": 1818294,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--tencent--Hunyuan-1.8B-Instruct/snapshots/de940610ab5d45b4fedbd37e08444d0dc502837c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit": {
      "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit",
      "model_name": "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit",
      "size_bytes": 1814099008,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:06:10.279484",
      "end_time": "2026-01-20T19:07:15.994253",
      "duration_seconds": 65.71,
      "vllm_pid": 2454354,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit/snapshots/c93b549ceae3f8137b567af565c2b204b3045091` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "m-a-p/YuE-s2-1B-general": {
      "model_id": "m-a-p/YuE-s2-1B-general",
      "model_name": "m-a-p/YuE-s2-1B-general",
      "size_bytes": 1962543104,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:07:15.997153",
      "end_time": "2026-01-20T19:07:16.032162",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kyutai/helium-1-2b": {
      "model_id": "kyutai/helium-1-2b",
      "model_name": "kyutai/helium-1-2b",
      "size_bytes": 2023868416,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:07:16.037311",
      "end_time": "2026-01-20T19:07:16.059629",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/FastVLM-1.5B": {
      "model_id": "apple/FastVLM-1.5B",
      "model_name": "apple/FastVLM-1.5B",
      "size_bytes": 1909278176,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:07:02.937718",
      "end_time": "2026-01-20T19:07:18.078380",
      "duration_seconds": 15.14,
      "vllm_pid": 1822186,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m Encountered exception while importing timm: No module named 'timm'\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 427, in get_cached_module_file\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     modules_needed = check_imports(resolved_module_file)\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 260, in check_imports\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=1822186)\u001b[0;0m ImportError: This modeling file requires the following packages that were not found in your environment: timm. Run `pip install timm`\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/CoDA-v0-Instruct": {
      "model_id": "Salesforce/CoDA-v0-Instruct",
      "model_name": "Salesforce/CoDA-v0-Instruct",
      "size_bytes": 2031739904,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:07:18.137899",
      "end_time": "2026-01-20T19:07:33.293216",
      "duration_seconds": 15.16,
      "vllm_pid": 1822896,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m   Value error, Model architectures ['CoDALanguageModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1822896)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Gensyn/Qwen2.5-1.5B-Instruct": {
      "model_id": "Gensyn/Qwen2.5-1.5B-Instruct",
      "model_name": "Gensyn/Qwen2.5-1.5B-Instruct",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:57:10.951006",
      "end_time": "2026-01-20T19:07:39.630521",
      "duration_seconds": 628.68,
      "vllm_pid": 2421813,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "internlm/internlm2-chat-1_8b": {
      "model_id": "internlm/internlm2-chat-1_8b",
      "model_name": "internlm/internlm2-chat-1_8b",
      "size_bytes": 1889110016,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:06:30.783814",
      "end_time": "2026-01-20T19:08:03.581539",
      "duration_seconds": 92.8,
      "vllm_pid": 2455456,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--internlm--internlm2-chat-1_8b/snapshots/21ccc6447f57c3c6dd2a78e2248bd6afbe0133e1` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kakaocorp/kanana-nano-2.1b-embedding": {
      "model_id": "kakaocorp/kanana-nano-2.1b-embedding",
      "model_name": "kakaocorp/kanana-nano-2.1b-embedding",
      "size_bytes": 2086979328,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:08:03.584776",
      "end_time": "2026-01-20T19:08:18.743004",
      "duration_seconds": 15.16,
      "vllm_pid": 2459931,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m   Value error, Model architectures ['Kanana2VecModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2459931)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kakaocorp/kanana-nano-2.1b-instruct": {
      "model_id": "kakaocorp/kanana-nano-2.1b-instruct",
      "model_name": "kakaocorp/kanana-nano-2.1b-instruct",
      "size_bytes": 2086979328,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:08:18.746284",
      "end_time": "2026-01-20T19:09:12.339104",
      "duration_seconds": 53.59,
      "vllm_pid": 2460330,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--kakaocorp--kanana-nano-2.1b-instruct/snapshots/e12a48d5386ceb7dd608533e8ffaea82d6070554` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xLAM-1b-fc-r": {
      "model_id": "Salesforce/xLAM-1b-fc-r",
      "model_name": "Salesforce/xLAM-1b-fc-r",
      "size_bytes": 1346471936,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:56:35.554438",
      "end_time": "2026-01-20T19:09:54.750763",
      "duration_seconds": 799.2,
      "vllm_pid": 2419600,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kyutai/helium-1-preview-2b": {
      "model_id": "kyutai/helium-1-preview-2b",
      "model_name": "kyutai/helium-1-preview-2b",
      "size_bytes": 2172643840,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:09:54.755497",
      "end_time": "2026-01-20T19:09:54.788100",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BSC-LT/salamandra-2b": {
      "model_id": "BSC-LT/salamandra-2b",
      "model_name": "BSC-LT/salamandra-2b",
      "size_bytes": 2253490176,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:09:54.794259",
      "end_time": "2026-01-20T19:09:54.831542",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LGAI-EXAONE/EXAONE-4.0-1.2B": {
      "model_id": "LGAI-EXAONE/EXAONE-4.0-1.2B",
      "model_name": "LGAI-EXAONE/EXAONE-4.0-1.2B",
      "size_bytes": 1279391488,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:56:24.086261",
      "end_time": "2026-01-20T19:11:20.639089",
      "duration_seconds": 896.55,
      "vllm_pid": 1787928,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "maidalun1020/bce-reranker-base_v1": {
      "model_id": "maidalun1020/bce-reranker-base_v1",
      "model_name": "maidalun1020/bce-reranker-base_v1",
      "size_bytes": 2258360394,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:11:20.656817",
      "end_time": "2026-01-20T19:11:20.676962",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "WiroAI/WiroAI-Finance-Qwen-1.5B": {
      "model_id": "WiroAI/WiroAI-Finance-Qwen-1.5B",
      "model_name": "WiroAI/WiroAI-Finance-Qwen-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:01:37.311217",
      "end_time": "2026-01-20T19:15:06.118532",
      "duration_seconds": 808.81,
      "vllm_pid": 1806478,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BSC-LT/salamandra-2b-instruct": {
      "model_id": "BSC-LT/salamandra-2b-instruct",
      "model_name": "BSC-LT/salamandra-2b-instruct",
      "size_bytes": 2253490176,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:09:54.836787",
      "end_time": "2026-01-20T19:15:54.906749",
      "duration_seconds": 360.07,
      "vllm_pid": 2464221,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "internlm/internlm2_5-1_8b-chat": {
      "model_id": "internlm/internlm2_5-1_8b-chat",
      "model_name": "internlm/internlm2_5-1_8b-chat",
      "size_bytes": 1889110016,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:06:33.955853",
      "end_time": "2026-01-20T19:18:33.838093",
      "duration_seconds": 719.88,
      "vllm_pid": 2455466,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "vngrs-ai/Kumru-2B": {
      "model_id": "vngrs-ai/Kumru-2B",
      "model_name": "vngrs-ai/Kumru-2B",
      "size_bytes": 2375138304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:15:54.913015",
      "end_time": "2026-01-20T19:22:41.532173",
      "duration_seconds": 406.62,
      "vllm_pid": 2479645,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EuroBERT/EuroBERT-2.1B": {
      "model_id": "EuroBERT/EuroBERT-2.1B",
      "model_name": "EuroBERT/EuroBERT-2.1B",
      "size_bytes": 2403092736,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:22:41.535627",
      "end_time": "2026-01-20T19:22:41.576993",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "h2oai/h2ovl-mississippi-2b": {
      "model_id": "h2oai/h2ovl-mississippi-2b",
      "model_name": "h2oai/h2ovl-mississippi-2b",
      "size_bytes": 2152317440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:09:12.342368",
      "end_time": "2026-01-20T19:23:39.495960",
      "duration_seconds": 867.15,
      "vllm_pid": 2462861,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LGAI-EXAONE/EXAONE-Deep-2.4B-AWQ": {
      "model_id": "LGAI-EXAONE/EXAONE-Deep-2.4B-AWQ",
      "model_name": "LGAI-EXAONE/EXAONE-Deep-2.4B-AWQ",
      "size_bytes": 2405327360,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:23:39.505165",
      "end_time": "2026-01-20T19:24:09.652089",
      "duration_seconds": 30.15,
      "vllm_pid": 2499204,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.24s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.24s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 56, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     process_weights_after_loading(model, model_config, target_device)\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 108, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     quant_method.process_weights_after_loading(module)\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py\", line 385, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     marlin_scales = marlin_permute_scales(\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 302, in marlin_permute_scales\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m     s = s.reshape((-1, len(scale_perm)))[:, scale_perm]\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=2499302)\u001b[0;0m \n[rank0]:[W120 19:24:00.669370392 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2499204)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lmms-lab/Aero-1-Audio": {
      "model_id": "lmms-lab/Aero-1-Audio",
      "model_name": "lmms-lab/Aero-1-Audio",
      "size_bytes": 2416221184,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:24:09.660464",
      "end_time": "2026-01-20T19:24:29.797705",
      "duration_seconds": 20.14,
      "vllm_pid": 2500505,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m   Value error, Model architectures ['AeroForConditionalGeneration'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2500505)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "K-intelligence/Midm-2.0-Mini-Instruct": {
      "model_id": "K-intelligence/Midm-2.0-Mini-Instruct",
      "model_name": "K-intelligence/Midm-2.0-Mini-Instruct",
      "size_bytes": 2305517312,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:11:20.683576",
      "end_time": "2026-01-20T19:29:46.123916",
      "duration_seconds": 1105.44,
      "vllm_pid": 1832087,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jinaai/jina-reranker-m0": {
      "model_id": "jinaai/jina-reranker-m0",
      "model_name": "jinaai/jina-reranker-m0",
      "size_bytes": 2444721665,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:29:46.136082",
      "end_time": "2026-01-20T19:30:31.271296",
      "duration_seconds": 45.14,
      "vllm_pid": 1872375,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.99s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.99s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 240, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 126, in determine_available_memory\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self.collective_rpc(\"determine_available_memory\")\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 340, in determine_available_memory\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     self.model_runner.profile_run()\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4462, in profile_run\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     dummy_encoder_outputs = self.model.embed_multimodal(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1433, in embed_multimodal\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     video_embeddings = self._process_video_input(video_input)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1386, in _process_video_input\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     video_embeds = self.visual(pixel_values_videos, grid_thw=grid_thw)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 709, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     x = blk(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m         ^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 446, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     x = x + self.attn(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m             ^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 388, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     context_layer = self.attn(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                     ^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/custom_op.py\", line 47, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._forward_method(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layers/mm_encoder_attention.py\", line 229, in forward_cuda\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._forward_fa(query, key, value, cu_seqlens, max_seqlen)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layers/mm_encoder_attention.py\", line 199, in _forward_fa\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     output = vit_flash_attn_wrapper(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/ops/vit_attn_wrappers.py\", line 80, in vit_flash_attn_wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return torch.ops.vllm.flash_attn_maxseqlen_wrapper(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/ops/vit_attn_wrappers.py\", line 37, in flash_attn_maxseqlen_wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     output = flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 253, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     out, softmax_lse = torch.ops._vllm_fa2_C.varlen_fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=1873030)\u001b[0;0m \n[rank0]:[W120 19:30:21.040370250 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1872375)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SakanaAI/TinySwallow-1.5B": {
      "model_id": "SakanaAI/TinySwallow-1.5B",
      "model_name": "SakanaAI/TinySwallow-1.5B",
      "size_bytes": 1543714304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T18:57:43.092588",
      "end_time": "2026-01-20T19:31:32.756177",
      "duration_seconds": 2029.66,
      "vllm_pid": 1792180,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.3-2b-base": {
      "model_id": "ibm-granite/granite-3.3-2b-base",
      "model_name": "ibm-granite/granite-3.3-2b-base",
      "size_bytes": 2533525504,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:31:32.844450",
      "end_time": "2026-01-20T19:31:32.883616",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/ZR1-1.5B": {
      "model_id": "Zyphra/ZR1-1.5B",
      "model_name": "Zyphra/ZR1-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:01:44.348410",
      "end_time": "2026-01-20T19:31:34.172088",
      "duration_seconds": 1789.82,
      "vllm_pid": 2440399,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kakaocorp/kanana-1.5-2.1b-instruct-2505": {
      "model_id": "kakaocorp/kanana-1.5-2.1b-instruct-2505",
      "model_name": "kakaocorp/kanana-1.5-2.1b-instruct-2505",
      "size_bytes": 2316824832,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:15:06.173005",
      "end_time": "2026-01-20T19:37:16.845585",
      "duration_seconds": 1330.67,
      "vllm_pid": 1840488,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.2-2b-instruct": {
      "model_id": "ibm-granite/granite-3.2-2b-instruct",
      "model_name": "ibm-granite/granite-3.2-2b-instruct",
      "size_bytes": 2533531648,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:31:34.176477",
      "end_time": "2026-01-20T19:39:01.522426",
      "duration_seconds": 447.35,
      "vllm_pid": 2516384,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.1-2b-instruct": {
      "model_id": "ibm-granite/granite-3.1-2b-instruct",
      "model_name": "ibm-granite/granite-3.1-2b-instruct",
      "size_bytes": 2533531648,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:31:32.893233",
      "end_time": "2026-01-20T19:39:03.638134",
      "duration_seconds": 450.74,
      "vllm_pid": 1876533,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PerceptronAI/Isaac-0.1": {
      "model_id": "PerceptronAI/Isaac-0.1",
      "model_name": "PerceptronAI/Isaac-0.1",
      "size_bytes": 2567073008,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:39:03.677383",
      "end_time": "2026-01-20T19:39:18.857140",
      "duration_seconds": 15.18,
      "vllm_pid": 1896324,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m Encountered exception while importing perceptron: No module named 'perceptron'\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 427, in get_cached_module_file\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     modules_needed = check_imports(resolved_module_file)\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 260, in check_imports\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=1896324)\u001b[0;0m ImportError: This modeling file requires the following packages that were not found in your environment: perceptron. Run `pip install perceptron`\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-guardian-3.0-2b": {
      "model_id": "ibm-granite/granite-guardian-3.0-2b",
      "model_name": "ibm-granite/granite-guardian-3.0-2b",
      "size_bytes": 2533531648,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:37:16.870197",
      "end_time": "2026-01-20T19:39:35.913803",
      "duration_seconds": 139.04,
      "vllm_pid": 1891379,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "anakin87/gemma-2b-orpo": {
      "model_id": "anakin87/gemma-2b-orpo",
      "model_name": "anakin87/gemma-2b-orpo",
      "size_bytes": 2506172416,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:30:31.339156",
      "end_time": "2026-01-20T19:39:38.606553",
      "duration_seconds": 547.27,
      "vllm_pid": 1874030,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Motif-Technologies/Motif-2.6B": {
      "model_id": "Motif-Technologies/Motif-2.6B",
      "model_name": "Motif-Technologies/Motif-2.6B",
      "size_bytes": 2597218432,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:39:35.980291",
      "end_time": "2026-01-20T19:39:51.153755",
      "duration_seconds": 15.17,
      "vllm_pid": 1897465,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m [2026-01-20 19:39:44] INFO configuration_motif.py:171:  kwargs : {'absolute_position_embedding': False, 'architectures': ['MotifForCausalLM'], 'auto_map': {'AutoConfig': 'configuration_motif.MotifConfig', 'AutoModelForCausalLM': 'modeling_motif.MotifForCausalLM'}, 'bos_token_id': 219396, 'eos_token_id': 219395, 'loss_reduction': 'mean', 'model_type': 'Motif', 'torch_dtype': 'bfloat16', 'transformers_version': '4.46.3', 'use_bias': False, '_commit_hash': '5b901f07cdecaa3a3f55ad1a1a89ef78bf2beb4d', 'attn_implementation': None}\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m [2026-01-20 19:39:44] INFO configuration_motif.py:171:  kwargs : {}\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m   Value error, Model architecture MotifForCausalLM was supported in vLLM until v0.10.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1897465)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Motif-Technologies/Motif-2.6b-v1.1-LC": {
      "model_id": "Motif-Technologies/Motif-2.6b-v1.1-LC",
      "model_name": "Motif-Technologies/Motif-2.6b-v1.1-LC",
      "size_bytes": 2597218432,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:39:38.680582",
      "end_time": "2026-01-20T19:39:53.855260",
      "duration_seconds": 15.17,
      "vllm_pid": 1897472,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m [2026-01-20 19:39:47] INFO configuration_motif.py:171:  kwargs : {'absolute_position_embedding': False, 'architectures': ['MotifForCausalLM'], 'auto_map': {'AutoConfig': 'configuration_motif.MotifConfig', 'AutoModelForCausalLM': 'modeling_motif.MotifForCausalLM'}, 'bos_token_id': 219396, 'eos_token_id': 219395, 'loss_reduction': 'mean', 'model_type': 'Motif', 'torch_dtype': 'bfloat16', 'transformers_version': '4.46.3', 'use_bias': False, '_commit_hash': '70bf316e166f2a256b1068e35c8310541a6a06bc', 'attn_implementation': None}\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m [2026-01-20 19:39:47] INFO configuration_motif.py:171:  kwargs : {}\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m   Value error, Model architecture MotifForCausalLM was supported in vLLM until v0.10.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1897472)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mobiuslabsgmbh/DeepSeek-R1-ReDistill-Qwen-1.5B-v1.0": {
      "model_id": "mobiuslabsgmbh/DeepSeek-R1-ReDistill-Qwen-1.5B-v1.0",
      "model_name": "mobiuslabsgmbh/DeepSeek-R1-ReDistill-Qwen-1.5B-v1.0",
      "size_bytes": 1777481216,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:05:48.168445",
      "end_time": "2026-01-20T19:40:06.728465",
      "duration_seconds": 2058.56,
      "vllm_pid": 2453025,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MBZUAI-Paris/Atlas-Chat-2B": {
      "model_id": "MBZUAI-Paris/Atlas-Chat-2B",
      "model_name": "MBZUAI-Paris/Atlas-Chat-2B",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:39:51.181690",
      "end_time": "2026-01-20T19:40:51.344442",
      "duration_seconds": 60.16,
      "vllm_pid": 1898197,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.37s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.54s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.82s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m 2026-01-20 19:40:40,667 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m 2026-01-20 19:40:40,676 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|\u258d         | 2/51 [00:00<00:02, 18.16it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:00<00:02, 21.83it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|\u2588\u258c        | 8/51 [00:00<00:01, 23.68it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|\u2588\u2588\u258e       | 12/51 [00:00<00:01, 26.03it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:00<00:01, 26.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|\u2588\u2588\u2588\u258c      | 18/51 [00:00<00:01, 27.45it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:00<00:01, 27.74it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:00<00:00, 29.50it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:01<00:00, 30.86it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:01<00:00, 31.55it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 37/51 [00:01<00:00, 31.88it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 41/51 [00:01<00:00, 31.91it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:01<00:00, 32.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 49/51 [00:01<00:00, 32.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 29.71it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1898999)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:40:43.758327571 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1898197)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Metin/Gemma-2-2B-TR-Knowledge-Graph": {
      "model_id": "Metin/Gemma-2-2B-TR-Knowledge-Graph",
      "model_name": "Metin/Gemma-2-2B-TR-Knowledge-Graph",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:39:53.882205",
      "end_time": "2026-01-20T19:40:54.033734",
      "duration_seconds": 60.15,
      "vllm_pid": 1898204,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  5.56it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.03s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.75s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m 2026-01-20 19:40:40,895 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m 2026-01-20 19:40:40,904 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|\u258d         | 2/51 [00:00<00:03, 15.47it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:00<00:02, 20.49it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|\u2588\u258c        | 8/51 [00:00<00:01, 23.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|\u2588\u2588\u258e       | 12/51 [00:00<00:01, 25.63it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:00<00:01, 25.70it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|\u2588\u2588\u2588\u258c      | 18/51 [00:00<00:01, 26.84it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|\u2588\u2588\u2588\u2588\u258e     | 22/51 [00:00<00:01, 28.57it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|\u2588\u2588\u2588\u2588\u2588     | 26/51 [00:00<00:00, 30.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 30/51 [00:01<00:00, 31.04it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 34/51 [00:01<00:00, 31.33it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 38/51 [00:01<00:00, 31.78it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:01<00:00, 32.01it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 46/51 [00:01<00:00, 31.79it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 50/51 [00:01<00:00, 32.93it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 29.39it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1899251)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:40:43.059183286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1898204)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Unbabel/Tower-Plus-2B": {
      "model_id": "Unbabel/Tower-Plus-2B",
      "model_name": "Unbabel/Tower-Plus-2B",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:40:06.732731",
      "end_time": "2026-01-20T19:40:56.920611",
      "duration_seconds": 50.19,
      "vllm_pid": 2535979,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  6.18it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.08s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.79s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m 2026-01-20 19:40:44,995 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m 2026-01-20 19:40:45,003 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:06,  7.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|\u258a         | 4/51 [00:00<00:02, 17.17it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:00<00:02, 21.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|\u2588\u2589        | 10/51 [00:00<00:01, 23.96it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:00<00:01, 25.34it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|\u2588\u2588\u2588\u258f      | 16/51 [00:00<00:01, 26.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|\u2588\u2588\u2588\u2589      | 20/51 [00:00<00:01, 28.10it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|\u2588\u2588\u2588\u2588\u258b     | 24/51 [00:00<00:00, 29.87it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 28/51 [00:01<00:00, 30.78it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 32/51 [00:01<00:00, 32.00it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:01<00:00, 32.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 40/51 [00:01<00:00, 32.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [00:01<00:00, 32.29it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:01<00:00, 32.13it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 29.10it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2536134)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:40:47.905885093 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2535979)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/gemma-2-baku-2b-it": {
      "model_id": "rinna/gemma-2-baku-2b-it",
      "model_name": "rinna/gemma-2-baku-2b-it",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:40:51.389096",
      "end_time": "2026-01-20T19:41:46.588919",
      "duration_seconds": 55.2,
      "vllm_pid": 1901049,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:00,  2.51it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:05<00:02,  2.91s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:08<00:00,  3.37s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:08<00:00,  2.99s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m 2026-01-20 19:41:36,553 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m 2026-01-20 19:41:36,561 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:05,  9.29it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|\u258a         | 4/51 [00:00<00:02, 18.93it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:00<00:01, 22.17it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|\u2588\u2589        | 10/51 [00:00<00:01, 24.61it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:00<00:01, 25.81it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|\u2588\u2588\u2588\u258f      | 16/51 [00:00<00:01, 26.43it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|\u2588\u2588\u2588\u2589      | 20/51 [00:00<00:01, 28.16it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|\u2588\u2588\u2588\u2588\u258b     | 24/51 [00:00<00:00, 29.77it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 28/51 [00:01<00:00, 30.58it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 32/51 [00:01<00:00, 31.72it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:01<00:00, 31.76it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 40/51 [00:01<00:00, 31.77it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [00:01<00:00, 31.36it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:01<00:00, 31.33it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 29.14it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901679)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:41:39.499704439 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1901049)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/Zamba2-2.7B": {
      "model_id": "Zyphra/Zamba2-2.7B",
      "model_name": "Zyphra/Zamba2-2.7B",
      "size_bytes": 2662214560,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:41:46.690424",
      "end_time": "2026-01-20T19:41:46.739480",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "silma-ai/SILMA-Kashif-2B-Instruct-v1.0": {
      "model_id": "silma-ai/SILMA-Kashif-2B-Instruct-v1.0",
      "model_name": "silma-ai/SILMA-Kashif-2B-Instruct-v1.0",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:40:54.086610",
      "end_time": "2026-01-20T19:41:49.250426",
      "duration_seconds": 55.16,
      "vllm_pid": 1901088,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.17s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.98s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m 2026-01-20 19:41:37,014 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m 2026-01-20 19:41:37,023 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:06,  8.18it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|\u258a         | 4/51 [00:00<00:02, 18.04it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:00<00:02, 21.69it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|\u2588\u2589        | 10/51 [00:00<00:01, 24.23it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:00<00:01, 25.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|\u2588\u2588\u2588\u258f      | 16/51 [00:00<00:01, 26.24it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|\u2588\u2588\u2588\u2589      | 20/51 [00:00<00:01, 28.01it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|\u2588\u2588\u2588\u2588\u258b     | 24/51 [00:00<00:00, 29.69it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 28/51 [00:01<00:00, 30.56it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 32/51 [00:01<00:00, 31.25it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:01<00:00, 31.44it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 40/51 [00:01<00:00, 31.56it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [00:01<00:00, 31.81it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:01<00:00, 31.72it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 29.00it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1901700)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:41:39.100023309 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1901088)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/MiniCPM-Reranker": {
      "model_id": "openbmb/MiniCPM-Reranker",
      "model_name": "openbmb/MiniCPM-Reranker",
      "size_bytes": 2724883200,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:41:49.290741",
      "end_time": "2026-01-20T19:41:49.341192",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/phi-2": {
      "model_id": "microsoft/phi-2",
      "model_name": "microsoft/phi-2",
      "size_bytes": 2779683840,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:41:49.348154",
      "end_time": "2026-01-20T19:41:49.395640",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "winninghealth/WiNGPT-Babel-2": {
      "model_id": "winninghealth/WiNGPT-Babel-2",
      "model_name": "winninghealth/WiNGPT-Babel-2",
      "size_bytes": 2614341888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:40:56.924061",
      "end_time": "2026-01-20T19:41:52.076041",
      "duration_seconds": 55.15,
      "vllm_pid": 2537622,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  6.25it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.19s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.89s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m 2026-01-20 19:41:37,507 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m 2026-01-20 19:41:37,516 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:06,  7.22it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|\u258a         | 4/51 [00:00<00:02, 16.75it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:00<00:02, 20.55it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|\u2588\u2589        | 10/51 [00:00<00:01, 23.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:00<00:01, 24.33it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|\u2588\u2588\u2588\u258f      | 16/51 [00:00<00:01, 25.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:00<00:01, 26.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:00<00:00, 28.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:01<00:00, 29.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:01<00:00, 30.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 35/51 [00:01<00:00, 30.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:01<00:00, 30.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 43/51 [00:01<00:00, 30.55it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 47/51 [00:01<00:00, 30.64it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 31.39it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 27.74it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2538245)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:41:40.521170034 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2537622)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-EEVE-2.8B": {
      "model_id": "yanolja/YanoljaNEXT-EEVE-2.8B",
      "model_name": "yanolja/YanoljaNEXT-EEVE-2.8B",
      "size_bytes": 2819340864,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:41:52.079486",
      "end_time": "2026-01-20T19:42:12.250333",
      "duration_seconds": 20.17,
      "vllm_pid": 2539223,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 406, in get_cached_module_file\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     resolved_module_file = cached_file(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m                            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/utils/hub.py\", line 322, in cached_file\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/utils/hub.py\", line 583, in cached_files\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m     raise OSError(\n\u001b[0;36m(APIServer pid=2539223)\u001b[0;0m OSError: microsoft/phi-2 does not appear to have a file named configuration_phi.py. Checkout 'https://huggingface.co/microsoft/phi-2/tree/main' for available files.\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ragraph-ai/stable-cypher-instruct-3b": {
      "model_id": "ragraph-ai/stable-cypher-instruct-3b",
      "model_name": "ragraph-ai/stable-cypher-instruct-3b",
      "size_bytes": 2796295168,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:41:49.401024",
      "end_time": "2026-01-20T19:42:24.547047",
      "duration_seconds": 35.15,
      "vllm_pid": 1903396,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/stablelm.py\", line 314, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.model = StableLMEpochModel(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/stablelm.py\", line 231, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/stablelm.py\", line 233, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     lambda prefix: StablelmDecoderLayer(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/stablelm.py\", line 186, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.self_attn = StablelmAttention(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/stablelm.py\", line 133, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=1904050)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W120 19:42:15.753985855 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1903396)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jhu-clsp/mmBERT-small": {
      "model_id": "jhu-clsp/mmBERT-small",
      "model_name": "jhu-clsp/mmBERT-small",
      "size_bytes": 2835604957,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:42:24.594181",
      "end_time": "2026-01-20T19:42:24.610554",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/Lucy": {
      "model_id": "Menlo/Lucy",
      "model_name": "Menlo/Lucy",
      "size_bytes": 1720574976,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:00:26.905391",
      "end_time": "2026-01-20T19:42:46.176341",
      "duration_seconds": 2539.27,
      "vllm_pid": 1802334,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance/Ouro-1.4B": {
      "model_id": "ByteDance/Ouro-1.4B",
      "model_name": "ByteDance/Ouro-1.4B",
      "size_bytes": 2870191799,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:42:24.617393",
      "end_time": "2026-01-20T19:43:27.199935",
      "duration_seconds": 62.58,
      "vllm_pid": 1904923,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ByteDance--Ouro-1.4B/snapshots/fcfcfbc429c7b10ee73dfc718033351d8936d2a8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-3B": {
      "model_id": "apple/OpenELM-3B",
      "model_name": "apple/OpenELM-3B",
      "size_bytes": 3036647424,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:43:27.298585",
      "end_time": "2026-01-20T19:43:27.344408",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/OpenELM-3B-Instruct": {
      "model_id": "apple/OpenELM-3B-Instruct",
      "model_name": "apple/OpenELM-3B-Instruct",
      "size_bytes": 3036647424,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:43:27.351156",
      "end_time": "2026-01-20T19:43:27.395179",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM3-3B": {
      "model_id": "HuggingFaceTB/SmolLM3-3B",
      "model_name": "HuggingFaceTB/SmolLM3-3B",
      "size_bytes": 3075098624,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:43:27.400243",
      "end_time": "2026-01-20T19:44:42.206484",
      "duration_seconds": 74.81,
      "vllm_pid": 1907767,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--HuggingFaceTB--SmolLM3-3B/snapshots/a07cc9a04f16550a088caea529712d1d335b0ac1` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM3-3B-Base": {
      "model_id": "HuggingFaceTB/SmolLM3-3B-Base",
      "model_name": "HuggingFaceTB/SmolLM3-3B-Base",
      "size_bytes": 3075098624,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:44:42.311289",
      "end_time": "2026-01-20T19:44:42.367026",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-EEVE-Instruct-2.8B": {
      "model_id": "yanolja/YanoljaNEXT-EEVE-Instruct-2.8B",
      "model_name": "yanolja/YanoljaNEXT-EEVE-Instruct-2.8B",
      "size_bytes": 2819340864,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T19:42:12.253686",
      "end_time": "2026-01-20T19:46:21.052776",
      "duration_seconds": 248.8,
      "vllm_pid": 2540415,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1112 input tokens (1536 > 2048 - 1112). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 110, in generate\n    return self.handle_bad_request(ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 150, in handle_bad_request\n    return openai_handle_bad_request(self.model_name, ex)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 569, in openai_handle_bad_request\n    raise e\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 1536. This model's maximum context length is 2048 tokens and your request has 1112 input tokens (1536 > 2048 - 1112). None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "agentica-org/DeepCoder-1.5B-Preview": {
      "model_id": "agentica-org/DeepCoder-1.5B-Preview",
      "model_name": "agentica-org/DeepCoder-1.5B-Preview",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:02:01.313097",
      "end_time": "2026-01-20T19:48:17.547198",
      "duration_seconds": 2776.23,
      "vllm_pid": 1807672,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yulan-team/YuLan-Mini": {
      "model_id": "yulan-team/YuLan-Mini",
      "model_name": "yulan-team/YuLan-Mini",
      "size_bytes": 2424375168,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:24:29.801255",
      "end_time": "2026-01-20T19:48:43.872579",
      "duration_seconds": 1454.07,
      "vllm_pid": 2501258,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/Lucy-128k": {
      "model_id": "Menlo/Lucy-128k",
      "model_name": "Menlo/Lucy-128k",
      "size_bytes": 1720574976,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:00:37.396740",
      "end_time": "2026-01-20T19:49:39.005008",
      "duration_seconds": 2941.61,
      "vllm_pid": 2436974,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.3-2b-instruct": {
      "model_id": "ibm-granite/granite-3.3-2b-instruct",
      "model_name": "ibm-granite/granite-3.3-2b-instruct",
      "size_bytes": 2533539840,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:39:01.527320",
      "end_time": "2026-01-20T19:51:49.628804",
      "duration_seconds": 768.1,
      "vllm_pid": 2533128,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "trillionlabs/Tri-1.8B-Translation": {
      "model_id": "trillionlabs/Tri-1.8B-Translation",
      "model_name": "trillionlabs/Tri-1.8B-Translation",
      "size_bytes": 1809426432,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:05:57.466662",
      "end_time": "2026-01-20T19:52:21.895538",
      "duration_seconds": 2784.43,
      "vllm_pid": 1818401,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B": {
      "model_id": "Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B",
      "model_name": "Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B",
      "size_bytes": 3088677584,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:52:21.953972",
      "end_time": "2026-01-20T19:52:42.083059",
      "duration_seconds": 20.13,
      "vllm_pid": 1929363,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m   Value error, Model architectures ['Qwen2ForPrmModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1929363)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "amd/Instella-3B": {
      "model_id": "amd/Instella-3B",
      "model_name": "amd/Instella-3B",
      "size_bytes": 3112675840,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:52:42.154388",
      "end_time": "2026-01-20T19:52:42.190122",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "amd/Instella-3B-Instruct": {
      "model_id": "amd/Instella-3B-Instruct",
      "model_name": "amd/Instella-3B-Instruct",
      "size_bytes": 3112675840,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:52:42.196267",
      "end_time": "2026-01-20T19:53:02.350346",
      "duration_seconds": 20.15,
      "vllm_pid": 1929645,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m   Value error, Model architectures ['InstellaForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1929645)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-1.7B": {
      "model_id": "Qwen/Qwen3-1.7B",
      "model_name": "Qwen/Qwen3-1.7B",
      "size_bytes": 2031739904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:07:16.064821",
      "end_time": "2026-01-20T19:53:03.565688",
      "duration_seconds": 2747.5,
      "vllm_pid": 2457256,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-10B-Instruct-1.58bit": {
      "model_id": "tiiuae/Falcon3-10B-Instruct-1.58bit",
      "model_name": "tiiuae/Falcon3-10B-Instruct-1.58bit",
      "size_bytes": 3180580120,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:53:02.355634",
      "end_time": "2026-01-20T19:53:17.492206",
      "duration_seconds": 15.14,
      "vllm_pid": 1931003,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m   Value error, Unknown quantization method: bitnet. Must be one of ['awq', 'deepspeedfp', 'tpu_int8', 'fp8', 'ptpc_fp8', 'fbgemm_fp8', 'fp_quant', 'modelopt', 'modelopt_fp4', 'bitblas', 'gguf', 'gptq_marlin_24', 'gptq_marlin', 'gptq_bitblas', 'awq_marlin', 'gptq', 'compressed-tensors', 'bitsandbytes', 'hqq', 'experts_int8', 'ipex', 'quark', 'moe_wna16', 'torchao', 'auto-round', 'rtn', 'inc', 'mxfp4', 'petit_nvfp4', 'cpu_gptq', 'cpu_awq']. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=1931003)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-micro-base": {
      "model_id": "ibm-granite/granite-4.0-h-micro-base",
      "model_name": "ibm-granite/granite-4.0-h-micro-base",
      "size_bytes": 3191396096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T19:53:17.557074",
      "end_time": "2026-01-20T19:53:17.599531",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-Research-Reasoning-Qwen-1.5B": {
      "model_id": "nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
      "model_name": "nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
      "size_bytes": 1777088000,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:05:30.829243",
      "end_time": "2026-01-20T19:54:26.561659",
      "duration_seconds": 2935.73,
      "vllm_pid": 1817564,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "driaforall/Dria-Agent-a-3B": {
      "model_id": "driaforall/Dria-Agent-a-3B",
      "model_name": "driaforall/Dria-Agent-a-3B",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:46:21.055971",
      "end_time": "2026-01-20T19:55:04.689312",
      "duration_seconds": 523.63,
      "vllm_pid": 2549383,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ModelSpace/GemmaX2-28-2B-v0.1": {
      "model_id": "ModelSpace/GemmaX2-28-2B-v0.1",
      "model_name": "ModelSpace/GemmaX2-28-2B-v0.1",
      "size_bytes": 3204165888,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T19:54:26.663126",
      "end_time": "2026-01-20T19:55:16.890270",
      "duration_seconds": 50.23,
      "vllm_pid": 1934369,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.25s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.49s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.75s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m 2026-01-20 19:55:06,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m 2026-01-20 19:55:07,007 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:06,  7.34it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|\u258a         | 4/51 [00:00<00:02, 16.84it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:00<00:02, 20.87it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|\u2588\u2589        | 10/51 [00:00<00:01, 23.67it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:00<00:01, 25.12it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|\u2588\u2588\u2588\u258f      | 16/51 [00:00<00:01, 25.98it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|\u2588\u2588\u2588\u2589      | 20/51 [00:00<00:01, 27.84it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|\u2588\u2588\u2588\u2588\u258b     | 24/51 [00:00<00:00, 29.55it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 28/51 [00:01<00:00, 30.41it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 32/51 [00:01<00:00, 31.22it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:01<00:00, 31.32it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 40/51 [00:01<00:00, 31.47it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [00:01<00:00, 31.56it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:01<00:00, 31.39it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:01<00:00, 28.59it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"<eval_with_key>.54\", line 225, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1934456)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W120 19:55:09.931507970 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1934369)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LiquidAI/LFM2-2.6B": {
      "model_id": "LiquidAI/LFM2-2.6B",
      "model_name": "LiquidAI/LFM2-2.6B",
      "size_bytes": 2569272320,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:39:18.878734",
      "end_time": "2026-01-20T19:56:55.024364",
      "duration_seconds": 1056.15,
      "vllm_pid": 1897227,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kakaocorp/kanana-nano-2.1b-base": {
      "model_id": "kakaocorp/kanana-nano-2.1b-base",
      "model_name": "kakaocorp/kanana-nano-2.1b-base",
      "size_bytes": 2086979328,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:07:39.634402",
      "end_time": "2026-01-20T20:00:02.721854",
      "duration_seconds": 3143.09,
      "vllm_pid": 2458428,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-3B": {
      "model_id": "unsloth/Llama-3.2-3B",
      "model_name": "unsloth/Llama-3.2-3B",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:00:02.727833",
      "end_time": "2026-01-20T20:00:02.789772",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-Coder-3B-Instruct": {
      "model_id": "Qwen/Qwen2.5-Coder-3B-Instruct",
      "model_name": "Qwen/Qwen2.5-Coder-3B-Instruct",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:44:42.373819",
      "end_time": "2026-01-20T20:00:21.471773",
      "duration_seconds": 939.1,
      "vllm_pid": 1910758,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "katanemo/Arch-Function-3B": {
      "model_id": "katanemo/Arch-Function-3B",
      "model_name": "katanemo/Arch-Function-3B",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:48:43.876378",
      "end_time": "2026-01-20T20:02:25.957110",
      "duration_seconds": 822.08,
      "vllm_pid": 2554673,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/DeepHermes-3-Llama-3-3B-Preview": {
      "model_id": "NousResearch/DeepHermes-3-Llama-3-3B-Preview",
      "model_name": "NousResearch/DeepHermes-3-Llama-3-3B-Preview",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:55:16.966259",
      "end_time": "2026-01-20T20:02:30.142183",
      "duration_seconds": 433.18,
      "vllm_pid": 1936538,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit": {
      "model_id": "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
      "model_name": "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
      "size_bytes": 3301123016,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:02:30.197263",
      "end_time": "2026-01-20T20:03:00.346028",
      "duration_seconds": 30.15,
      "vllm_pid": 1954338,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.model = self._init_model(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 611, in _init_model\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     return LlamaModel(vllm_config=vllm_config, prefix=prefix, layer_type=layer_type)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 393, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 395, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     lambda prefix: layer_type(vllm_config=vllm_config, prefix=prefix),\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 302, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.self_attn = LlamaAttention(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=1954433)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W120 20:02:50.305929339 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1954338)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-3B-bnb-4bit": {
      "model_id": "unsloth/Llama-3.2-3B-bnb-4bit",
      "model_name": "unsloth/Llama-3.2-3B-bnb-4bit",
      "size_bytes": 3301123026,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:03:00.399470",
      "end_time": "2026-01-20T20:03:00.429607",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "katanemo/Arch-Function-Chat-3B": {
      "model_id": "katanemo/Arch-Function-Chat-3B",
      "model_name": "katanemo/Arch-Function-Chat-3B",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:49:39.009614",
      "end_time": "2026-01-20T20:03:24.679928",
      "duration_seconds": 825.67,
      "vllm_pid": 2557195,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/Zamba2-2.7B-instruct": {
      "model_id": "Zyphra/Zamba2-2.7B-instruct",
      "model_name": "Zyphra/Zamba2-2.7B-instruct",
      "size_bytes": 2662214560,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:41:46.745163",
      "end_time": "2026-01-20T20:03:35.336082",
      "duration_seconds": 1308.59,
      "vllm_pid": 1903391,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "driaforall/Tiny-Agent-a-3B": {
      "model_id": "driaforall/Tiny-Agent-a-3B",
      "model_name": "driaforall/Tiny-Agent-a-3B",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:48:17.636370",
      "end_time": "2026-01-20T20:03:36.281069",
      "duration_seconds": 918.64,
      "vllm_pid": 1919585,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-micro-base": {
      "model_id": "ibm-granite/granite-4.0-micro-base",
      "model_name": "ibm-granite/granite-4.0-micro-base",
      "size_bytes": 3402836480,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:03:36.302832",
      "end_time": "2026-01-20T20:03:36.339806",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Writer/palmyra-mini": {
      "model_id": "Writer/palmyra-mini",
      "model_name": "Writer/palmyra-mini",
      "size_bytes": 3563972136,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:03:36.347162",
      "end_time": "2026-01-20T20:03:51.491932",
      "duration_seconds": 15.14,
      "vllm_pid": 1956878,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 114, in __init__\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     tokenizer = cached_tokenizer_from_config(self.model_config)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/registry.py\", line 219, in cached_tokenizer_from_config\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return cached_get_tokenizer(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/registry.py\", line 202, in get_tokenizer\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     tokenizer = tokenizer_cls_.from_pretrained(tokenizer_name, *args, **kwargs)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/hf.py\", line 79, in from_pretrained\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 1156, in from_pretrained\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2113, in from_pretrained\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return cls._from_pretrained(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2151, in _from_pretrained\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2359, in _from_pretrained\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     tokenizer = cls(*init_inputs, **init_kwargs)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py\", line 171, in __init__\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py\", line 198, in get_spm_processor\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     tokenizer.Load(self.vocab_file)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 961, in Load\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return self.LoadFromFile(model_file)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 316, in LoadFromFile\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1956878)\u001b[0;0m TypeError: not a string\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "vngrs-ai/Kumru-2B-Base": {
      "model_id": "vngrs-ai/Kumru-2B-Base",
      "model_name": "vngrs-ai/Kumru-2B-Base",
      "size_bytes": 2375138304,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:18:33.842980",
      "end_time": "2026-01-20T20:05:46.789727",
      "duration_seconds": 2832.95,
      "vllm_pid": 2487046,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-micro": {
      "model_id": "ibm-granite/granite-4.0-micro",
      "model_name": "ibm-granite/granite-4.0-micro",
      "size_bytes": 3402836480,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:03:35.402240",
      "end_time": "2026-01-20T20:06:16.665517",
      "duration_seconds": 161.26,
      "vllm_pid": 1956830,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-micro/snapshots/56111ae135df9c53a78c99028e7bc24035a9e979` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-micro/snapshots/56111ae135df9c53a78c99028e7bc24035a9e979` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Writer/palmyra-mini-thinking-a": {
      "model_id": "Writer/palmyra-mini-thinking-a",
      "model_name": "Writer/palmyra-mini-thinking-a",
      "size_bytes": 3563972687,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:03:51.503755",
      "end_time": "2026-01-20T20:06:42.692697",
      "duration_seconds": 171.19,
      "vllm_pid": 1957970,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Writer--palmyra-mini-thinking-a/snapshots/64444a2e8a36388f094a920a2870db0860236f0c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Writer--palmyra-mini-thinking-a/snapshots/64444a2e8a36388f094a920a2870db0860236f0c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "malteos/german-r1": {
      "model_id": "malteos/german-r1",
      "model_name": "malteos/german-r1",
      "size_bytes": 3085938688,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:51:49.632487",
      "end_time": "2026-01-20T20:07:02.549111",
      "duration_seconds": 912.92,
      "vllm_pid": 2562394,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jhu-clsp/mmBERT-base": {
      "model_id": "jhu-clsp/mmBERT-base",
      "model_name": "jhu-clsp/mmBERT-base",
      "size_bytes": 3711031983,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:07:02.553574",
      "end_time": "2026-01-20T20:07:02.559693",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B": {
      "model_id": "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B",
      "model_name": "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B",
      "size_bytes": 3721243520,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:07:02.567560",
      "end_time": "2026-01-20T20:07:27.741691",
      "duration_seconds": 25.17,
      "vllm_pid": 2601856,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m   Value error, Model architectures ['HCXVisionForCausalLM'] failed to be inspected. Please check the logs for more details. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2601856)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-3B-Instruct": {
      "model_id": "tiiuae/Falcon3-3B-Instruct",
      "model_name": "tiiuae/Falcon3-3B-Instruct",
      "size_bytes": 3227655168,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:00:21.488286",
      "end_time": "2026-01-20T20:08:31.899208",
      "duration_seconds": 490.41,
      "vllm_pid": 1949300,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inference-net/Schematron-3B": {
      "model_id": "inference-net/Schematron-3B",
      "model_name": "inference-net/Schematron-3B",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:56:55.074246",
      "end_time": "2026-01-20T20:11:17.189580",
      "duration_seconds": 862.12,
      "vllm_pid": 1940157,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Phi-3.5-mini-instruct_Uncensored": {
      "model_id": "SicariusSicariiStuff/Phi-3.5-mini-instruct_Uncensored",
      "model_name": "SicariusSicariiStuff/Phi-3.5-mini-instruct_Uncensored",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:11:17.238137",
      "end_time": "2026-01-20T20:11:32.382909",
      "duration_seconds": 15.14,
      "vllm_pid": 1977069,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 406, in get_cached_module_file\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     resolved_module_file = cached_file(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m                            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/utils/hub.py\", line 322, in cached_file\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/utils/hub.py\", line 437, in cached_files\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m     raise OSError(\n\u001b[0;36m(APIServer pid=1977069)\u001b[0;0m OSError: /mnt/baai_cp_perf/qbw/models--SicariusSicariiStuff--Phi-3.5-mini-instruct_Uncensored/snapshots/e8d9599bda7f5e0ec559d92d6e35ad261050687f does not appear to have a file named configuration_phi3.py. Checkout 'https://huggingface.co//mnt/baai_cp_perf/qbw/models--SicariusSicariiStuff--Phi-3.5-mini-instruct_Uncensored/snapshots/e8d9599bda7f5e0ec559d92d6e35ad261050687f/tree/main' for available files.\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404": {
      "model_id": "Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404",
      "model_name": "Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:55:04.694427",
      "end_time": "2026-01-20T20:12:21.382275",
      "duration_seconds": 1036.69,
      "vllm_pid": 2571551,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.1-3b-a800m-instruct": {
      "model_id": "ibm-granite/granite-3.1-3b-a800m-instruct",
      "model_name": "ibm-granite/granite-3.1-3b-a800m-instruct",
      "size_bytes": 3298793472,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:02:25.965440",
      "end_time": "2026-01-20T20:13:43.742163",
      "duration_seconds": 677.78,
      "vllm_pid": 2589522,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Llama-3.2-3B-Instruct": {
      "model_id": "unsloth/Llama-3.2-3B-Instruct",
      "model_name": "unsloth/Llama-3.2-3B-Instruct",
      "size_bytes": 3212749824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:00:02.798552",
      "end_time": "2026-01-20T20:16:58.838661",
      "duration_seconds": 1016.04,
      "vllm_pid": 2584038,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Arcee-VyLinh": {
      "model_id": "arcee-ai/Arcee-VyLinh",
      "model_name": "arcee-ai/Arcee-VyLinh",
      "size_bytes": 3397103616,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:03:24.684001",
      "end_time": "2026-01-20T20:17:43.717929",
      "duration_seconds": 859.03,
      "vllm_pid": 2591728,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepcogito/cogito-v1-preview-llama-3B": {
      "model_id": "deepcogito/cogito-v1-preview-llama-3B",
      "model_name": "deepcogito/cogito-v1-preview-llama-3B",
      "size_bytes": 3606752256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:06:16.715570",
      "end_time": "2026-01-20T20:18:30.847395",
      "duration_seconds": 734.13,
      "vllm_pid": 1964897,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/MediPhi-Instruct": {
      "model_id": "microsoft/MediPhi-Instruct",
      "model_name": "microsoft/MediPhi-Instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:11:32.439300",
      "end_time": "2026-01-20T20:22:27.250288",
      "duration_seconds": 654.81,
      "vllm_pid": 1977269,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lianghsun/Llama-3.2-Taiwan-3B-Instruct": {
      "model_id": "lianghsun/Llama-3.2-Taiwan-3B-Instruct",
      "model_name": "lianghsun/Llama-3.2-Taiwan-3B-Instruct",
      "size_bytes": 3606752256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:06:42.717491",
      "end_time": "2026-01-20T20:22:49.874116",
      "duration_seconds": 967.16,
      "vllm_pid": 1965130,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moelanoby/phi-3-M3-coder": {
      "model_id": "moelanoby/phi-3-M3-coder",
      "model_name": "moelanoby/phi-3-M3-coder",
      "size_bytes": 3823686912,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:22:27.285129",
      "end_time": "2026-01-20T20:22:57.467294",
      "duration_seconds": 30.18,
      "vllm_pid": 2004346,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.71s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 640, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     return loader.load_weights(\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 261, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     loaded_params = module_load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 507, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m     param = params_dict[name]\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m             ~~~~~~~~~~~^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m KeyError: 'layers.15.mlp.gate_up_proj.correction_head.weight'\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.29s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2004412)\u001b[0;0m \n[rank0]:[W120 20:22:49.676991593 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2004346)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zstanjj/HTML-Pruner-Phi-3.8B": {
      "model_id": "zstanjj/HTML-Pruner-Phi-3.8B",
      "model_name": "zstanjj/HTML-Pruner-Phi-3.8B",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:18:30.868336",
      "end_time": "2026-01-20T20:23:55.349407",
      "duration_seconds": 324.48,
      "vllm_pid": 1994416,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-mini-instruct": {
      "model_id": "microsoft/Phi-4-mini-instruct",
      "model_name": "microsoft/Phi-4-mini-instruct",
      "size_bytes": 3836021760,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:22:49.886554",
      "end_time": "2026-01-20T20:25:30.677104",
      "duration_seconds": 160.79,
      "vllm_pid": 2005098,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/5a149550068a1eb93398160d8953f5f56c3603e9` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/5a149550068a1eb93398160d8953f5f56c3603e9` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-mini-flash-reasoning": {
      "model_id": "microsoft/Phi-4-mini-flash-reasoning",
      "model_name": "microsoft/Phi-4-mini-flash-reasoning",
      "size_bytes": 3852562944,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:25:30.699362",
      "end_time": "2026-01-20T20:25:45.867042",
      "duration_seconds": 15.17,
      "vllm_pid": 2011396,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d_cuda: No module named 'causal_conv1d_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing selective_scan_cuda: No module named 'selective_scan_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d_cuda: No module named 'causal_conv1d_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing selective_scan_cuda: No module named 'selective_scan_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d_cuda: No module named 'causal_conv1d_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing selective_scan_cuda: No module named 'selective_scan_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing causal_conv1d_cuda: No module named 'causal_conv1d_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing flash_attn: No module named 'flash_attn'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Encountered exception while importing selective_scan_cuda: No module named 'selective_scan_cuda'\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m   Value error, Model architecture Phi4FlashForCausalLM was supported in vLLM until v0.10.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2011396)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-mini-reasoning": {
      "model_id": "microsoft/Phi-4-mini-reasoning",
      "model_name": "microsoft/Phi-4-mini-reasoning",
      "size_bytes": 3836021760,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:22:57.487304",
      "end_time": "2026-01-20T20:25:50.834424",
      "duration_seconds": 173.35,
      "vllm_pid": 2005146,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--microsoft--Phi-4-mini-reasoning/snapshots/7a8c4e2e81eae20a606d811f475d7dc316dd916a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--microsoft--Phi-4-mini-reasoning/snapshots/7a8c4e2e81eae20a606d811f475d7dc316dd916a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PatronusAI/glider": {
      "model_id": "PatronusAI/glider",
      "model_name": "PatronusAI/glider",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:08:31.925501",
      "end_time": "2026-01-20T20:26:00.679281",
      "duration_seconds": 1048.75,
      "vllm_pid": 1969878,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tomg-group-umd/huginn-0125": {
      "model_id": "tomg-group-umd/huginn-0125",
      "model_name": "tomg-group-umd/huginn-0125",
      "size_bytes": 3911400096,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:26:00.703079",
      "end_time": "2026-01-20T20:26:15.884262",
      "duration_seconds": 15.18,
      "vllm_pid": 2012236,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m   Value error, Model architectures ['RavenForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2012236)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit": {
      "model_id": "unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit",
      "model_name": "unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit",
      "size_bytes": 3929865122,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T20:26:15.903427",
      "end_time": "2026-01-20T20:26:46.043284",
      "duration_seconds": 30.14,
      "vllm_pid": 2013496,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.model = self._init_model(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 611, in _init_model\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     return LlamaModel(vllm_config=vllm_config, prefix=prefix, layer_type=layer_type)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 393, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 395, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     lambda prefix: layer_type(vllm_config=vllm_config, prefix=prefix),\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 302, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.self_attn = LlamaAttention(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=2013711)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W120 20:26:36.526855551 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2013496)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jet-ai/Jet-Nemotron-4B": {
      "model_id": "jet-ai/Jet-Nemotron-4B",
      "model_name": "jet-ai/Jet-Nemotron-4B",
      "size_bytes": 3960424768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:26:46.106091",
      "end_time": "2026-01-20T20:27:41.652813",
      "duration_seconds": 55.55,
      "vllm_pid": 2015081,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--jet-ai--Jet-Nemotron-4B/snapshots/950eaee5ae588210ed2e49d9bc623231aa27fdea` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "JetBrains/Mellum-4b-base": {
      "model_id": "JetBrains/Mellum-4b-base",
      "model_name": "JetBrains/Mellum-4b-base",
      "size_bytes": 4019248128,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:27:41.709958",
      "end_time": "2026-01-20T20:27:41.755871",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "JetBrains/Mellum-4b-sft-kotlin": {
      "model_id": "JetBrains/Mellum-4b-sft-kotlin",
      "model_name": "JetBrains/Mellum-4b-sft-kotlin",
      "size_bytes": 4019248128,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:27:41.761904",
      "end_time": "2026-01-20T20:27:41.808761",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "JetBrains/Mellum-4b-sft-python": {
      "model_id": "JetBrains/Mellum-4b-sft-python",
      "model_name": "JetBrains/Mellum-4b-sft-python",
      "size_bytes": 4019248128,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:27:41.814424",
      "end_time": "2026-01-20T20:27:41.857406",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MBZUAI-Paris/Nile-Chat-4B": {
      "model_id": "MBZUAI-Paris/Nile-Chat-4B",
      "model_name": "MBZUAI-Paris/Nile-Chat-4B",
      "size_bytes": 3880263168,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:25:45.900584",
      "end_time": "2026-01-20T20:28:25.300636",
      "duration_seconds": 159.4,
      "vllm_pid": 2012150,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--MBZUAI-Paris--Nile-Chat-4B/snapshots/2a365667659bbd6786c2760d868e37c506d76ce3` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--MBZUAI-Paris--Nile-Chat-4B/snapshots/2a365667659bbd6786c2760d868e37c506d76ce3` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-Rosetta-4B": {
      "model_id": "yanolja/YanoljaNEXT-Rosetta-4B",
      "model_name": "yanolja/YanoljaNEXT-Rosetta-4B",
      "size_bytes": 3880263168,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:25:50.900974",
      "end_time": "2026-01-20T20:28:36.078778",
      "duration_seconds": 165.18,
      "vllm_pid": 2012159,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--yanolja--YanoljaNEXT-Rosetta-4B/snapshots/b3d3ea827eb0ba002f0e4048071268e9af31370d` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--yanolja--YanoljaNEXT-Rosetta-4B/snapshots/b3d3ea827eb0ba002f0e4048071268e9af31370d` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FractalAIResearch/Fathom-Synthesizer-4B": {
      "model_id": "FractalAIResearch/Fathom-Synthesizer-4B",
      "model_name": "FractalAIResearch/Fathom-Synthesizer-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:28:25.313794",
      "end_time": "2026-01-20T20:29:32.975040",
      "duration_seconds": 67.66,
      "vllm_pid": 2018993,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--FractalAIResearch--Fathom-Synthesizer-4B/snapshots/fa37e5602641e5009b80bb51c2197a8069aa18de` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-mini-4k-instruct": {
      "model_id": "microsoft/Phi-3-mini-4k-instruct",
      "model_name": "microsoft/Phi-3-mini-4k-instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:13:43.750171",
      "end_time": "2026-01-20T20:29:55.591222",
      "duration_seconds": 971.84,
      "vllm_pid": 2617755,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-tiny-MoE-instruct": {
      "model_id": "microsoft/Phi-tiny-MoE-instruct",
      "model_name": "microsoft/Phi-tiny-MoE-instruct",
      "size_bytes": 3755220288,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:07:27.745546",
      "end_time": "2026-01-20T20:30:05.707348",
      "duration_seconds": 1357.96,
      "vllm_pid": 2602796,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-1.7B-FP8": {
      "model_id": "Qwen/Qwen3-1.7B-FP8",
      "model_name": "Qwen/Qwen3-1.7B-FP8",
      "size_bytes": 2031825920,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:07:33.339001",
      "end_time": "2026-01-20T20:30:14.017114",
      "duration_seconds": 4960.68,
      "vllm_pid": 1823039,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sbintuitions/sarashina2.2-3b-instruct-v0.1": {
      "model_id": "sbintuitions/sarashina2.2-3b-instruct-v0.1",
      "model_name": "sbintuitions/sarashina2.2-3b-instruct-v0.1",
      "size_bytes": 3355609600,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:03:00.435294",
      "end_time": "2026-01-20T20:30:39.166361",
      "duration_seconds": 1658.73,
      "vllm_pid": 1955293,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Pinkstack/DistilGPT-OSS-qwen3-4B": {
      "model_id": "Pinkstack/DistilGPT-OSS-qwen3-4B",
      "model_name": "Pinkstack/DistilGPT-OSS-qwen3-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:30:14.124368",
      "end_time": "2026-01-20T20:31:06.726382",
      "duration_seconds": 52.6,
      "vllm_pid": 2024208,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Pinkstack--DistilGPT-OSS-qwen3-4B/snapshots/1e918870ecf8387bcff3b8e7091363d3e7285b22` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-mini-128k-instruct": {
      "model_id": "microsoft/Phi-3-mini-128k-instruct",
      "model_name": "microsoft/Phi-3-mini-128k-instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:12:21.387819",
      "end_time": "2026-01-20T20:31:20.392397",
      "duration_seconds": 1139.0,
      "vllm_pid": 2614512,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-AWQ": {
      "model_id": "Qwen/Qwen3-4B-AWQ",
      "model_name": "Qwen/Qwen3-4B-AWQ",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:31:06.826901",
      "end_time": "2026-01-20T20:32:12.387921",
      "duration_seconds": 65.56,
      "vllm_pid": 2026390,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen3-4B-AWQ/snapshots/74d4bd2bd4bff9cafc9345221320bffb08b406a3` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Phi-4-mini-instruct": {
      "model_id": "unsloth/Phi-4-mini-instruct",
      "model_name": "unsloth/Phi-4-mini-instruct",
      "size_bytes": 3836021760,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:23:55.391947",
      "end_time": "2026-01-20T20:37:42.950345",
      "duration_seconds": 827.56,
      "vllm_pid": 2007791,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3.5-mini-instruct": {
      "model_id": "microsoft/Phi-3.5-mini-instruct",
      "model_name": "microsoft/Phi-3.5-mini-instruct",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:16:58.846007",
      "end_time": "2026-01-20T20:40:54.806624",
      "duration_seconds": 1435.96,
      "vllm_pid": 2625510,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tesslate/UIGEN-X-4B-0729": {
      "model_id": "Tesslate/UIGEN-X-4B-0729",
      "model_name": "Tesslate/UIGEN-X-4B-0729",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-20T20:40:54.814840",
      "end_time": "2026-01-20T20:40:54.880604",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--Tesslate--UIGEN-X-4B-0729/snapshots/17e1cd7f7bc46f552aa54c4634357111c26e2eb0/model-00002-of-00002.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "numind/NuExtract-1.5": {
      "model_id": "numind/NuExtract-1.5",
      "model_name": "numind/NuExtract-1.5",
      "size_bytes": 3821079552,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:17:43.721957",
      "end_time": "2026-01-20T20:41:01.336210",
      "duration_seconds": 1397.61,
      "vllm_pid": 2627123,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "driaforall/mem-agent": {
      "model_id": "driaforall/mem-agent",
      "model_name": "driaforall/mem-agent",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:41:01.340162",
      "end_time": "2026-01-20T20:43:54.615056",
      "duration_seconds": 173.27,
      "vllm_pid": 2679393,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--driaforall--mem-agent/snapshots/b409b82d4733e5e67abca25a36750a58e2dfb87f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--driaforall--mem-agent/snapshots/b409b82d4733e5e67abca25a36750a58e2dfb87f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Writer/palmyra-mini-thinking-b": {
      "model_id": "Writer/palmyra-mini-thinking-b",
      "model_name": "Writer/palmyra-mini-thinking-b",
      "size_bytes": 3568761805,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:05:46.798260",
      "end_time": "2026-01-20T20:44:51.186965",
      "duration_seconds": 2344.39,
      "vllm_pid": 2597968,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2": {
      "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
      "model_name": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v2",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:28:36.114883",
      "end_time": "2026-01-20T20:48:29.621905",
      "duration_seconds": 1193.51,
      "vllm_pid": 2019514,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/Jan-nano": {
      "model_id": "Menlo/Jan-nano",
      "model_name": "Menlo/Jan-nano",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:29:55.596889",
      "end_time": "2026-01-20T20:48:46.753061",
      "duration_seconds": 1131.16,
      "vllm_pid": 2654614,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Instruct-2507": {
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "model_name": "Qwen/Qwen3-4B-Instruct-2507",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:32:12.432101",
      "end_time": "2026-01-20T20:49:47.248095",
      "duration_seconds": 1054.82,
      "vllm_pid": 2028969,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-abliterated-v2": {
      "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-abliterated-v2",
      "model_name": "Goekdeniz-Guelmez/Josiefied-Qwen3-4B-abliterated-v2",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:29:33.025853",
      "end_time": "2026-01-20T20:51:33.319238",
      "duration_seconds": 1320.29,
      "vllm_pid": 2022358,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Menlo/Jan-nano-128k": {
      "model_id": "Menlo/Jan-nano-128k",
      "model_name": "Menlo/Jan-nano-128k",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:30:05.711617",
      "end_time": "2026-01-20T20:52:18.462435",
      "duration_seconds": 1332.75,
      "vllm_pid": 2655665,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LGAI-EXAONE/EXAONE-Deep-2.4B": {
      "model_id": "LGAI-EXAONE/EXAONE-Deep-2.4B",
      "model_name": "LGAI-EXAONE/EXAONE-Deep-2.4B",
      "size_bytes": 2405327360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:22:41.585869",
      "end_time": "2026-01-20T20:57:58.038637",
      "duration_seconds": 5716.45,
      "vllm_pid": 2496556,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tesslate/WEBGEN-4B-Preview": {
      "model_id": "Tesslate/WEBGEN-4B-Preview",
      "model_name": "Tesslate/WEBGEN-4B-Preview",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:40:54.887520",
      "end_time": "2026-01-20T20:58:19.514162",
      "duration_seconds": 1044.63,
      "vllm_pid": 2678821,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "erax-ai/EraX-Translator-V1.0": {
      "model_id": "erax-ai/EraX-Translator-V1.0",
      "model_name": "erax-ai/EraX-Translator-V1.0",
      "size_bytes": 4300079472,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:57:58.044418",
      "end_time": "2026-01-20T21:00:10.698756",
      "duration_seconds": 132.65,
      "vllm_pid": 2716995,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"unexpected char '\\\\\\\\' at 418 None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "neo4j/text-to-cypher-Gemma-3-4B-Instruct-2025.04.0": {
      "model_id": "neo4j/text-to-cypher-Gemma-3-4B-Instruct-2025.04.0",
      "model_name": "neo4j/text-to-cypher-Gemma-3-4B-Instruct-2025.04.0",
      "size_bytes": 4300079472,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T20:58:19.518499",
      "end_time": "2026-01-20T21:00:17.164888",
      "duration_seconds": 117.65,
      "vllm_pid": 2717745,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"unexpected char '\\\\\\\\' at 418 None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "soob3123/amoral-gemma3-4B-v1": {
      "model_id": "soob3123/amoral-gemma3-4B-v1",
      "model_name": "soob3123/amoral-gemma3-4B-v1",
      "size_bytes": 4300079472,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-20T21:00:17.168531",
      "end_time": "2026-01-20T21:00:47.354867",
      "duration_seconds": 30.19,
      "vllm_pid": 2722911,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py\", line 507, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2723594)\u001b[0;0m \n[rank0]:[W120 21:00:37.463682160 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2722911)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Intelligent-Internet/II-Search-4B": {
      "model_id": "Intelligent-Internet/II-Search-4B",
      "model_name": "Intelligent-Internet/II-Search-4B",
      "size_bytes": 4411424256,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-20T21:00:47.358336",
      "end_time": "2026-01-20T21:02:06.270292",
      "duration_seconds": 78.91,
      "vllm_pid": 2723989,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Intelligent-Internet--II-Search-4B/snapshots/d54fe4d3321120859f68e38ea9169e704778e5d7` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated": {
      "model_id": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
      "model_name": "huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:43:54.618637",
      "end_time": "2026-01-20T21:04:50.715929",
      "duration_seconds": 1256.1,
      "vllm_pid": 2685695,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3Guard-Gen-4B": {
      "model_id": "Qwen/Qwen3Guard-Gen-4B",
      "model_name": "Qwen/Qwen3Guard-Gen-4B",
      "size_bytes": 4411424256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T21:04:50.720142",
      "end_time": "2026-01-20T21:07:53.263322",
      "duration_seconds": 182.54,
      "vllm_pid": 2733407,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-vision-128k-instruct": {
      "model_id": "microsoft/Phi-3-vision-128k-instruct",
      "model_name": "microsoft/Phi-3-vision-128k-instruct",
      "size_bytes": 4146621440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:51:33.412440",
      "end_time": "2026-01-20T21:12:13.137213",
      "duration_seconds": 1239.72,
      "vllm_pid": 2071299,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Base": {
      "model_id": "Qwen/Qwen3-4B-Base",
      "model_name": "Qwen/Qwen3-4B-Base",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:31:20.397136",
      "end_time": "2026-01-20T21:14:56.963722",
      "duration_seconds": 2616.57,
      "vllm_pid": 2658495,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sarvamai/sarvam-translate": {
      "model_id": "sarvamai/sarvam-translate",
      "model_name": "sarvamai/sarvam-translate",
      "size_bytes": 4300079472,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T21:00:10.703204",
      "end_time": "2026-01-20T21:15:00.819767",
      "duration_seconds": 890.12,
      "vllm_pid": 2722881,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lastmass/Qwen3_Medical_GRPO": {
      "model_id": "lastmass/Qwen3_Medical_GRPO",
      "model_name": "lastmass/Qwen3_Medical_GRPO",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:48:46.758312",
      "end_time": "2026-01-20T21:16:27.281254",
      "duration_seconds": 1660.52,
      "vllm_pid": 2696820,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Reasoning-3B": {
      "model_id": "ai21labs/AI21-Jamba-Reasoning-3B",
      "model_name": "ai21labs/AI21-Jamba-Reasoning-3B",
      "size_bytes": 3197109632,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:53:17.604857",
      "end_time": "2026-01-20T21:17:28.367761",
      "duration_seconds": 5050.76,
      "vllm_pid": 1931733,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FractalAIResearch/Fathom-Search-4B": {
      "model_id": "FractalAIResearch/Fathom-Search-4B",
      "model_name": "FractalAIResearch/Fathom-Search-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:27:41.862895",
      "end_time": "2026-01-20T21:23:50.133290",
      "duration_seconds": 3368.27,
      "vllm_pid": 2017028,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SamuelBang/AesCoder-4B": {
      "model_id": "SamuelBang/AesCoder-4B",
      "model_name": "SamuelBang/AesCoder-4B",
      "size_bytes": 4411424256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T21:07:53.267764",
      "end_time": "2026-01-20T21:26:26.155737",
      "duration_seconds": 1112.89,
      "vllm_pid": 2741240,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B": {
      "model_id": "Qwen/Qwen3-4B",
      "model_name": "Qwen/Qwen3-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:30:39.225323",
      "end_time": "2026-01-20T21:31:20.861891",
      "duration_seconds": 3641.64,
      "vllm_pid": 2024944,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance/Ouro-1.4B-Thinking": {
      "model_id": "ByteDance/Ouro-1.4B-Thinking",
      "model_name": "ByteDance/Ouro-1.4B-Thinking",
      "size_bytes": 2870509465,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T19:42:46.195744",
      "end_time": "2026-01-20T21:38:22.976640",
      "duration_seconds": 6936.78,
      "vllm_pid": 1905949,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-SafeRL": {
      "model_id": "Qwen/Qwen3-4B-SafeRL",
      "model_name": "Qwen/Qwen3-4B-SafeRL",
      "size_bytes": 4411424256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T21:02:06.273597",
      "end_time": "2026-01-20T21:58:46.607448",
      "duration_seconds": 3400.33,
      "vllm_pid": 2727943,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "janhq/Jan-v1-4B": {
      "model_id": "janhq/Jan-v1-4B",
      "model_name": "janhq/Jan-v1-4B",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:48:29.699966",
      "end_time": "2026-01-20T22:08:16.899074",
      "duration_seconds": 4787.2,
      "vllm_pid": 2064547,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Thinking-2507": {
      "model_id": "Qwen/Qwen3-4B-Thinking-2507",
      "model_name": "Qwen/Qwen3-4B-Thinking-2507",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:37:43.054739",
      "end_time": "2026-01-20T22:17:16.580359",
      "duration_seconds": 5973.53,
      "vllm_pid": 2041019,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-4B-Instruct": {
      "model_id": "tencent/Hunyuan-4B-Instruct",
      "model_name": "tencent/Hunyuan-4B-Instruct",
      "size_bytes": 4221757440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:52:18.468156",
      "end_time": "2026-01-20T22:21:20.916159",
      "duration_seconds": 5342.45,
      "vllm_pid": 2704774,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated": {
      "model_id": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
      "model_name": "huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated",
      "size_bytes": 4022468096,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:44:51.191846",
      "end_time": "2026-01-20T22:25:47.763085",
      "duration_seconds": 6056.57,
      "vllm_pid": 2688063,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-FP8": {
      "model_id": "Qwen/Qwen3-4B-FP8",
      "model_name": "Qwen/Qwen3-4B-FP8",
      "size_bytes": 4411646016,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T21:12:13.195784",
      "end_time": "2026-01-20T23:12:28.240937",
      "duration_seconds": 7215.05,
      "vllm_pid": 2113251,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TIGER-Lab/VLM2Vec-Full": {
      "model_id": "TIGER-Lab/VLM2Vec-Full",
      "model_name": "TIGER-Lab/VLM2Vec-Full",
      "size_bytes": 4146621440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-20T20:49:47.305089",
      "end_time": "2026-01-20T23:32:51.273114",
      "duration_seconds": 9783.97,
      "vllm_pid": 2067479,
      "vllm_version": null,
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Impish_LLAMA_4B": {
      "model_id": "SicariusSicariiStuff/Impish_LLAMA_4B",
      "model_name": "SicariusSicariiStuff/Impish_LLAMA_4B",
      "size_bytes": 4512746496,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.629519",
      "end_time": "2026-01-21T17:10:21.629526",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AryanLala/autonlp-Scientific_Title_Generator-34558227": {
      "model_id": "AryanLala/autonlp-Scientific_Title_Generator-34558227",
      "model_name": "AryanLala/autonlp-Scientific_Title_Generator-34558227",
      "size_bytes": 4561009881,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.648145",
      "end_time": "2026-01-21T17:10:21.648153",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/deepseek-r1-distill-qwen-1.5b": {
      "model_id": "mlx-community/deepseek-r1-distill-qwen-1.5b",
      "model_name": "mlx-community/deepseek-r1-distill-qwen-1.5b",
      "size_bytes": 4565538539,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.666415",
      "end_time": "2026-01-21T17:10:21.666423",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Unbabel/wmt22-comet-da": {
      "model_id": "Unbabel/wmt22-comet-da",
      "model_name": "Unbabel/wmt22-comet-da",
      "size_bytes": 4647172101,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.685864",
      "end_time": "2026-01-21T17:10:21.685873",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ChatterjeeLab/PepMLM-650M": {
      "model_id": "ChatterjeeLab/PepMLM-650M",
      "model_name": "ChatterjeeLab/PepMLM-650M",
      "size_bytes": 5219123874,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.705013",
      "end_time": "2026-01-21T17:10:21.705021",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "knowledgator/gliner-multitask-large-v0.5": {
      "model_id": "knowledgator/gliner-multitask-large-v0.5",
      "model_name": "knowledgator/gliner-multitask-large-v0.5",
      "size_bytes": 5285552704,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.723503",
      "end_time": "2026-01-21T17:10:21.723511",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "suayptalha/Arcana-Qwen3-2.4B-A0.6B": {
      "model_id": "suayptalha/Arcana-Qwen3-2.4B-A0.6B",
      "model_name": "suayptalha/Arcana-Qwen3-2.4B-A0.6B",
      "size_bytes": 5358966990,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.747684",
      "end_time": "2026-01-21T17:10:21.747694",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5p-770m": {
      "model_id": "Salesforce/codet5p-770m",
      "model_name": "Salesforce/codet5p-770m",
      "size_bytes": 5891447267,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.766464",
      "end_time": "2026-01-21T17:10:21.766473",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dropbox-dash/Llama-3.1-8b-instruct_4bitgs64_hqq_calib": {
      "model_id": "dropbox-dash/Llama-3.1-8b-instruct_4bitgs64_hqq_calib",
      "model_name": "dropbox-dash/Llama-3.1-8b-instruct_4bitgs64_hqq_calib",
      "size_bytes": 6039955314,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.784755",
      "end_time": "2026-01-21T17:10:21.784763",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Trinity-Nano-Base": {
      "model_id": "arcee-ai/Trinity-Nano-Base",
      "model_name": "arcee-ai/Trinity-Nano-Base",
      "size_bytes": 6120003328,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.803137",
      "end_time": "2026-01-21T17:10:21.803145",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Trinity-Nano-Preview": {
      "model_id": "arcee-ai/Trinity-Nano-Preview",
      "model_name": "arcee-ai/Trinity-Nano-Preview",
      "size_bytes": 6120003328,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.821437",
      "end_time": "2026-01-21T17:10:21.821446",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "knowledgator/gliner-decoder-large-v1.0": {
      "model_id": "knowledgator/gliner-decoder-large-v1.0",
      "model_name": "knowledgator/gliner-decoder-large-v1.0",
      "size_bytes": 6185670847,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.840064",
      "end_time": "2026-01-21T17:10:21.840072",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EvaByte/EvaByte": {
      "model_id": "EvaByte/EvaByte",
      "model_name": "EvaByte/EvaByte",
      "size_bytes": 6488330240,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.858642",
      "end_time": "2026-01-21T17:10:21.858649",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EvaByte/EvaByte-SFT": {
      "model_id": "EvaByte/EvaByte-SFT",
      "model_name": "EvaByte/EvaByte-SFT",
      "size_bytes": 6488330240,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.877168",
      "end_time": "2026-01-21T17:10:21.877176",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/orca_mini_v3_7b": {
      "model_id": "pankajmathur/orca_mini_v3_7b",
      "model_name": "pankajmathur/orca_mini_v3_7b",
      "size_bytes": 6738415616,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.895848",
      "end_time": "2026-01-21T17:10:21.895858",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yentinglin/Taiwan-LLM-7B-v2.0.1-chat": {
      "model_id": "yentinglin/Taiwan-LLM-7B-v2.0.1-chat",
      "model_name": "yentinglin/Taiwan-LLM-7B-v2.0.1-chat",
      "size_bytes": 6738415616,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.914309",
      "end_time": "2026-01-21T17:10:21.914316",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yentinglin/Taiwan-LLM-7B-v2.1-chat": {
      "model_id": "yentinglin/Taiwan-LLM-7B-v2.1-chat",
      "model_name": "yentinglin/Taiwan-LLM-7B-v2.1-chat",
      "size_bytes": 6738415616,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.932796",
      "end_time": "2026-01-21T17:10:21.932802",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KBlueLeaf/guanaco-7B-leh": {
      "model_id": "KBlueLeaf/guanaco-7B-leh",
      "model_name": "KBlueLeaf/guanaco-7B-leh",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.951375",
      "end_time": "2026-01-21T17:10:21.951382",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KBlueLeaf/guanaco-7b-leh-v2": {
      "model_id": "KBlueLeaf/guanaco-7b-leh-v2",
      "model_name": "KBlueLeaf/guanaco-7b-leh-v2",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.970029",
      "end_time": "2026-01-21T17:10:21.970036",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "georgesung/open_llama_7b_qlora_uncensored": {
      "model_id": "georgesung/open_llama_7b_qlora_uncensored",
      "model_name": "georgesung/open_llama_7b_qlora_uncensored",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:21.988774",
      "end_time": "2026-01-21T17:10:21.988781",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lmsys/longchat-7b-v1.5-32k": {
      "model_id": "lmsys/longchat-7b-v1.5-32k",
      "model_name": "lmsys/longchat-7b-v1.5-32k",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.007392",
      "end_time": "2026-01-21T17:10:22.007398",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/orca_mini_v2_7b": {
      "model_id": "pankajmathur/orca_mini_v2_7b",
      "model_name": "pankajmathur/orca_mini_v2_7b",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.026010",
      "end_time": "2026-01-21T17:10:22.026016",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/youri-7b": {
      "model_id": "rinna/youri-7b",
      "model_name": "rinna/youri-7b",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.044769",
      "end_time": "2026-01-21T17:10:22.044775",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "projecte-aina/aguila-7b": {
      "model_id": "projecte-aina/aguila-7b",
      "model_name": "projecte-aina/aguila-7b",
      "size_bytes": 6854619456,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.063585",
      "end_time": "2026-01-21T17:10:22.063592",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NX-AI/xLSTM-7b": {
      "model_id": "NX-AI/xLSTM-7b",
      "model_name": "NX-AI/xLSTM-7b",
      "size_bytes": 6865424896,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.082305",
      "end_time": "2026-01-21T17:10:22.082312",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/maira-2": {
      "model_id": "microsoft/maira-2",
      "model_name": "microsoft/maira-2",
      "size_bytes": 6880185600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.101091",
      "end_time": "2026-01-21T17:10:22.101098",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "onnx-community/gemma-3-270m-it-ONNX": {
      "model_id": "onnx-community/gemma-3-270m-it-ONNX",
      "model_name": "onnx-community/gemma-3-270m-it-ONNX",
      "size_bytes": 6899401814,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.119907",
      "end_time": "2026-01-21T17:10:22.119912",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EleutherAI/pythia-6.9b": {
      "model_id": "EleutherAI/pythia-6.9b",
      "model_name": "EleutherAI/pythia-6.9b",
      "size_bytes": 6991520256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.138991",
      "end_time": "2026-01-21T17:10:22.138998",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "common-pile/comma-v0.1-1t": {
      "model_id": "common-pile/comma-v0.1-1t",
      "model_name": "common-pile/comma-v0.1-1t",
      "size_bytes": 7002656768,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.157981",
      "end_time": "2026-01-21T17:10:22.157987",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "common-pile/comma-v0.1-2t": {
      "model_id": "common-pile/comma-v0.1-2t",
      "model_name": "common-pile/comma-v0.1-2t",
      "size_bytes": 7002656768,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.176964",
      "end_time": "2026-01-21T17:10:22.176970",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/AReaL-1.5B-Preview-Stage-2": {
      "model_id": "inclusionAI/AReaL-1.5B-Preview-Stage-2",
      "model_name": "inclusionAI/AReaL-1.5B-Preview-Stage-2",
      "size_bytes": 7108525990,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.196152",
      "end_time": "2026-01-21T17:10:22.196158",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/AReaL-1.5B-Preview-Stage-3": {
      "model_id": "inclusionAI/AReaL-1.5B-Preview-Stage-3",
      "model_name": "inclusionAI/AReaL-1.5B-Preview-Stage-3",
      "size_bytes": 7108525990,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.215184",
      "end_time": "2026-01-21T17:10:22.215190",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Aleph-Alpha/llama-3_1-8b-tfree-hat-base": {
      "model_id": "Aleph-Alpha/llama-3_1-8b-tfree-hat-base",
      "model_name": "Aleph-Alpha/llama-3_1-8b-tfree-hat-base",
      "size_bytes": 7192495104,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.234167",
      "end_time": "2026-01-21T17:10:22.234174",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "onnx-community/Qwen3-0.6B-ONNX": {
      "model_id": "onnx-community/Qwen3-0.6B-ONNX",
      "model_name": "onnx-community/Qwen3-0.6B-ONNX",
      "size_bytes": 7224088967,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.253187",
      "end_time": "2026-01-21T17:10:22.253194",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "augmxnt/shisa-gamma-7b-v1": {
      "model_id": "augmxnt/shisa-gamma-7b-v1",
      "model_name": "augmxnt/shisa-gamma-7b-v1",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.272426",
      "end_time": "2026-01-21T17:10:22.272433",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-Instruct-v0.1": {
      "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
      "model_name": "mistralai/Mistral-7B-Instruct-v0.1",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.292191",
      "end_time": "2026-01-21T17:10:22.292198",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-v0.1": {
      "model_id": "mistralai/Mistral-7B-v0.1",
      "model_name": "mistralai/Mistral-7B-v0.1",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.311281",
      "end_time": "2026-01-21T17:10:22.311287",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CalderaAI/Naberius-7B": {
      "model_id": "CalderaAI/Naberius-7B",
      "model_name": "CalderaAI/Naberius-7B",
      "size_bytes": 7241748480,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.330535",
      "end_time": "2026-01-21T17:10:22.330540",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FPHam/Writing_Partner_Mistral_7B": {
      "model_id": "FPHam/Writing_Partner_Mistral_7B",
      "model_name": "FPHam/Writing_Partner_Mistral_7B",
      "size_bytes": 7241748480,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.349974",
      "end_time": "2026-01-21T17:10:22.349983",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-Instruct-v0.3": {
      "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
      "model_name": "mistralai/Mistral-7B-Instruct-v0.3",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.369313",
      "end_time": "2026-01-21T17:10:22.369320",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-v0.3": {
      "model_id": "mistralai/Mistral-7B-v0.3",
      "model_name": "mistralai/Mistral-7B-v0.3",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.477205",
      "end_time": "2026-01-21T17:10:22.477214",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mamba-Codestral-7B-v0.1": {
      "model_id": "mistralai/Mamba-Codestral-7B-v0.1",
      "model_name": "mistralai/Mamba-Codestral-7B-v0.1",
      "size_bytes": 7285403648,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.496888",
      "end_time": "2026-01-21T17:10:22.496894",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Olmo-3-1025-7B": {
      "model_id": "allenai/Olmo-3-1025-7B",
      "model_name": "allenai/Olmo-3-1025-7B",
      "size_bytes": 7298011136,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.516383",
      "end_time": "2026-01-21T17:10:22.516390",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-1124-7B": {
      "model_id": "allenai/OLMo-2-1124-7B",
      "model_name": "allenai/OLMo-2-1124-7B",
      "size_bytes": 7298617344,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.536125",
      "end_time": "2026-01-21T17:10:22.536131",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kernels-community/vllm-flash-attn3": {
      "model_id": "kernels-community/vllm-flash-attn3",
      "model_name": "kernels-community/vllm-flash-attn3",
      "size_bytes": 7319919000,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.555698",
      "end_time": "2026-01-21T17:10:22.555705",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/Zamba2-7B": {
      "model_id": "Zyphra/Zamba2-7B",
      "model_name": "Zyphra/Zamba2-7B",
      "size_bytes": 7356749648,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.575361",
      "end_time": "2026-01-21T17:10:22.575367",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/LLaDA-MoE-7B-A1B-Instruct": {
      "model_id": "inclusionAI/LLaDA-MoE-7B-A1B-Instruct",
      "model_name": "inclusionAI/LLaDA-MoE-7B-A1B-Instruct",
      "size_bytes": 7356880896,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.595035",
      "end_time": "2026-01-21T17:10:22.595042",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Rakuten/RakutenAI-7B-chat": {
      "model_id": "Rakuten/RakutenAI-7B-chat",
      "model_name": "Rakuten/RakutenAI-7B-chat",
      "size_bytes": 7372804096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.614566",
      "end_time": "2026-01-21T17:10:22.614573",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Fastweb/FastwebMIIA-7B": {
      "model_id": "Fastweb/FastwebMIIA-7B",
      "model_name": "Fastweb/FastwebMIIA-7B",
      "size_bytes": 7391399936,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.634227",
      "end_time": "2026-01-21T17:10:22.634233",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "aisingapore/SEA-LION-v1-7B": {
      "model_id": "aisingapore/SEA-LION-v1-7B",
      "model_name": "aisingapore/SEA-LION-v1-7B",
      "size_bytes": 7501651968,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.654039",
      "end_time": "2026-01-21T17:10:22.654047",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MadeAgents/Hammer2.1-7b": {
      "model_id": "MadeAgents/Hammer2.1-7b",
      "model_name": "MadeAgents/Hammer2.1-7b",
      "size_bytes": 7612756480,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.674027",
      "end_time": "2026-01-21T17:10:22.674037",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nicoboss/DeepSeek-R1-Distill-Qwen-7B-Uncensored": {
      "model_id": "nicoboss/DeepSeek-R1-Distill-Qwen-7B-Uncensored",
      "model_name": "nicoboss/DeepSeek-R1-Distill-Qwen-7B-Uncensored",
      "size_bytes": 7612756480,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.693723",
      "end_time": "2026-01-21T17:10:22.693731",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AgentFlow/agentflow-planner-7b": {
      "model_id": "AgentFlow/agentflow-planner-7b",
      "model_name": "AgentFlow/agentflow-planner-7b",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.713410",
      "end_time": "2026-01-21T17:10:22.713418",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MetaStoneTec/MetaStone-L1-7B": {
      "model_id": "MetaStoneTec/MetaStone-L1-7B",
      "model_name": "MetaStoneTec/MetaStone-L1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.733249",
      "end_time": "2026-01-21T17:10:22.733256",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenHands/openhands-lm-7b-v0.1": {
      "model_id": "OpenHands/openhands-lm-7b-v0.1",
      "model_name": "OpenHands/openhands-lm-7b-v0.1",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.753205",
      "end_time": "2026-01-21T17:10:22.753212",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "POLARIS-Project/Polaris-7B-Preview": {
      "model_id": "POLARIS-Project/Polaris-7B-Preview",
      "model_name": "POLARIS-Project/Polaris-7B-Preview",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.773017",
      "end_time": "2026-01-21T17:10:22.773023",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SUFE-AIFLM-Lab/Fin-R1": {
      "model_id": "SUFE-AIFLM-Lab/Fin-R1",
      "model_name": "SUFE-AIFLM-Lab/Fin-R1",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.792779",
      "end_time": "2026-01-21T17:10:22.792785",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Snowflake/Arctic-Text2SQL-R1-7B": {
      "model_id": "Snowflake/Arctic-Text2SQL-R1-7B",
      "model_name": "Snowflake/Arctic-Text2SQL-R1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.812647",
      "end_time": "2026-01-21T17:10:22.812653",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/DiffuCoder-7B-Base": {
      "model_id": "apple/DiffuCoder-7B-Base",
      "model_name": "apple/DiffuCoder-7B-Base",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.832573",
      "end_time": "2026-01-21T17:10:22.832579",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/DiffuCoder-7B-Instruct": {
      "model_id": "apple/DiffuCoder-7B-Instruct",
      "model_name": "apple/DiffuCoder-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.852768",
      "end_time": "2026-01-21T17:10:22.852775",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/DiffuCoder-7B-cpGRPO": {
      "model_id": "apple/DiffuCoder-7B-cpGRPO",
      "model_name": "apple/DiffuCoder-7B-cpGRPO",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.872808",
      "end_time": "2026-01-21T17:10:22.872814",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lightblue/DeepSeek-R1-Distill-Qwen-7B-Multilingual": {
      "model_id": "lightblue/DeepSeek-R1-Distill-Qwen-7B-Multilingual",
      "model_name": "lightblue/DeepSeek-R1-Distill-Qwen-7B-Multilingual",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.892900",
      "end_time": "2026-01-21T17:10:22.892907",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "virtuoussy/Qwen2.5-7B-Instruct-RLVR": {
      "model_id": "virtuoussy/Qwen2.5-7B-Instruct-RLVR",
      "model_name": "virtuoussy/Qwen2.5-7B-Instruct-RLVR",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.913047",
      "end_time": "2026-01-21T17:10:22.913053",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zed-industries/zeta": {
      "model_id": "zed-industries/zeta",
      "model_name": "zed-industries/zeta",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.933059",
      "end_time": "2026-01-21T17:10:22.933065",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tongyi-Zhiwen/QwenLong-CPRS-7B": {
      "model_id": "Tongyi-Zhiwen/QwenLong-CPRS-7B",
      "model_name": "Tongyi-Zhiwen/QwenLong-CPRS-7B",
      "size_bytes": 7615630852,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.953357",
      "end_time": "2026-01-21T17:10:22.953363",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview": {
      "model_id": "yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview",
      "model_name": "yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview",
      "size_bytes": 7660481024,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.973474",
      "end_time": "2026-01-21T17:10:22.973481",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "augmxnt/shisa-7b-v1": {
      "model_id": "augmxnt/shisa-7b-v1",
      "model_name": "augmxnt/shisa-7b-v1",
      "size_bytes": 7963676672,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:22.993765",
      "end_time": "2026-01-21T17:10:22.993771",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Ministral-8B-Instruct-2410": {
      "model_id": "mistralai/Ministral-8B-Instruct-2410",
      "model_name": "mistralai/Ministral-8B-Instruct-2410",
      "size_bytes": 8019808256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.014082",
      "end_time": "2026-01-21T17:10:23.014088",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mann-e/Hormoz-8B": {
      "model_id": "mann-e/Hormoz-8B",
      "model_name": "mann-e/Hormoz-8B",
      "size_bytes": 8028033024,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.034433",
      "end_time": "2026-01-21T17:10:23.034439",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ContactDoctor/Bio-Medical-Llama-3-8B": {
      "model_id": "ContactDoctor/Bio-Medical-Llama-3-8B",
      "model_name": "ContactDoctor/Bio-Medical-Llama-3-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.054659",
      "end_time": "2026-01-21T17:10:23.054665",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ContactDoctor/Bio-Medical-Llama-3-8B-CoT-012025": {
      "model_id": "ContactDoctor/Bio-Medical-Llama-3-8B-CoT-012025",
      "model_name": "ContactDoctor/Bio-Medical-Llama-3-8B-CoT-012025",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.074889",
      "end_time": "2026-01-21T17:10:23.074895",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Wingless_Imp_8B": {
      "model_id": "SicariusSicariiStuff/Wingless_Imp_8B",
      "model_name": "SicariusSicariiStuff/Wingless_Imp_8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.095379",
      "end_time": "2026-01-21T17:10:23.095385",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Team-ACE/ToolACE-2-Llama-3.1-8B": {
      "model_id": "Team-ACE/ToolACE-2-Llama-3.1-8B",
      "model_name": "Team-ACE/ToolACE-2-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.115630",
      "end_time": "2026-01-21T17:10:23.115636",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "collinzrj/DeepSeek-R1-Distill-Llama-8B-abliterate": {
      "model_id": "collinzrj/DeepSeek-R1-Distill-Llama-8B-abliterate",
      "model_name": "collinzrj/DeepSeek-R1-Distill-Llama-8B-abliterate",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.135924",
      "end_time": "2026-01-21T17:10:23.135930",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lolzinventor/Meta-Llama-3.1-8B-SurviveV3": {
      "model_id": "lolzinventor/Meta-Llama-3.1-8B-SurviveV3",
      "model_name": "lolzinventor/Meta-Llama-3.1-8B-SurviveV3",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.156394",
      "end_time": "2026-01-21T17:10:23.156400",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Dusk_Rainbow": {
      "model_id": "SicariusSicariiStuff/Dusk_Rainbow",
      "model_name": "SicariusSicariiStuff/Dusk_Rainbow",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.176830",
      "end_time": "2026-01-21T17:10:23.176835",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA": {
      "model_id": "SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA",
      "model_name": "SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.197314",
      "end_time": "2026-01-21T17:10:23.197321",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dphn/Dolphin3.0-Llama3.1-8B": {
      "model_id": "dphn/Dolphin3.0-Llama3.1-8B",
      "model_name": "dphn/Dolphin3.0-Llama3.1-8B",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.217696",
      "end_time": "2026-01-21T17:10:23.217702",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "jiviai/medX_v2": {
      "model_id": "jiviai/medX_v2",
      "model_name": "jiviai/medX_v2",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.238095",
      "end_time": "2026-01-21T17:10:23.238102",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Impish_Mind_8B": {
      "model_id": "SicariusSicariiStuff/Impish_Mind_8B",
      "model_name": "SicariusSicariiStuff/Impish_Mind_8B",
      "size_bytes": 8030294016,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.258737",
      "end_time": "2026-01-21T17:10:23.258743",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "IlyaGusev/saiga_yandexgpt_8b": {
      "model_id": "IlyaGusev/saiga_yandexgpt_8b",
      "model_name": "IlyaGusev/saiga_yandexgpt_8b",
      "size_bytes": 8036552704,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.279307",
      "end_time": "2026-01-21T17:10:23.279313",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenMEDLab/PULSE-7bv5": {
      "model_id": "OpenMEDLab/PULSE-7bv5",
      "model_name": "OpenMEDLab/PULSE-7bv5",
      "size_bytes": 8096620544,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.299879",
      "end_time": "2026-01-21T17:10:23.299885",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1": {
      "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
      "model_name": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.320356",
      "end_time": "2026-01-21T17:10:23.320361",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Kwai-Klear/Klear-Reasoner-8B": {
      "model_id": "Kwai-Klear/Klear-Reasoner-8B",
      "model_name": "Kwai-Klear/Klear-Reasoner-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.341123",
      "end_time": "2026-01-21T17:10:23.341130",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "gr0010/CustomThinker-0-8B": {
      "model_id": "gr0010/CustomThinker-0-8B",
      "model_name": "gr0010/CustomThinker-0-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.361991",
      "end_time": "2026-01-21T17:10:23.362001",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "miromind-ai/MiroThinker-v1.0-8B": {
      "model_id": "miromind-ai/MiroThinker-v1.0-8B",
      "model_name": "miromind-ai/MiroThinker-v1.0-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.382919",
      "end_time": "2026-01-21T17:10:23.382926",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-Orchestrator-8B": {
      "model_id": "nvidia/Nemotron-Orchestrator-8B",
      "model_name": "nvidia/Nemotron-Orchestrator-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.404025",
      "end_time": "2026-01-21T17:10:23.404032",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "open-thoughts/OpenThinker-Agent-v1": {
      "model_id": "open-thoughts/OpenThinker-Agent-v1",
      "model_name": "open-thoughts/OpenThinker-Agent-v1",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.424980",
      "end_time": "2026-01-21T17:10:23.424988",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rl-research/DR-Tulu-8B": {
      "model_id": "rl-research/DR-Tulu-8B",
      "model_name": "rl-research/DR-Tulu-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.445793",
      "end_time": "2026-01-21T17:10:23.445799",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "stepfun-ai/PaCoRe-8B": {
      "model_id": "stepfun-ai/PaCoRe-8B",
      "model_name": "stepfun-ai/PaCoRe-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.466704",
      "end_time": "2026-01-21T17:10:23.466711",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen2-1B_P": {
      "model_id": "Salesforce/codegen2-1B_P",
      "model_name": "Salesforce/codegen2-1B_P",
      "size_bytes": 8256730505,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.487679",
      "end_time": "2026-01-21T17:10:23.487686",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inceptionai/Llama-3.1-Sherkala-8B-Chat": {
      "model_id": "inceptionai/Llama-3.1-Sherkala-8B-Chat",
      "model_name": "inceptionai/Llama-3.1-Sherkala-8B-Chat",
      "size_bytes": 8288391168,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.508617",
      "end_time": "2026-01-21T17:10:23.508623",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EssentialAI/rnj-1": {
      "model_id": "EssentialAI/rnj-1",
      "model_name": "EssentialAI/rnj-1",
      "size_bytes": 8310501376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.529692",
      "end_time": "2026-01-21T17:10:23.529698",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "EssentialAI/rnj-1-instruct": {
      "model_id": "EssentialAI/rnj-1-instruct",
      "model_name": "EssentialAI/rnj-1-instruct",
      "size_bytes": 8310501376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.550575",
      "end_time": "2026-01-21T17:10:23.550582",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-mini-instruct-onnx": {
      "model_id": "microsoft/Phi-4-mini-instruct-onnx",
      "model_name": "microsoft/Phi-4-mini-instruct-onnx",
      "size_bytes": 8341619080,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.571574",
      "end_time": "2026-01-21T17:10:23.571580",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Xenova/donut-base-finetuned-docvqa": {
      "model_id": "Xenova/donut-base-finetuned-docvqa",
      "model_name": "Xenova/donut-base-finetuned-docvqa",
      "size_bytes": 8775040820,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.592880",
      "end_time": "2026-01-21T17:10:23.592887",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/ZAYA1-base": {
      "model_id": "Zyphra/ZAYA1-base",
      "model_name": "Zyphra/ZAYA1-base",
      "size_bytes": 8840489464,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.614044",
      "end_time": "2026-01-21T17:10:23.614051",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Xenova/Phi-3-mini-4k-instruct": {
      "model_id": "Xenova/Phi-3-mini-4k-instruct",
      "model_name": "Xenova/Phi-3-mini-4k-instruct",
      "size_bytes": 8984399884,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.635323",
      "end_time": "2026-01-21T17:10:23.635330",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tower-Babel/Babel-9B": {
      "model_id": "Tower-Babel/Babel-9B",
      "model_name": "Tower-Babel/Babel-9B",
      "size_bytes": 9013963264,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.656552",
      "end_time": "2026-01-21T17:10:23.656558",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tower-Babel/Babel-9B-Chat": {
      "model_id": "Tower-Babel/Babel-9B-Chat",
      "model_name": "Tower-Babel/Babel-9B-Chat",
      "size_bytes": 9013963264,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.678135",
      "end_time": "2026-01-21T17:10:23.678148",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pfnet/plamo-2-8b": {
      "model_id": "pfnet/plamo-2-8b",
      "model_name": "pfnet/plamo-2-8b",
      "size_bytes": 9118372864,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.699388",
      "end_time": "2026-01-21T17:10:23.699395",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "RekaAI/reka-flash-3.1-rekaquant-q3_k_s": {
      "model_id": "RekaAI/reka-flash-3.1-rekaquant-q3_k_s",
      "model_name": "RekaAI/reka-flash-3.1-rekaquant-q3_k_s",
      "size_bytes": 9251489216,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.720587",
      "end_time": "2026-01-21T17:10:23.720595",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "heack/HeackMT5-ZhSum100k": {
      "model_id": "heack/HeackMT5-ZhSum100k",
      "model_name": "heack/HeackMT5-ZhSum100k",
      "size_bytes": 9322933033,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.741934",
      "end_time": "2026-01-21T17:10:23.741942",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/glm-4-9b-chat": {
      "model_id": "zai-org/glm-4-9b-chat",
      "model_name": "zai-org/glm-4-9b-chat",
      "size_bytes": 9399951392,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.763467",
      "end_time": "2026-01-21T17:10:23.763475",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/glm-4-9b-chat-1m": {
      "model_id": "zai-org/glm-4-9b-chat-1m",
      "model_name": "zai-org/glm-4-9b-chat-1m",
      "size_bytes": 9483857952,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.784938",
      "end_time": "2026-01-21T17:10:23.784945",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "humarin/chatgpt_paraphraser_on_T5_base": {
      "model_id": "humarin/chatgpt_paraphraser_on_T5_base",
      "model_name": "humarin/chatgpt_paraphraser_on_T5_base",
      "size_bytes": 9809172969,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.806305",
      "end_time": "2026-01-21T17:10:23.806312",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai-sage/GigaChat3-10B-A1.8B": {
      "model_id": "ai-sage/GigaChat3-10B-A1.8B",
      "model_name": "ai-sage/GigaChat3-10B-A1.8B",
      "size_bytes": 11480401468,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.827610",
      "end_time": "2026-01-21T17:10:23.827616",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Oni_Mitsubishi_12B": {
      "model_id": "SicariusSicariiStuff/Oni_Mitsubishi_12B",
      "model_name": "SicariusSicariiStuff/Oni_Mitsubishi_12B",
      "size_bytes": 11766034176,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.849152",
      "end_time": "2026-01-21T17:10:23.849160",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Phi-lthy4": {
      "model_id": "SicariusSicariiStuff/Phi-lthy4",
      "model_name": "SicariusSicariiStuff/Phi-lthy4",
      "size_bytes": 11933127680,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.870615",
      "end_time": "2026-01-21T17:10:23.870622",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Fallen-Gemma3-12B-v1": {
      "model_id": "TheDrummer/Fallen-Gemma3-12B-v1",
      "model_name": "TheDrummer/Fallen-Gemma3-12B-v1",
      "size_bytes": 12187325040,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.892110",
      "end_time": "2026-01-21T17:10:23.892117",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Delta-Vector/Rei-V3-KTO-12B": {
      "model_id": "Delta-Vector/Rei-V3-KTO-12B",
      "model_name": "Delta-Vector/Rei-V3-KTO-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.913584",
      "end_time": "2026-01-21T17:10:23.913590",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "IlyaGusev/saiga_nemo_12b": {
      "model_id": "IlyaGusev/saiga_nemo_12b",
      "model_name": "IlyaGusev/saiga_nemo_12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.935238",
      "end_time": "2026-01-21T17:10:23.935245",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/CaptainErisNebula-12B-Chimera-v1.1": {
      "model_id": "Nitral-AI/CaptainErisNebula-12B-Chimera-v1.1",
      "model_name": "Nitral-AI/CaptainErisNebula-12B-Chimera-v1.1",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.956836",
      "end_time": "2026-01-21T17:10:23.956843",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/Violet_Magcap-12B": {
      "model_id": "Nitral-AI/Violet_Magcap-12B",
      "model_name": "Nitral-AI/Violet_Magcap-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:23.978327",
      "end_time": "2026-01-21T17:10:23.978334",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PocketDoc/Dans-SakuraKaze-V1.0.0-12b": {
      "model_id": "PocketDoc/Dans-SakuraKaze-V1.0.0-12b",
      "model_name": "PocketDoc/Dans-SakuraKaze-V1.0.0-12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.000060",
      "end_time": "2026-01-21T17:10:24.000068",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Sweet_Dreams_12B": {
      "model_id": "SicariusSicariiStuff/Sweet_Dreams_12B",
      "model_name": "SicariusSicariiStuff/Sweet_Dreams_12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.021953",
      "end_time": "2026-01-21T17:10:24.021960",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Rivermind-12B-v1": {
      "model_id": "TheDrummer/Rivermind-12B-v1",
      "model_name": "TheDrummer/Rivermind-12B-v1",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.043718",
      "end_time": "2026-01-21T17:10:24.043725",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Rivermind-Lux-12B-v1": {
      "model_id": "TheDrummer/Rivermind-Lux-12B-v1",
      "model_name": "TheDrummer/Rivermind-Lux-12B-v1",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.065239",
      "end_time": "2026-01-21T17:10:24.065246",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Nemo-Base-2407": {
      "model_id": "mistralai/Mistral-Nemo-Base-2407",
      "model_name": "mistralai/Mistral-Nemo-Base-2407",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.086908",
      "end_time": "2026-01-21T17:10:24.086915",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cgato/Nemo-12b-Humanize-KTO-Experimental-Latest": {
      "model_id": "cgato/Nemo-12b-Humanize-KTO-Experimental-Latest",
      "model_name": "cgato/Nemo-12b-Humanize-KTO-Experimental-Latest",
      "size_bytes": 12247823360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.108638",
      "end_time": "2026-01-21T17:10:24.108645",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cgato/Nemo-12b-Humanize-KTO-v0.1": {
      "model_id": "cgato/Nemo-12b-Humanize-KTO-v0.1",
      "model_name": "cgato/Nemo-12b-Humanize-KTO-v0.1",
      "size_bytes": 12247823360,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.132978",
      "end_time": "2026-01-21T17:10:24.132988",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Motif-Technologies/Motif-2-12.7B-Reasoning": {
      "model_id": "Motif-Technologies/Motif-2-12.7B-Reasoning",
      "model_name": "Motif-Technologies/Motif-2-12.7B-Reasoning",
      "size_bytes": 12703860896,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.154756",
      "end_time": "2026-01-21T17:10:24.154763",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/aya-101": {
      "model_id": "CohereLabs/aya-101",
      "model_name": "CohereLabs/aya-101",
      "size_bytes": 12921057280,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.176565",
      "end_time": "2026-01-21T17:10:24.176572",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FPHam/Free_Sydney_V2_13b_HF": {
      "model_id": "FPHam/Free_Sydney_V2_13b_HF",
      "model_name": "FPHam/Free_Sydney_V2_13b_HF",
      "size_bytes": 13015864320,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.198653",
      "end_time": "2026-01-21T17:10:24.198660",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Pclanglais/Brahe": {
      "model_id": "Pclanglais/Brahe",
      "model_name": "Pclanglais/Brahe",
      "size_bytes": 13015864320,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.220654",
      "end_time": "2026-01-21T17:10:24.220661",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/orca_mini_v3_13b": {
      "model_id": "pankajmathur/orca_mini_v3_13b",
      "model_name": "pankajmathur/orca_mini_v3_13b",
      "size_bytes": 13015864320,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.242392",
      "end_time": "2026-01-21T17:10:24.242399",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yentinglin/Taiwan-LLM-13B-v2.0-chat": {
      "model_id": "yentinglin/Taiwan-LLM-13B-v2.0-chat",
      "model_name": "yentinglin/Taiwan-LLM-13B-v2.0-chat",
      "size_bytes": 13015864320,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.264511",
      "end_time": "2026-01-21T17:10:24.264518",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/orca_mini_v2_13b": {
      "model_id": "pankajmathur/orca_mini_v2_13b",
      "model_name": "pankajmathur/orca_mini_v2_13b",
      "size_bytes": 13015866880,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.286637",
      "end_time": "2026-01-21T17:10:24.286644",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CalderaAI/13B-Thorns-l2": {
      "model_id": "CalderaAI/13B-Thorns-l2",
      "model_name": "CalderaAI/13B-Thorns-l2",
      "size_bytes": 13016194560,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.308624",
      "end_time": "2026-01-21T17:10:24.308631",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "minlik/chinese-alpaca-13b-merged": {
      "model_id": "minlik/chinese-alpaca-13b-merged",
      "model_name": "minlik/chinese-alpaca-13b-merged",
      "size_bytes": 13199715840,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.330831",
      "end_time": "2026-01-21T17:10:24.330837",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "polyglots/SinLlama_v01": {
      "model_id": "polyglots/SinLlama_v01",
      "model_name": "polyglots/SinLlama_v01",
      "size_bytes": 13448495535,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.353090",
      "end_time": "2026-01-21T17:10:24.353100",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "snwy/frankenqwen3-8B-235B-dense-conversion-interleaved-untuned": {
      "model_id": "snwy/frankenqwen3-8B-235B-dense-conversion-interleaved-untuned",
      "model_name": "snwy/frankenqwen3-8B-235B-dense-conversion-interleaved-untuned",
      "size_bytes": 13593235456,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.375392",
      "end_time": "2026-01-21T17:10:24.375402",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-1124-13B": {
      "model_id": "allenai/OLMo-2-1124-13B",
      "model_name": "allenai/OLMo-2-1124-13B",
      "size_bytes": 13716198400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.397516",
      "end_time": "2026-01-21T17:10:24.397522",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "uncensoredai/UncensoredLM-DeepSeek-R1-Distill-Qwen-14B": {
      "model_id": "uncensoredai/UncensoredLM-DeepSeek-R1-Distill-Qwen-14B",
      "model_name": "uncensoredai/UncensoredLM-DeepSeek-R1-Distill-Qwen-14B",
      "size_bytes": 14219496448,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.419758",
      "end_time": "2026-01-21T17:10:24.419765",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AI4PD/ZymCTRL": {
      "model_id": "AI4PD/ZymCTRL",
      "model_name": "AI4PD/ZymCTRL",
      "size_bytes": 14394866301,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.441958",
      "end_time": "2026-01-21T17:10:24.441965",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NyxKrage/Microsoft_Phi-4": {
      "model_id": "NyxKrage/Microsoft_Phi-4",
      "model_name": "NyxKrage/Microsoft_Phi-4",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.464386",
      "end_time": "2026-01-21T17:10:24.464393",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Phi-Line_14B": {
      "model_id": "SicariusSicariiStuff/Phi-Line_14B",
      "model_name": "SicariusSicariiStuff/Phi-Line_14B",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.486717",
      "end_time": "2026-01-21T17:10:24.486724",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nicoboss/DeepSeek-R1-Distill-Qwen-14B-Uncensored": {
      "model_id": "nicoboss/DeepSeek-R1-Distill-Qwen-14B-Uncensored",
      "model_name": "nicoboss/DeepSeek-R1-Distill-Qwen-14B-Uncensored",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.508891",
      "end_time": "2026-01-21T17:10:24.508898",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "manifestai/Brumby-14B-Base": {
      "model_id": "manifestai/Brumby-14B-Base",
      "model_name": "manifestai/Brumby-14B-Base",
      "size_bytes": 14769945600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.531238",
      "end_time": "2026-01-21T17:10:24.531245",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AetherArchitectural/lwd-Mirau-RP-14B": {
      "model_id": "AetherArchitectural/lwd-Mirau-RP-14B",
      "model_name": "AetherArchitectural/lwd-Mirau-RP-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.553622",
      "end_time": "2026-01-21T17:10:24.553630",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BytedTsinghua-SIA/RL-MemoryAgent-14B": {
      "model_id": "BytedTsinghua-SIA/RL-MemoryAgent-14B",
      "model_name": "BytedTsinghua-SIA/RL-MemoryAgent-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.575907",
      "end_time": "2026-01-21T17:10:24.575913",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Orion-zhen/Qwen2.5-14B-Instruct-Uncensored": {
      "model_id": "Orion-zhen/Qwen2.5-14B-Instruct-Uncensored",
      "model_name": "Orion-zhen/Qwen2.5-14B-Instruct-Uncensored",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.598441",
      "end_time": "2026-01-21T17:10:24.598448",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Impish_QWEN_14B-1M": {
      "model_id": "SicariusSicariiStuff/Impish_QWEN_14B-1M",
      "model_name": "SicariusSicariiStuff/Impish_QWEN_14B-1M",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.620970",
      "end_time": "2026-01-21T17:10:24.620977",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Virtuoso-Small": {
      "model_id": "arcee-ai/Virtuoso-Small",
      "model_name": "arcee-ai/Virtuoso-Small",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.643513",
      "end_time": "2026-01-21T17:10:24.643519",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "clouditera/secgpt": {
      "model_id": "clouditera/secgpt",
      "model_name": "clouditera/secgpt",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.666232",
      "end_time": "2026-01-21T17:10:24.666239",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Snowpiercer-15B-v1": {
      "model_id": "TheDrummer/Snowpiercer-15B-v1",
      "model_name": "TheDrummer/Snowpiercer-15B-v1",
      "size_bytes": 14974182400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.688959",
      "end_time": "2026-01-21T17:10:24.688969",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Snowpiercer-15B-v3": {
      "model_id": "TheDrummer/Snowpiercer-15B-v3",
      "model_name": "TheDrummer/Snowpiercer-15B-v3",
      "size_bytes": 14974182400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.711857",
      "end_time": "2026-01-21T17:10:24.711864",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CausalLM/7B-DPO-alpha": {
      "model_id": "CausalLM/7B-DPO-alpha",
      "model_name": "CausalLM/7B-DPO-alpha",
      "size_bytes": 15441963229,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.734778",
      "end_time": "2026-01-21T17:10:24.734785",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CausalLM/7B": {
      "model_id": "CausalLM/7B",
      "model_name": "CausalLM/7B",
      "size_bytes": 15441967325,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.757768",
      "end_time": "2026-01-21T17:10:24.757777",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Qwen3-30B-A3B-NVFP4": {
      "model_id": "nvidia/Qwen3-30B-A3B-NVFP4",
      "model_name": "nvidia/Qwen3-30B-A3B-NVFP4",
      "size_bytes": 15583623168,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.780308",
      "end_time": "2026-01-21T17:10:24.780314",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ServiceNow-AI/Apriel-H1-15b-Thinker-SFT": {
      "model_id": "ServiceNow-AI/Apriel-H1-15b-Thinker-SFT",
      "model_name": "ServiceNow-AI/Apriel-H1-15b-Thinker-SFT",
      "size_bytes": 15694627840,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.803056",
      "end_time": "2026-01-21T17:10:24.803063",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kalomaze/Qwen3-16B-A3B": {
      "model_id": "kalomaze/Qwen3-16B-A3B",
      "model_name": "kalomaze/Qwen3-16B-A3B",
      "size_bytes": 16030316544,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.825958",
      "end_time": "2026-01-21T17:10:24.825964",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "saturated-labs/T-Rex-mini": {
      "model_id": "saturated-labs/T-Rex-mini",
      "model_name": "saturated-labs/T-Rex-mini",
      "size_bytes": 16060556336,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.848945",
      "end_time": "2026-01-21T17:10:24.848953",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/LLaDA2.0-mini": {
      "model_id": "inclusionAI/LLaDA2.0-mini",
      "model_name": "inclusionAI/LLaDA2.0-mini",
      "size_bytes": 16255643392,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.871755",
      "end_time": "2026-01-21T17:10:24.871761",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-lite-2506": {
      "model_id": "inclusionAI/Ring-lite-2506",
      "model_name": "inclusionAI/Ring-lite-2506",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.894851",
      "end_time": "2026-01-21T17:10:24.894859",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-lite-2507": {
      "model_id": "inclusionAI/Ring-lite-2507",
      "model_name": "inclusionAI/Ring-lite-2507",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.917780",
      "end_time": "2026-01-21T17:10:24.917786",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-mini-base-2.0": {
      "model_id": "inclusionAI/Ling-mini-base-2.0",
      "model_name": "inclusionAI/Ling-mini-base-2.0",
      "size_bytes": 17083504896,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.940612",
      "end_time": "2026-01-21T17:10:24.940620",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-mini-4k-instruct-onnx": {
      "model_id": "microsoft/Phi-3-mini-4k-instruct-onnx",
      "model_name": "microsoft/Phi-3-mini-4k-instruct-onnx",
      "size_bytes": 17516081261,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.963726",
      "end_time": "2026-01-21T17:10:24.963734",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Chun121/Qwen3-4B-RPG-Roleplay-V2": {
      "model_id": "Chun121/Qwen3-4B-RPG-Roleplay-V2",
      "model_name": "Chun121/Qwen3-4B-RPG-Roleplay-V2",
      "size_bytes": 17994199134,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:24.986724",
      "end_time": "2026-01-21T17:10:24.986731",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ": {
      "model_id": "mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ",
      "model_name": "mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ",
      "size_bytes": 18242353468,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.009897",
      "end_time": "2026-01-21T17:10:25.009902",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "karpathy/nanochat-d32": {
      "model_id": "karpathy/nanochat-d32",
      "model_name": "karpathy/nanochat-d32",
      "size_bytes": 18523384754,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.032897",
      "end_time": "2026-01-21T17:10:25.032904",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "codefuse-ai/CodeFuse-CodeLlama-34B-4bits": {
      "model_id": "codefuse-ai/CodeFuse-CodeLlama-34B-4bits",
      "model_name": "codefuse-ai/CodeFuse-CodeLlama-34B-4bits",
      "size_bytes": 18978840283,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.055901",
      "end_time": "2026-01-21T17:10:25.055909",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "VMware/flan-ul2-alpaca-lora": {
      "model_id": "VMware/flan-ul2-alpaca-lora",
      "model_name": "VMware/flan-ul2-alpaca-lora",
      "size_bytes": 19722806272,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.079118",
      "end_time": "2026-01-21T17:10:25.079125",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/phi-4-onnx": {
      "model_id": "microsoft/phi-4-onnx",
      "model_name": "microsoft/phi-4-onnx",
      "size_bytes": 19904832770,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.102341",
      "end_time": "2026-01-21T17:10:25.102349",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "onnx-community/gemma-3-1b-it-ONNX": {
      "model_id": "onnx-community/gemma-3-1b-it-ONNX",
      "model_name": "onnx-community/gemma-3-1b-it-ONNX",
      "size_bytes": 20533209329,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.125459",
      "end_time": "2026-01-21T17:10:25.125465",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QwQZh/gated_attention": {
      "model_id": "QwQZh/gated_attention",
      "model_name": "QwQZh/gated_attention",
      "size_bytes": 20683483054,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.148597",
      "end_time": "2026-01-21T17:10:25.148604",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "RekaAI/reka-flash-3.1": {
      "model_id": "RekaAI/reka-flash-3.1",
      "model_name": "RekaAI/reka-flash-3.1",
      "size_bytes": 20905482240,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.172009",
      "end_time": "2026-01-21T17:10:25.172017",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ArliAI/gpt-oss-20b-Derestricted": {
      "model_id": "ArliAI/gpt-oss-20b-Derestricted",
      "model_name": "ArliAI/gpt-oss-20b-Derestricted",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.195530",
      "end_time": "2026-01-21T17:10:25.195537",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "p-e-w/gpt-oss-20b-heretic": {
      "model_id": "p-e-w/gpt-oss-20b-heretic",
      "model_name": "p-e-w/gpt-oss-20b-heretic",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.218912",
      "end_time": "2026-01-21T17:10:25.218920",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tiiny/SmallThinker-21BA3B-Instruct": {
      "model_id": "Tiiny/SmallThinker-21BA3B-Instruct",
      "model_name": "Tiiny/SmallThinker-21BA3B-Instruct",
      "size_bytes": 21506562560,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.242173",
      "end_time": "2026-01-21T17:10:25.242181",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-ReduX-22B-v1": {
      "model_id": "TheDrummer/Cydonia-ReduX-22B-v1",
      "model_name": "TheDrummer/Cydonia-ReduX-22B-v1",
      "size_bytes": 22247282688,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.265607",
      "end_time": "2026-01-21T17:10:25.265615",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "concedo/Beepo-22B": {
      "model_id": "concedo/Beepo-22B",
      "model_name": "concedo/Beepo-22B",
      "size_bytes": 22247282688,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.289069",
      "end_time": "2026-01-21T17:10:25.289077",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Codestral-22B-v0.1": {
      "model_id": "mistralai/Codestral-22B-v0.1",
      "model_name": "mistralai/Codestral-22B-v0.1",
      "size_bytes": 22247282688,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.312394",
      "end_time": "2026-01-21T17:10:25.312402",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Small-Instruct-2409": {
      "model_id": "mistralai/Mistral-Small-Instruct-2409",
      "model_name": "mistralai/Mistral-Small-Instruct-2409",
      "size_bytes": 22247282688,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.336043",
      "end_time": "2026-01-21T17:10:25.336050",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-2B-mono": {
      "model_id": "Salesforce/codegen-2B-mono",
      "model_name": "Salesforce/codegen-2B-mono",
      "size_bytes": 22641957696,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.359837",
      "end_time": "2026-01-21T17:10:25.359847",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-2B-multi": {
      "model_id": "Salesforce/codegen-2B-multi",
      "model_name": "Salesforce/codegen-2B-multi",
      "size_bytes": 22641957696,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.383495",
      "end_time": "2026-01-21T17:10:25.383503",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CrucibleLab/M3.2-24B-Loki-V1.3": {
      "model_id": "CrucibleLab/M3.2-24B-Loki-V1.3",
      "model_name": "CrucibleLab/M3.2-24B-Loki-V1.3",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.407056",
      "end_time": "2026-01-21T17:10:25.407063",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Gryphe/Codex-24B-Small-3.2": {
      "model_id": "Gryphe/Codex-24B-Small-3.2",
      "model_name": "Gryphe/Codex-24B-Small-3.2",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.430450",
      "end_time": "2026-01-21T17:10:25.430457",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Gryphe/Pantheon-RP-1.8-24b-Small-3.1": {
      "model_id": "Gryphe/Pantheon-RP-1.8-24b-Small-3.1",
      "model_name": "Gryphe/Pantheon-RP-1.8-24b-Small-3.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.454175",
      "end_time": "2026-01-21T17:10:25.454183",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LatitudeGames/Harbinger-24B": {
      "model_id": "LatitudeGames/Harbinger-24B",
      "model_name": "LatitudeGames/Harbinger-24B",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.477866",
      "end_time": "2026-01-21T17:10:25.477873",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ReadyArt/Broken-Tutu-24B": {
      "model_id": "ReadyArt/Broken-Tutu-24B",
      "model_name": "ReadyArt/Broken-Tutu-24B",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.501584",
      "end_time": "2026-01-21T17:10:25.501592",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.1": {
      "model_id": "ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.1",
      "model_name": "ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.525385",
      "end_time": "2026-01-21T17:10:25.525392",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SlerpE/CardProjector-24B-v1": {
      "model_id": "SlerpE/CardProjector-24B-v1",
      "model_name": "SlerpE/CardProjector-24B-v1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.549056",
      "end_time": "2026-01-21T17:10:25.549063",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v2": {
      "model_id": "TheDrummer/Cydonia-24B-v2",
      "model_name": "TheDrummer/Cydonia-24B-v2",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.573030",
      "end_time": "2026-01-21T17:10:25.573039",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v2.1": {
      "model_id": "TheDrummer/Cydonia-24B-v2.1",
      "model_name": "TheDrummer/Cydonia-24B-v2.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.596943",
      "end_time": "2026-01-21T17:10:25.596952",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v3": {
      "model_id": "TheDrummer/Cydonia-24B-v3",
      "model_name": "TheDrummer/Cydonia-24B-v3",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.621079",
      "end_time": "2026-01-21T17:10:25.621086",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v3.1": {
      "model_id": "TheDrummer/Cydonia-24B-v3.1",
      "model_name": "TheDrummer/Cydonia-24B-v3.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.645205",
      "end_time": "2026-01-21T17:10:25.645212",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v4": {
      "model_id": "TheDrummer/Cydonia-24B-v4",
      "model_name": "TheDrummer/Cydonia-24B-v4",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.669796",
      "end_time": "2026-01-21T17:10:25.669804",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v4.1": {
      "model_id": "TheDrummer/Cydonia-24B-v4.1",
      "model_name": "TheDrummer/Cydonia-24B-v4.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.693968",
      "end_time": "2026-01-21T17:10:25.693978",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-24B-v4.2.0": {
      "model_id": "TheDrummer/Cydonia-24B-v4.2.0",
      "model_name": "TheDrummer/Cydonia-24B-v4.2.0",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.718327",
      "end_time": "2026-01-21T17:10:25.718336",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-R1-24B-v4": {
      "model_id": "TheDrummer/Cydonia-R1-24B-v4",
      "model_name": "TheDrummer/Cydonia-R1-24B-v4",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.742398",
      "end_time": "2026-01-21T17:10:25.742406",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Cydonia-R1-24B-v4.1": {
      "model_id": "TheDrummer/Cydonia-R1-24B-v4.1",
      "model_name": "TheDrummer/Cydonia-R1-24B-v4.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.766580",
      "end_time": "2026-01-21T17:10:25.766587",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Magidonia-24B-v4.2.0": {
      "model_id": "TheDrummer/Magidonia-24B-v4.2.0",
      "model_name": "TheDrummer/Magidonia-24B-v4.2.0",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.790824",
      "end_time": "2026-01-21T17:10:25.790831",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TroyDoesAI/BlackSheep-24B": {
      "model_id": "TroyDoesAI/BlackSheep-24B",
      "model_name": "TroyDoesAI/BlackSheep-24B",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.815044",
      "end_time": "2026-01-21T17:10:25.815051",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Undi95/MistralThinker-v1.1": {
      "model_id": "Undi95/MistralThinker-v1.1",
      "model_name": "Undi95/MistralThinker-v1.1",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.839356",
      "end_time": "2026-01-21T17:10:25.839363",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "darkc0de/XortronCriminalComputingConfig": {
      "model_id": "darkc0de/XortronCriminalComputingConfig",
      "model_name": "darkc0de/XortronCriminalComputingConfig",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.863827",
      "end_time": "2026-01-21T17:10:25.863835",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lars1234/Mistral-Small-24B-Instruct-2501-writer": {
      "model_id": "lars1234/Mistral-Small-24B-Instruct-2501-writer",
      "model_name": "lars1234/Mistral-Small-24B-Instruct-2501-writer",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.888145",
      "end_time": "2026-01-21T17:10:25.888152",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Devstral-Small-2505": {
      "model_id": "mistralai/Devstral-Small-2505",
      "model_name": "mistralai/Devstral-Small-2505",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.912461",
      "end_time": "2026-01-21T17:10:25.912467",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Devstral-Small-2507": {
      "model_id": "mistralai/Devstral-Small-2507",
      "model_name": "mistralai/Devstral-Small-2507",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.936873",
      "end_time": "2026-01-21T17:10:25.936881",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Magistral-Small-2506": {
      "model_id": "mistralai/Magistral-Small-2506",
      "model_name": "mistralai/Magistral-Small-2506",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.961452",
      "end_time": "2026-01-21T17:10:25.961461",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Magistral-Small-2507": {
      "model_id": "mistralai/Magistral-Small-2507",
      "model_name": "mistralai/Magistral-Small-2507",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:25.985880",
      "end_time": "2026-01-21T17:10:25.985887",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Small-24B-Base-2501": {
      "model_id": "mistralai/Mistral-Small-24B-Base-2501",
      "model_name": "mistralai/Mistral-Small-24B-Base-2501",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.010905",
      "end_time": "2026-01-21T17:10:26.010912",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Small-24B-Instruct-2501": {
      "model_id": "mistralai/Mistral-Small-24B-Instruct-2501",
      "model_name": "mistralai/Mistral-Small-24B-Instruct-2501",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.035618",
      "end_time": "2026-01-21T17:10:26.035625",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "trashpanda-org/MS-24B-Instruct-Mullein-v0": {
      "model_id": "trashpanda-org/MS-24B-Instruct-Mullein-v0",
      "model_name": "trashpanda-org/MS-24B-Instruct-Mullein-v0",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.060368",
      "end_time": "2026-01-21T17:10:26.060375",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zerofata/MS3.2-PaintedFantasy-v3-24B": {
      "model_id": "zerofata/MS3.2-PaintedFantasy-v3-24B",
      "model_name": "zerofata/MS3.2-PaintedFantasy-v3-24B",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.084913",
      "end_time": "2026-01-21T17:10:26.084920",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Impish_Magic_24B": {
      "model_id": "SicariusSicariiStuff/Impish_Magic_24B",
      "model_name": "SicariusSicariiStuff/Impish_Magic_24B",
      "size_bytes": 23572413440,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.109364",
      "end_time": "2026-01-21T17:10:26.109371",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OddTheGreat/Apparatus_24B": {
      "model_id": "OddTheGreat/Apparatus_24B",
      "model_name": "OddTheGreat/Apparatus_24B",
      "size_bytes": 23572423680,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.133962",
      "end_time": "2026-01-21T17:10:26.133969",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Redemption_Wind_24B": {
      "model_id": "SicariusSicariiStuff/Redemption_Wind_24B",
      "model_name": "SicariusSicariiStuff/Redemption_Wind_24B",
      "size_bytes": 23572433920,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.158746",
      "end_time": "2026-01-21T17:10:26.158753",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "futurehouse/ether0": {
      "model_id": "futurehouse/ether0",
      "model_name": "futurehouse/ether0",
      "size_bytes": 23573058560,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.183331",
      "end_time": "2026-01-21T17:10:26.183338",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Devstral-Small-2-24B-Instruct-2512": {
      "model_id": "mistralai/Devstral-Small-2-24B-Instruct-2512",
      "model_name": "mistralai/Devstral-Small-2-24B-Instruct-2512",
      "size_bytes": 24011361840,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.208210",
      "end_time": "2026-01-21T17:10:26.208217",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ": {
      "model_id": "mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ",
      "model_name": "mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ",
      "size_bytes": 24144521071,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.233073",
      "end_time": "2026-01-21T17:10:26.233080",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "onnx-community/Llama-3.2-1B-Instruct-ONNX": {
      "model_id": "onnx-community/Llama-3.2-1B-Instruct-ONNX",
      "model_name": "onnx-community/Llama-3.2-1B-Instruct-ONNX",
      "size_bytes": 24480684688,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.257872",
      "end_time": "2026-01-21T17:10:26.257879",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Trinity-Mini": {
      "model_id": "arcee-ai/Trinity-Mini",
      "model_name": "arcee-ai/Trinity-Mini",
      "size_bytes": 26123974400,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.282602",
      "end_time": "2026-01-21T17:10:26.282610",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Trinity-Mini-Base": {
      "model_id": "arcee-ai/Trinity-Mini-Base",
      "model_name": "arcee-ai/Trinity-Mini-Base",
      "size_bytes": 26123974400,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.307431",
      "end_time": "2026-01-21T17:10:26.307440",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/llava-med-7b-delta": {
      "model_id": "microsoft/llava-med-7b-delta",
      "model_name": "microsoft/llava-med-7b-delta",
      "size_bytes": 26971251576,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.332436",
      "end_time": "2026-01-21T17:10:26.332444",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "google/medgemma-27b-text-it": {
      "model_id": "google/medgemma-27b-text-it",
      "model_name": "google/medgemma-27b-text-it",
      "size_bytes": 27009002240,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.357771",
      "end_time": "2026-01-21T17:10:26.357780",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-Rosetta-27B-2511": {
      "model_id": "yanolja/YanoljaNEXT-Rosetta-27B-2511",
      "model_name": "yanolja/YanoljaNEXT-Rosetta-27B-2511",
      "size_bytes": 27009346304,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.382729",
      "end_time": "2026-01-21T17:10:26.382737",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Fallen-Gemma3-27B-v1": {
      "model_id": "TheDrummer/Fallen-Gemma3-27B-v1",
      "model_name": "TheDrummer/Fallen-Gemma3-27B-v1",
      "size_bytes": 27432406640,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.407724",
      "end_time": "2026-01-21T17:10:26.407731",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "YanLabs/gemma-3-27b-it-abliterated-normpreserve": {
      "model_id": "YanLabs/gemma-3-27b-it-abliterated-normpreserve",
      "model_name": "YanLabs/gemma-3-27b-it-abliterated-normpreserve",
      "size_bytes": 27432406640,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.432768",
      "end_time": "2026-01-21T17:10:26.432775",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "leon-se/gemma-3-27b-it-FP8-Dynamic": {
      "model_id": "leon-se/gemma-3-27b-it-FP8-Dynamic",
      "model_name": "leon-se/gemma-3-27b-it-FP8-Dynamic",
      "size_bytes": 27436247664,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.457749",
      "end_time": "2026-01-21T17:10:26.457756",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5p-2b": {
      "model_id": "Salesforce/codet5p-2b",
      "model_name": "Salesforce/codet5p-2b",
      "size_bytes": 28000680508,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.482887",
      "end_time": "2026-01-21T17:10:26.482895",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CausalLM/14B-DPO-alpha": {
      "model_id": "CausalLM/14B-DPO-alpha",
      "model_name": "CausalLM/14B-DPO-alpha",
      "size_bytes": 28333479120,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.508079",
      "end_time": "2026-01-21T17:10:26.508086",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Big-Alice-28B-v1": {
      "model_id": "TheDrummer/Big-Alice-28B-v1",
      "model_name": "TheDrummer/Big-Alice-28B-v1",
      "size_bytes": 28606182400,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.533230",
      "end_time": "2026-01-21T17:10:26.533237",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX": {
      "model_id": "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX",
      "model_name": "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX",
      "size_bytes": 28871288494,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.558380",
      "end_time": "2026-01-21T17:10:26.558388",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "haoranxu/X-ALMA": {
      "model_id": "haoranxu/X-ALMA",
      "model_name": "haoranxu/X-ALMA",
      "size_bytes": 29038105600,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.583579",
      "end_time": "2026-01-21T17:10:26.583585",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Xenova/LaMini-Flan-T5-783M": {
      "model_id": "Xenova/LaMini-Flan-T5-783M",
      "model_name": "Xenova/LaMini-Flan-T5-783M",
      "size_bytes": 29104461506,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.608735",
      "end_time": "2026-01-21T17:10:26.608742",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen2-3_7B_P": {
      "model_id": "Salesforce/codegen2-3_7B_P",
      "model_name": "Salesforce/codegen2-3_7B_P",
      "size_bytes": 29263675764,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.633868",
      "end_time": "2026-01-21T17:10:26.633875",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TrustAI-lab/DeepNovel-7B-Roleplay": {
      "model_id": "TrustAI-lab/DeepNovel-7B-Roleplay",
      "model_name": "TrustAI-lab/DeepNovel-7B-Roleplay",
      "size_bytes": 30461976682,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.659198",
      "end_time": "2026-01-21T17:10:26.659205",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/AReaL-boba-RL-7B": {
      "model_id": "inclusionAI/AReaL-boba-RL-7B",
      "model_name": "inclusionAI/AReaL-boba-RL-7B",
      "size_bytes": 30462646366,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.684740",
      "end_time": "2026-01-21T17:10:26.684749",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Ewere/Qwen3-30B-A3B-abliterated-erotic": {
      "model_id": "Ewere/Qwen3-30B-A3B-abliterated-erotic",
      "model_name": "Ewere/Qwen3-30B-A3B-abliterated-erotic",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.709988",
      "end_time": "2026-01-21T17:10:26.709996",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-30B-A3B-Base": {
      "model_id": "Qwen/Qwen3-30B-A3B-Base",
      "model_name": "Qwen/Qwen3-30B-A3B-Base",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.735478",
      "end_time": "2026-01-21T17:10:26.735485",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated": {
      "model_id": "huihui-ai/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated",
      "model_name": "huihui-ai/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.760998",
      "end_time": "2026-01-21T17:10:26.761005",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "miromind-ai/MiroThinker-v1.0-30B": {
      "model_id": "miromind-ai/MiroThinker-v1.0-30B",
      "model_name": "miromind-ai/MiroThinker-v1.0-30B",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.786326",
      "end_time": "2026-01-21T17:10:26.786333",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "perplexity-ai/browsesafe": {
      "model_id": "perplexity-ai/browsesafe",
      "model_name": "perplexity-ai/browsesafe",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.811776",
      "end_time": "2026-01-21T17:10:26.811784",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "radicalnumerics/RND1-Base-0910": {
      "model_id": "radicalnumerics/RND1-Base-0910",
      "model_name": "radicalnumerics/RND1-Base-0910",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.837310",
      "end_time": "2026-01-21T17:10:26.837317",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Qwen3-30B-A3B": {
      "model_id": "unsloth/Qwen3-30B-A3B",
      "model_name": "unsloth/Qwen3-30B-A3B",
      "size_bytes": 30532122624,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.863035",
      "end_time": "2026-01-21T17:10:26.863042",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-30B-A3B-FP8": {
      "model_id": "Qwen/Qwen3-30B-A3B-FP8",
      "model_name": "Qwen/Qwen3-30B-A3B-FP8",
      "size_bytes": 30533947392,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.888520",
      "end_time": "2026-01-21T17:10:26.888527",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8": {
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8",
      "model_name": "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8",
      "size_bytes": 30533947392,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.913901",
      "end_time": "2026-01-21T17:10:26.913908",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Efficient-Large-Model/NVILA-15B": {
      "model_id": "Efficient-Large-Model/NVILA-15B",
      "model_name": "Efficient-Large-Model/NVILA-15B",
      "size_bytes": 30552774184,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.939605",
      "end_time": "2026-01-21T17:10:26.939612",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TildeAI/TildeOpen-30b": {
      "model_id": "TildeAI/TildeOpen-30b",
      "model_name": "TildeAI/TildeOpen-30b",
      "size_bytes": 30677882880,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.965533",
      "end_time": "2026-01-21T17:10:26.965540",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ": {
      "model_id": "QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ",
      "model_name": "QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ",
      "size_bytes": 31070754032,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:26.991351",
      "end_time": "2026-01-21T17:10:26.991359",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Skyfall-31B-v4": {
      "model_id": "TheDrummer/Skyfall-31B-v4",
      "model_name": "TheDrummer/Skyfall-31B-v4",
      "size_bytes": 31352980480,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.017107",
      "end_time": "2026-01-21T17:10:27.017114",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Qwen3-30B-A3B-bnb-4bit": {
      "model_id": "unsloth/Qwen3-30B-A3B-bnb-4bit",
      "model_name": "unsloth/Qwen3-30B-A3B-bnb-4bit",
      "size_bytes": 31480088776,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.042741",
      "end_time": "2026-01-21T17:10:27.042748",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenHands/openhands-critic-32b-exp-20250417": {
      "model_id": "OpenHands/openhands-critic-32b-exp-20250417",
      "model_name": "OpenHands/openhands-critic-32b-exp-20250417",
      "size_bytes": 31985313793,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.068644",
      "end_time": "2026-01-21T17:10:27.068652",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-small-base": {
      "model_id": "ibm-granite/granite-4.0-h-small-base",
      "model_name": "ibm-granite/granite-4.0-h-small-base",
      "size_bytes": 32207337984,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.094448",
      "end_time": "2026-01-21T17:10:27.094456",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Olmo-3-1125-32B": {
      "model_id": "allenai/Olmo-3-1125-32B",
      "model_name": "allenai/Olmo-3-1125-32B",
      "size_bytes": 32233522176,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.120479",
      "end_time": "2026-01-21T17:10:27.120487",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Olmo-3.1-32B-Think": {
      "model_id": "allenai/Olmo-3.1-32B-Think",
      "model_name": "allenai/Olmo-3.1-32B-Think",
      "size_bytes": 32233522176,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.146592",
      "end_time": "2026-01-21T17:10:27.146600",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-0325-32B": {
      "model_id": "allenai/OLMo-2-0325-32B",
      "model_name": "allenai/OLMo-2-0325-32B",
      "size_bytes": 32234279936,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.172598",
      "end_time": "2026-01-21T17:10:27.172605",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-0325-32B-Instruct": {
      "model_id": "allenai/OLMo-2-0325-32B-Instruct",
      "model_name": "allenai/OLMo-2-0325-32B-Instruct",
      "size_bytes": 32234279936,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.198408",
      "end_time": "2026-01-21T17:10:27.198414",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/c4ai-command-r-08-2024": {
      "model_id": "CohereLabs/c4ai-command-r-08-2024",
      "model_name": "CohereLabs/c4ai-command-r-08-2024",
      "size_bytes": 32296476672,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.224247",
      "end_time": "2026-01-21T17:10:27.224254",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HuggingFaceTB/SmolLM3-3B-ONNX": {
      "model_id": "HuggingFaceTB/SmolLM3-3B-ONNX",
      "model_name": "HuggingFaceTB/SmolLM3-3B-ONNX",
      "size_bytes": 32367164480,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.250094",
      "end_time": "2026-01-21T17:10:27.250101",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "medicalai/ClinicalGPT-base-zh": {
      "model_id": "medicalai/ClinicalGPT-base-zh",
      "model_name": "medicalai/ClinicalGPT-base-zh",
      "size_bytes": 32401155007,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.276202",
      "end_time": "2026-01-21T17:10:27.276209",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CalderaAI/30B-Lazarus": {
      "model_id": "CalderaAI/30B-Lazarus",
      "model_name": "CalderaAI/30B-Lazarus",
      "size_bytes": 32528947456,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.302229",
      "end_time": "2026-01-21T17:10:27.302235",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4-32B-0414": {
      "model_id": "zai-org/GLM-4-32B-0414",
      "model_name": "zai-org/GLM-4-32B-0414",
      "size_bytes": 32566081536,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.328085",
      "end_time": "2026-01-21T17:10:27.328092",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-Z1-32B-0414": {
      "model_id": "zai-org/GLM-Z1-32B-0414",
      "model_name": "zai-org/GLM-Z1-32B-0414",
      "size_bytes": 32566081536,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.354380",
      "end_time": "2026-01-21T17:10:27.354390",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "facebook/cwm": {
      "model_id": "facebook/cwm",
      "model_name": "facebook/cwm",
      "size_bytes": 32581097472,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.380843",
      "end_time": "2026-01-21T17:10:27.380851",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "facebook/cwm-pretrain": {
      "model_id": "facebook/cwm-pretrain",
      "model_name": "facebook/cwm-pretrain",
      "size_bytes": 32581097472,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.406983",
      "end_time": "2026-01-21T17:10:27.406990",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "facebook/cwm-sft": {
      "model_id": "facebook/cwm-sft",
      "model_name": "facebook/cwm-sft",
      "size_bytes": 32581097472,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.433088",
      "end_time": "2026-01-21T17:10:27.433096",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "RefalMachine/RuadaptQwen3-32B-Instruct": {
      "model_id": "RefalMachine/RuadaptQwen3-32B-Instruct",
      "model_name": "RefalMachine/RuadaptQwen3-32B-Instruct",
      "size_bytes": 32704001024,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.459354",
      "end_time": "2026-01-21T17:10:27.459362",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nn-tech/MetalGPT-1": {
      "model_id": "nn-tech/MetalGPT-1",
      "model_name": "nn-tech/MetalGPT-1",
      "size_bytes": 32759593984,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.485825",
      "end_time": "2026-01-21T17:10:27.485832",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepcogito/cogito-v1-preview-qwen-32B": {
      "model_id": "deepcogito/cogito-v1-preview-qwen-32B",
      "model_name": "deepcogito/cogito-v1-preview-qwen-32B",
      "size_bytes": 32759790592,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.512121",
      "end_time": "2026-01-21T17:10:27.512129",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nicoboss/DeepSeek-R1-Distill-Qwen-32B-Uncensored": {
      "model_id": "nicoboss/DeepSeek-R1-Distill-Qwen-32B-Uncensored",
      "model_name": "nicoboss/DeepSeek-R1-Distill-Qwen-32B-Uncensored",
      "size_bytes": 32759790592,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.538459",
      "end_time": "2026-01-21T17:10:27.538465",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DMindAI/DMind-1": {
      "model_id": "DMindAI/DMind-1",
      "model_name": "DMindAI/DMind-1",
      "size_bytes": 32762123264,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.564905",
      "end_time": "2026-01-21T17:10:27.564912",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MetaStoneTec/XBai-o4": {
      "model_id": "MetaStoneTec/XBai-o4",
      "model_name": "MetaStoneTec/XBai-o4",
      "size_bytes": 32762123264,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.591426",
      "end_time": "2026-01-21T17:10:27.591434",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-32B-AWQ": {
      "model_id": "Qwen/Qwen3-32B-AWQ",
      "model_name": "Qwen/Qwen3-32B-AWQ",
      "size_bytes": 32762123264,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.617965",
      "end_time": "2026-01-21T17:10:27.617974",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/MindLink-32B-0801": {
      "model_id": "Skywork/MindLink-32B-0801",
      "model_name": "Skywork/MindLink-32B-0801",
      "size_bytes": 32762123264,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.644207",
      "end_time": "2026-01-21T17:10:27.644214",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "agentica-org/DeepSWE-Preview": {
      "model_id": "agentica-org/DeepSWE-Preview",
      "model_name": "agentica-org/DeepSWE-Preview",
      "size_bytes": 32762123264,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.670573",
      "end_time": "2026-01-21T17:10:27.670580",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT": {
      "model_id": "OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT",
      "model_name": "OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT",
      "size_bytes": 32763433984,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.697027",
      "end_time": "2026-01-21T17:10:27.697036",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ArliAI/QwQ-32B-ArliAI-RpR-v1": {
      "model_id": "ArliAI/QwQ-32B-ArliAI-RpR-v1",
      "model_name": "ArliAI/QwQ-32B-ArliAI-RpR-v1",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.723716",
      "end_time": "2026-01-21T17:10:27.723725",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview": {
      "model_id": "FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview",
      "model_name": "FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.750392",
      "end_time": "2026-01-21T17:10:27.750399",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview": {
      "model_id": "FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview",
      "model_name": "FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.777038",
      "end_time": "2026-01-21T17:10:27.777045",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview": {
      "model_id": "FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview",
      "model_name": "FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.803642",
      "end_time": "2026-01-21T17:10:27.803649",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "GAIR/LIMO": {
      "model_id": "GAIR/LIMO",
      "model_name": "GAIR/LIMO",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.830169",
      "end_time": "2026-01-21T17:10:27.830176",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MetaStoneTec/MetaStone-S1-32B": {
      "model_id": "MetaStoneTec/MetaStone-S1-32B",
      "model_name": "MetaStoneTec/MetaStone-S1-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.856852",
      "end_time": "2026-01-21T17:10:27.856860",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenHands/openhands-lm-32b-v0.1": {
      "model_id": "OpenHands/openhands-lm-32b-v0.1",
      "model_name": "OpenHands/openhands-lm-32b-v0.1",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.883761",
      "end_time": "2026-01-21T17:10:27.883768",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenPipe/Deductive-Reasoning-Qwen-32B": {
      "model_id": "OpenPipe/Deductive-Reasoning-Qwen-32B",
      "model_name": "OpenPipe/Deductive-Reasoning-Qwen-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.910561",
      "end_time": "2026-01-21T17:10:27.910568",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PKU-DS-LAB/FairyR1-32B": {
      "model_id": "PKU-DS-LAB/FairyR1-32B",
      "model_name": "PKU-DS-LAB/FairyR1-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.937295",
      "end_time": "2026-01-21T17:10:27.937302",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PrimeIntellect/INTELLECT-2": {
      "model_id": "PrimeIntellect/INTELLECT-2",
      "model_name": "PrimeIntellect/INTELLECT-2",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.963961",
      "end_time": "2026-01-21T17:10:27.963969",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/Skywork-OR1-32B": {
      "model_id": "Skywork/Skywork-OR1-32B",
      "model_name": "Skywork/Skywork-OR1-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:27.990581",
      "end_time": "2026-01-21T17:10:27.990588",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/Skywork-OR1-32B-Preview": {
      "model_id": "Skywork/Skywork-OR1-32B-Preview",
      "model_name": "Skywork/Skywork-OR1-32B-Preview",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.017613",
      "end_time": "2026-01-21T17:10:28.017620",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "THU-KEG/LongWriter-Zero-32B": {
      "model_id": "THU-KEG/LongWriter-Zero-32B",
      "model_name": "THU-KEG/LongWriter-Zero-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.044396",
      "end_time": "2026-01-21T17:10:28.044403",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ": {
      "model_id": "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ",
      "model_name": "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.071336",
      "end_time": "2026-01-21T17:10:28.071343",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "bespokelabs/Bespoke-Stratos-32B": {
      "model_id": "bespokelabs/Bespoke-Stratos-32B",
      "model_name": "bespokelabs/Bespoke-Stratos-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.098404",
      "end_time": "2026-01-21T17:10:28.098413",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese": {
      "model_id": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
      "model_name": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.125598",
      "end_time": "2026-01-21T17:10:28.125605",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated": {
      "model_id": "huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated",
      "model_name": "huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.152754",
      "end_time": "2026-01-21T17:10:28.152762",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/QwQ-32B-abliterated": {
      "model_id": "huihui-ai/QwQ-32B-abliterated",
      "model_name": "huihui-ai/QwQ-32B-abliterated",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.179818",
      "end_time": "2026-01-21T17:10:28.179826",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/Qwen2.5-32B-Instruct-abliterated": {
      "model_id": "huihui-ai/Qwen2.5-32B-Instruct-abliterated",
      "model_name": "huihui-ai/Qwen2.5-32B-Instruct-abliterated",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.206902",
      "end_time": "2026-01-21T17:10:28.206908",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lixiaoxi45/WebThinker-QwQ-32B": {
      "model_id": "lixiaoxi45/WebThinker-QwQ-32B",
      "model_name": "lixiaoxi45/WebThinker-QwQ-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.234110",
      "end_time": "2026-01-21T17:10:28.234118",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/OpenCodeReasoning-Nemotron-1.1-32B": {
      "model_id": "nvidia/OpenCodeReasoning-Nemotron-1.1-32B",
      "model_name": "nvidia/OpenCodeReasoning-Nemotron-1.1-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.261361",
      "end_time": "2026-01-21T17:10:28.261369",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "open-r1/OlympicCoder-32B": {
      "model_id": "open-r1/OlympicCoder-32B",
      "model_name": "open-r1/OlympicCoder-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.288480",
      "end_time": "2026-01-21T17:10:28.288487",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "open-thoughts/OpenThinker-32B": {
      "model_id": "open-thoughts/OpenThinker-32B",
      "model_name": "open-thoughts/OpenThinker-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.315582",
      "end_time": "2026-01-21T17:10:28.315589",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "qihoo360/Light-R1-32B": {
      "model_id": "qihoo360/Light-R1-32B",
      "model_name": "qihoo360/Light-R1-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.343006",
      "end_time": "2026-01-21T17:10:28.343013",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "simplescaling/s1-32B": {
      "model_id": "simplescaling/s1-32B",
      "model_name": "simplescaling/s1-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.370542",
      "end_time": "2026-01-21T17:10:28.370552",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/SWE-Dev-32B": {
      "model_id": "zai-org/SWE-Dev-32B",
      "model_name": "zai-org/SWE-Dev-32B",
      "size_bytes": 32763876352,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.397964",
      "end_time": "2026-01-21T17:10:28.397972",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/FlexOlmo-7x7B-1T": {
      "model_id": "allenai/FlexOlmo-7x7B-1T",
      "model_name": "allenai/FlexOlmo-7x7B-1T",
      "size_bytes": 33270665216,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.425306",
      "end_time": "2026-01-21T17:10:28.425313",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "goonsai-com/civitaiprompts": {
      "model_id": "goonsai-com/civitaiprompts",
      "model_name": "goonsai-com/civitaiprompts",
      "size_bytes": 33300604120,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.452546",
      "end_time": "2026-01-21T17:10:28.452553",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-4-Mini-Hindi-4B-Instruct": {
      "model_id": "nvidia/Nemotron-4-Mini-Hindi-4B-Instruct",
      "model_name": "nvidia/Nemotron-4-Mini-Hindi-4B-Instruct",
      "size_bytes": 33553021765,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.480028",
      "end_time": "2026-01-21T17:10:28.480036",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "codefuse-ai/CodeFuse-CodeLlama-34B": {
      "model_id": "codefuse-ai/CodeFuse-CodeLlama-34B",
      "model_name": "codefuse-ai/CodeFuse-CodeLlama-34B",
      "size_bytes": 33743970304,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.507534",
      "end_time": "2026-01-21T17:10:28.507542",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zerofata/MS3.2-PaintedFantasy-Visage-v3-34B": {
      "model_id": "zerofata/MS3.2-PaintedFantasy-Visage-v3-34B",
      "model_name": "zerofata/MS3.2-PaintedFantasy-Visage-v3-34B",
      "size_bytes": 34131758080,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.535046",
      "end_time": "2026-01-21T17:10:28.535054",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LumiOpen/Poro-34B": {
      "model_id": "LumiOpen/Poro-34B",
      "model_name": "LumiOpen/Poro-34B",
      "size_bytes": 34216949760,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.562892",
      "end_time": "2026-01-21T17:10:28.562899",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "skt/A.X-3.1": {
      "model_id": "skt/A.X-3.1",
      "model_name": "skt/A.X-3.1",
      "size_bytes": 34670911488,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.590584",
      "end_time": "2026-01-21T17:10:28.590592",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "recursal/QRWKV6-32B-Instruct-Preview-v0.1": {
      "model_id": "recursal/QRWKV6-32B-Instruct-Preview-v0.1",
      "model_name": "recursal/QRWKV6-32B-Instruct-Preview-v0.1",
      "size_bytes": 34737492992,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.618330",
      "end_time": "2026-01-21T17:10:28.618337",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/c4ai-command-r-v01": {
      "model_id": "CohereLabs/c4ai-command-r-v01",
      "model_name": "CohereLabs/c4ai-command-r-v01",
      "size_bytes": 34980831232,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.645802",
      "end_time": "2026-01-21T17:10:28.645808",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-7B-Instruct-0124": {
      "model_id": "tencent/Hunyuan-7B-Instruct-0124",
      "model_name": "tencent/Hunyuan-7B-Instruct-0124",
      "size_bytes": 35008406130,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.673508",
      "end_time": "2026-01-21T17:10:28.673517",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/Kimi-Linear-REAP-35B-A3B-Instruct": {
      "model_id": "cerebras/Kimi-Linear-REAP-35B-A3B-Instruct",
      "model_name": "cerebras/Kimi-Linear-REAP-35B-A3B-Instruct",
      "size_bytes": 35132220360,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.701051",
      "end_time": "2026-01-21T17:10:28.701058",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/c4ai-command-r-v01-4bit": {
      "model_id": "CohereLabs/c4ai-command-r-v01-4bit",
      "model_name": "CohereLabs/c4ai-command-r-v01-4bit",
      "size_bytes": 35494684112,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.728837",
      "end_time": "2026-01-21T17:10:28.728846",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/Seed-OSS-36B-Base-woSyn": {
      "model_id": "ByteDance-Seed/Seed-OSS-36B-Base-woSyn",
      "model_name": "ByteDance-Seed/Seed-OSS-36B-Base-woSyn",
      "size_bytes": 36151104512,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.756775",
      "end_time": "2026-01-21T17:10:28.756783",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Hermes-4.3-36B": {
      "model_id": "NousResearch/Hermes-4.3-36B",
      "model_name": "NousResearch/Hermes-4.3-36B",
      "size_bytes": 36151104512,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.784847",
      "end_time": "2026-01-21T17:10:28.784854",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Skyfall-36B-v2": {
      "model_id": "TheDrummer/Skyfall-36B-v2",
      "model_name": "TheDrummer/Skyfall-36B-v2",
      "size_bytes": 36910535680,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.812702",
      "end_time": "2026-01-21T17:10:28.812709",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-mini-128k-instruct-onnx": {
      "model_id": "microsoft/Phi-3-mini-128k-instruct-onnx",
      "model_name": "microsoft/Phi-3-mini-128k-instruct-onnx",
      "size_bytes": 37912340153,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.840499",
      "end_time": "2026-01-21T17:10:28.840506",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BSC-LT/ALIA-40b": {
      "model_id": "BSC-LT/ALIA-40b",
      "model_name": "BSC-LT/ALIA-40b",
      "size_bytes": 40433885184,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.868522",
      "end_time": "2026-01-21T17:10:28.868529",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.3-70B-Instruct-NVFP4": {
      "model_id": "nvidia/Llama-3.3-70B-Instruct-NVFP4",
      "model_name": "nvidia/Llama-3.3-70B-Instruct-NVFP4",
      "size_bytes": 40606376096,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.896572",
      "end_time": "2026-01-21T17:10:28.896579",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rasbt/llama-3.2-from-scratch": {
      "model_id": "rasbt/llama-3.2-from-scratch",
      "model_name": "rasbt/llama-3.2-from-scratch",
      "size_bytes": 41113113098,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.924629",
      "end_time": "2026-01-21T17:10:28.924637",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/GRIN-MoE": {
      "model_id": "microsoft/GRIN-MoE",
      "model_name": "microsoft/GRIN-MoE",
      "size_bytes": 41873153344,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.952549",
      "end_time": "2026-01-21T17:10:28.952557",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DavidAU/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER": {
      "model_id": "DavidAU/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER",
      "model_name": "DavidAU/Qwen3-42B-A3B-2507-Thinking-Abliterated-uncensored-TOTAL-RECALL-v2-Medium-MASTER-CODER",
      "size_bytes": 42371414784,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:28.980441",
      "end_time": "2026-01-21T17:10:28.980448",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "facebook/blt-7b": {
      "model_id": "facebook/blt-7b",
      "model_name": "facebook/blt-7b",
      "size_bytes": 42612601843,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.008350",
      "end_time": "2026-01-21T17:10:29.008357",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-6B-mono": {
      "model_id": "Salesforce/codegen-6B-mono",
      "model_name": "Salesforce/codegen-6B-mono",
      "size_bytes": 42666539616,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.036378",
      "end_time": "2026-01-21T17:10:29.036385",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-6B-multi": {
      "model_id": "Salesforce/codegen-6B-multi",
      "model_name": "Salesforce/codegen-6B-multi",
      "size_bytes": 42666539616,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.064320",
      "end_time": "2026-01-21T17:10:29.064327",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dropbox-dash/lama-3.1-70b-instruct_4bitgs64_hqq": {
      "model_id": "dropbox-dash/lama-3.1-70b-instruct_4bitgs64_hqq",
      "model_name": "dropbox-dash/lama-3.1-70b-instruct_4bitgs64_hqq",
      "size_bytes": 42718825597,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.092418",
      "end_time": "2026-01-21T17:10:29.092425",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "apple/sage-ft-mixtral-8x7b": {
      "model_id": "apple/sage-ft-mixtral-8x7b",
      "model_name": "apple/sage-ft-mixtral-8x7b",
      "size_bytes": 46702792704,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.120574",
      "end_time": "2026-01-21T17:10:29.120581",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mixtral-8x7B-Instruct-v0.1": {
      "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "size_bytes": 46702792704,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.148449",
      "end_time": "2026-01-21T17:10:29.148456",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mixtral-8x7B-v0.1": {
      "model_id": "mistralai/Mixtral-8x7B-v0.1",
      "model_name": "mistralai/Mixtral-8x7B-v0.1",
      "size_bytes": 46702792704,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.176536",
      "end_time": "2026-01-21T17:10:29.176543",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Nemotron-H-47B-Reasoning-128K": {
      "model_id": "nvidia/Nemotron-H-47B-Reasoning-128K",
      "model_name": "nvidia/Nemotron-H-47B-Reasoning-128K",
      "size_bytes": 46791554816,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.204602",
      "end_time": "2026-01-21T17:10:29.204609",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Rakuten/RakutenAI-2.0-8x7B-instruct": {
      "model_id": "Rakuten/RakutenAI-2.0-8x7B-instruct",
      "model_name": "Rakuten/RakutenAI-2.0-8x7B-instruct",
      "size_bytes": 46833864704,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.232804",
      "end_time": "2026-01-21T17:10:29.232811",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Kimi-Linear-48B-A3B-Instruct": {
      "model_id": "moonshotai/Kimi-Linear-48B-A3B-Instruct",
      "model_name": "moonshotai/Kimi-Linear-48B-A3B-Instruct",
      "size_bytes": 49122681728,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.261173",
      "end_time": "2026-01-21T17:10:29.261181",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Valkyrie-49B-v1": {
      "model_id": "TheDrummer/Valkyrie-49B-v1",
      "model_name": "TheDrummer/Valkyrie-49B-v1",
      "size_bytes": 49867145216,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.289339",
      "end_time": "2026-01-21T17:10:29.289347",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Valkyrie-49B-v2": {
      "model_id": "TheDrummer/Valkyrie-49B-v2",
      "model_name": "TheDrummer/Valkyrie-49B-v2",
      "size_bytes": 49867145216,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.317707",
      "end_time": "2026-01-21T17:10:29.317715",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3_3-Nemotron-Super-49B-GenRM": {
      "model_id": "nvidia/Llama-3_3-Nemotron-Super-49B-GenRM",
      "model_name": "nvidia/Llama-3_3-Nemotron-Super-49B-GenRM",
      "size_bytes": 49867145216,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.346162",
      "end_time": "2026-01-21T17:10:29.346169",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3_1-Nemotron-51B-Instruct": {
      "model_id": "nvidia/Llama-3_1-Nemotron-51B-Instruct",
      "model_name": "nvidia/Llama-3_1-Nemotron-51B-Instruct",
      "size_bytes": 51501015040,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.374648",
      "end_time": "2026-01-21T17:10:29.374657",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Mini-1.5": {
      "model_id": "ai21labs/AI21-Jamba-Mini-1.5",
      "model_name": "ai21labs/AI21-Jamba-Mini-1.5",
      "size_bytes": 51570323328,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.403130",
      "end_time": "2026-01-21T17:10:29.403138",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Mini-1.6": {
      "model_id": "ai21labs/AI21-Jamba-Mini-1.6",
      "model_name": "ai21labs/AI21-Jamba-Mini-1.6",
      "size_bytes": 51570323328,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.431756",
      "end_time": "2026-01-21T17:10:29.431763",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Mini-1.7": {
      "model_id": "ai21labs/AI21-Jamba-Mini-1.7",
      "model_name": "ai21labs/AI21-Jamba-Mini-1.7",
      "size_bytes": 51570323328,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.460297",
      "end_time": "2026-01-21T17:10:29.460304",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "maicomputer/toolpaca": {
      "model_id": "maicomputer/toolpaca",
      "model_name": "maicomputer/toolpaca",
      "size_bytes": 52064150058,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.488841",
      "end_time": "2026-01-21T17:10:29.488848",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MediaTek-Research/Breeze-7B-FC-v1_0": {
      "model_id": "MediaTek-Research/Breeze-7B-FC-v1_0",
      "model_name": "MediaTek-Research/Breeze-7B-FC-v1_0",
      "size_bytes": 54831078600,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.517357",
      "end_time": "2026-01-21T17:10:29.517364",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen25-7b-instruct_P": {
      "model_id": "Salesforce/codegen25-7b-instruct_P",
      "model_name": "Salesforce/codegen25-7b-instruct_P",
      "size_bytes": 55165780440,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.545855",
      "end_time": "2026-01-21T17:10:29.545862",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen25-7b-mono_P": {
      "model_id": "Salesforce/codegen25-7b-mono_P",
      "model_name": "Salesforce/codegen25-7b-mono_P",
      "size_bytes": 55165780440,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.574598",
      "end_time": "2026-01-21T17:10:29.574605",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen25-7b-multi_P": {
      "model_id": "Salesforce/codegen25-7b-multi_P",
      "model_name": "Salesforce/codegen25-7b-multi_P",
      "size_bytes": 55165780440,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.603276",
      "end_time": "2026-01-21T17:10:29.603283",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xgen-7b-4k-base": {
      "model_id": "Salesforce/xgen-7b-4k-base",
      "model_name": "Salesforce/xgen-7b-4k-base",
      "size_bytes": 55165780440,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.631857",
      "end_time": "2026-01-21T17:10:29.631864",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xgen-7b-8k-base": {
      "model_id": "Salesforce/xgen-7b-8k-base",
      "model_name": "Salesforce/xgen-7b-8k-base",
      "size_bytes": 55165780440,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.660517",
      "end_time": "2026-01-21T17:10:29.660525",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xgen-7b-8k-inst": {
      "model_id": "Salesforce/xgen-7b-8k-inst",
      "model_name": "Salesforce/xgen-7b-8k-inst",
      "size_bytes": 55165780440,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.689314",
      "end_time": "2026-01-21T17:10:29.689323",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen2-7B_P": {
      "model_id": "Salesforce/codegen2-7B_P",
      "model_name": "Salesforce/codegen2-7B_P",
      "size_bytes": 55171431752,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.718353",
      "end_time": "2026-01-21T17:10:29.718361",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "internlm/internlm2-7b": {
      "model_id": "internlm/internlm2-7b",
      "model_name": "internlm/internlm2-7b",
      "size_bytes": 61903351588,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.747081",
      "end_time": "2026-01-21T17:10:29.747089",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moxin-org/Moxin-7B-Chat": {
      "model_id": "moxin-org/Moxin-7B-Chat",
      "model_name": "moxin-org/Moxin-7B-Chat",
      "size_bytes": 64916751578,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.775783",
      "end_time": "2026-01-21T17:10:29.775790",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codet5p-16b": {
      "model_id": "Salesforce/codet5p-16b",
      "model_name": "Salesforce/codet5p-16b",
      "size_bytes": 66436322219,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.804521",
      "end_time": "2026-01-21T17:10:29.804528",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/instructcodet5p-16b": {
      "model_id": "Salesforce/instructcodet5p-16b",
      "model_name": "Salesforce/instructcodet5p-16b",
      "size_bytes": 66436322219,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.833434",
      "end_time": "2026-01-21T17:10:29.833441",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ValiantLabs/Llama2-70B-ShiningValiant": {
      "model_id": "ValiantLabs/Llama2-70B-ShiningValiant",
      "model_name": "ValiantLabs/Llama2-70B-ShiningValiant",
      "size_bytes": 68976648192,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.862490",
      "end_time": "2026-01-21T17:10:29.862497",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "elinas/chronos-70b-v2": {
      "model_id": "elinas/chronos-70b-v2",
      "model_name": "elinas/chronos-70b-v2",
      "size_bytes": 68976648192,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.891442",
      "end_time": "2026-01-21T17:10:29.891449",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/model_007": {
      "model_id": "pankajmathur/model_007",
      "model_name": "pankajmathur/model_007",
      "size_bytes": 68976648192,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.920377",
      "end_time": "2026-01-21T17:10:29.920384",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pankajmathur/orca_mini_v3_70b": {
      "model_id": "pankajmathur/orca_mini_v3_70b",
      "model_name": "pankajmathur/orca_mini_v3_70b",
      "size_bytes": 68976648192,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.949541",
      "end_time": "2026-01-21T17:10:29.949549",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "infly/INF-ORM-Llama3.1-70B": {
      "model_id": "infly/INF-ORM-Llama3.1-70B",
      "model_name": "infly/INF-ORM-Llama3.1-70B",
      "size_bytes": 69570158593,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:29.978587",
      "end_time": "2026-01-21T17:10:29.978594",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ArliAI/DS-R1-Distill-70B-ArliAI-RpR-v4-Large": {
      "model_id": "ArliAI/DS-R1-Distill-70B-ArliAI-RpR-v4-Large",
      "model_name": "ArliAI/DS-R1-Distill-70B-ArliAI-RpR-v4-Large",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.007754",
      "end_time": "2026-01-21T17:10:30.007762",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BruhzWater/Sapphira-L3.3-70b-0.1": {
      "model_id": "BruhzWater/Sapphira-L3.3-70b-0.1",
      "model_name": "BruhzWater/Sapphira-L3.3-70b-0.1",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.037110",
      "end_time": "2026-01-21T17:10:30.037117",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ReadyArt/L3.3-The-Omega-Directive-70B-Unslop-v2.1": {
      "model_id": "ReadyArt/L3.3-The-Omega-Directive-70B-Unslop-v2.1",
      "model_name": "ReadyArt/L3.3-The-Omega-Directive-70B-Unslop-v2.1",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.066481",
      "end_time": "2026-01-21T17:10:30.066488",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "RedHatAI/Meta-Llama-3.1-70B-Instruct-quantized.w4a16": {
      "model_id": "RedHatAI/Meta-Llama-3.1-70B-Instruct-quantized.w4a16",
      "model_name": "RedHatAI/Meta-Llama-3.1-70B-Instruct-quantized.w4a16",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.095657",
      "end_time": "2026-01-21T17:10:30.095664",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Sao10K/70B-L3.3-Cirrus-x1": {
      "model_id": "Sao10K/70B-L3.3-Cirrus-x1",
      "model_name": "Sao10K/70B-L3.3-Cirrus-x1",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.124905",
      "end_time": "2026-01-21T17:10:30.124912",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SentientAGI/Dobby-Unhinged-Llama-3.3-70B": {
      "model_id": "SentientAGI/Dobby-Unhinged-Llama-3.3-70B",
      "model_name": "SentientAGI/Dobby-Unhinged-Llama-3.3-70B",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.154111",
      "end_time": "2026-01-21T17:10:30.154119",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SicariusSicariiStuff/Negative_LLAMA_70B": {
      "model_id": "SicariusSicariiStuff/Negative_LLAMA_70B",
      "model_name": "SicariusSicariiStuff/Negative_LLAMA_70B",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.183671",
      "end_time": "2026-01-21T17:10:30.183678",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Steelskull/L3.3-Damascus-R1": {
      "model_id": "Steelskull/L3.3-Damascus-R1",
      "model_name": "Steelskull/L3.3-Damascus-R1",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.212911",
      "end_time": "2026-01-21T17:10:30.212918",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Steelskull/L3.3-MS-Nevoria-70b": {
      "model_id": "Steelskull/L3.3-MS-Nevoria-70b",
      "model_name": "Steelskull/L3.3-MS-Nevoria-70b",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.242443",
      "end_time": "2026-01-21T17:10:30.242449",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Steelskull/L3.3-Nevoria-R1-70b": {
      "model_id": "Steelskull/L3.3-Nevoria-R1-70b",
      "model_name": "Steelskull/L3.3-Nevoria-R1-70b",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.271964",
      "end_time": "2026-01-21T17:10:30.271972",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Steelskull/L3.3-Shakudo-70b": {
      "model_id": "Steelskull/L3.3-Shakudo-70b",
      "model_name": "Steelskull/L3.3-Shakudo-70b",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.301435",
      "end_time": "2026-01-21T17:10:30.301441",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Anubis-70B-v1.1": {
      "model_id": "TheDrummer/Anubis-70B-v1.1",
      "model_name": "TheDrummer/Anubis-70B-v1.1",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.330808",
      "end_time": "2026-01-21T17:10:30.330815",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Fallen-Llama-3.3-R1-70B-v1": {
      "model_id": "TheDrummer/Fallen-Llama-3.3-R1-70B-v1",
      "model_name": "TheDrummer/Fallen-Llama-3.3-R1-70B-v1",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.360864",
      "end_time": "2026-01-21T17:10:30.360874",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepcogito/cogito-v2-preview-llama-70B": {
      "model_id": "deepcogito/cogito-v2-preview-llama-70B",
      "model_name": "deepcogito/cogito-v2-preview-llama-70B",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.390573",
      "end_time": "2026-01-21T17:10:30.390582",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated": {
      "model_id": "huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated",
      "model_name": "huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.419966",
      "end_time": "2026-01-21T17:10:30.419973",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Meta-Llama-3-70B-Instruct": {
      "model_id": "meta-llama/Meta-Llama-3-70B-Instruct",
      "model_name": "meta-llama/Meta-Llama-3-70B-Instruct",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.449397",
      "end_time": "2026-01-21T17:10:30.449405",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Nemotron-70B-Reward-HF": {
      "model_id": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
      "model_name": "nvidia/Llama-3.1-Nemotron-70B-Reward-HF",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.479169",
      "end_time": "2026-01-21T17:10:30.479176",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "perplexity-ai/r1-1776-distill-llama-70b": {
      "model_id": "perplexity-ai/r1-1776-distill-llama-70b",
      "model_name": "perplexity-ai/r1-1776-distill-llama-70b",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.508851",
      "end_time": "2026-01-21T17:10:30.508858",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sophosympatheia/Strawberrylemonade-L3-70B-v1.2": {
      "model_id": "sophosympatheia/Strawberrylemonade-L3-70B-v1.2",
      "model_name": "sophosympatheia/Strawberrylemonade-L3-70B-v1.2",
      "size_bytes": 70553706496,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.538450",
      "end_time": "2026-01-21T17:10:30.538458",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "swiss-ai/Apertus-70B-Instruct-2509": {
      "model_id": "swiss-ai/Apertus-70B-Instruct-2509",
      "model_name": "swiss-ai/Apertus-70B-Instruct-2509",
      "size_bytes": 70599864480,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.568060",
      "end_time": "2026-01-21T17:10:30.568067",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mozilla-ai/Llama-3.2-1B-Instruct-llamafile": {
      "model_id": "mozilla-ai/Llama-3.2-1B-Instruct-llamafile",
      "model_name": "mozilla-ai/Llama-3.2-1B-Instruct-llamafile",
      "size_bytes": 70997930719,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.597686",
      "end_time": "2026-01-21T17:10:30.597694",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QuixiAI/Qwen3-72B-Embiggened": {
      "model_id": "QuixiAI/Qwen3-72B-Embiggened",
      "model_name": "QuixiAI/Qwen3-72B-Embiggened",
      "size_bytes": 72703307776,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.627468",
      "end_time": "2026-01-21T17:10:30.627476",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Virtuoso-Large": {
      "model_id": "arcee-ai/Virtuoso-Large",
      "model_name": "arcee-ai/Virtuoso-Large",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.657359",
      "end_time": "2026-01-21T17:10:30.657367",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "miromind-ai/MiroThinker-v1.0-72B": {
      "model_id": "miromind-ai/MiroThinker-v1.0-72B",
      "model_name": "miromind-ai/MiroThinker-v1.0-72B",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.687133",
      "end_time": "2026-01-21T17:10:30.687142",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Kimi-Dev-72B": {
      "model_id": "moonshotai/Kimi-Dev-72B",
      "model_name": "moonshotai/Kimi-Dev-72B",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.716868",
      "end_time": "2026-01-21T17:10:30.716876",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/AceMath-72B-Instruct": {
      "model_id": "nvidia/AceMath-72B-Instruct",
      "model_name": "nvidia/AceMath-72B-Instruct",
      "size_bytes": 72706203648,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.746945",
      "end_time": "2026-01-21T17:10:30.746953",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-Math-PRM-72B": {
      "model_id": "Qwen/Qwen2.5-Math-PRM-72B",
      "model_name": "Qwen/Qwen2.5-Math-PRM-72B",
      "size_bytes": 72773337090,
      "tp_size": 4,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.776803",
      "end_time": "2026-01-21T17:10:30.776811",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "featherless-ai/QRWKV-72B": {
      "model_id": "featherless-ai/QRWKV-72B",
      "model_name": "featherless-ai/QRWKV-72B",
      "size_bytes": 79295848448,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.806657",
      "end_time": "2026-01-21T17:10:30.806665",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MBZUAI/Llama-3-Nanda-10B-Chat": {
      "model_id": "MBZUAI/Llama-3-Nanda-10B-Chat",
      "model_name": "MBZUAI/Llama-3-Nanda-10B-Chat",
      "size_bytes": 79893205624,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.836833",
      "end_time": "2026-01-21T17:10:30.836839",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-A13B-Instruct": {
      "model_id": "tencent/Hunyuan-A13B-Instruct",
      "model_name": "tencent/Hunyuan-A13B-Instruct",
      "size_bytes": 80393183232,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.866937",
      "end_time": "2026-01-21T17:10:30.866944",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-A13B-Instruct-FP8": {
      "model_id": "tencent/Hunyuan-A13B-Instruct-FP8",
      "model_name": "tencent/Hunyuan-A13B-Instruct-FP8",
      "size_bytes": 80393183232,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.896836",
      "end_time": "2026-01-21T17:10:30.896843",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-A13B-Pretrain": {
      "model_id": "tencent/Hunyuan-A13B-Pretrain",
      "model_name": "tencent/Hunyuan-A13B-Pretrain",
      "size_bytes": 80393183232,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.926906",
      "end_time": "2026-01-21T17:10:30.926913",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-Next-80B-A3B-Instruct": {
      "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct",
      "model_name": "Qwen/Qwen3-Next-80B-A3B-Instruct",
      "size_bytes": 81324862720,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.956799",
      "end_time": "2026-01-21T17:10:30.956806",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Qwen3-Next-80B-A3B-Instruct": {
      "model_id": "unsloth/Qwen3-Next-80B-A3B-Instruct",
      "model_name": "unsloth/Qwen3-Next-80B-A3B-Instruct",
      "size_bytes": 81324862720,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:30.986804",
      "end_time": "2026-01-21T17:10:30.986811",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8": {
      "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8",
      "model_name": "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8",
      "size_bytes": 81329784384,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.016994",
      "end_time": "2026-01-21T17:10:31.017001",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8": {
      "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8",
      "model_name": "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8",
      "size_bytes": 81329784384,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.047043",
      "end_time": "2026-01-21T17:10:31.047050",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/GLM-4.5-Air-REAP-82B-A12B": {
      "model_id": "cerebras/GLM-4.5-Air-REAP-82B-A12B",
      "model_name": "cerebras/GLM-4.5-Air-REAP-82B-A12B",
      "size_bytes": 81932181504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.077278",
      "end_time": "2026-01-21T17:10:31.077285",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Writer/palmyra-large": {
      "model_id": "Writer/palmyra-large",
      "model_name": "Writer/palmyra-large",
      "size_bytes": 82998479813,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.107734",
      "end_time": "2026-01-21T17:10:31.107741",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "krutrim-ai-labs/Krutrim-2-instruct": {
      "model_id": "krutrim-ai-labs/Krutrim-2-instruct",
      "model_name": "krutrim-ai-labs/Krutrim-2-instruct",
      "size_bytes": 97999506638,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.138186",
      "end_time": "2026-01-21T17:10:31.138193",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/LLaDA2.0-flash": {
      "model_id": "inclusionAI/LLaDA2.0-flash",
      "model_name": "inclusionAI/LLaDA2.0-flash",
      "size_bytes": 102889705216,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.168520",
      "end_time": "2026-01-21T17:10:31.168528",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/LLaDA2.0-flash-preview": {
      "model_id": "inclusionAI/LLaDA2.0-flash-preview",
      "model_name": "inclusionAI/LLaDA2.0-flash-preview",
      "size_bytes": 102889705216,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.198837",
      "end_time": "2026-01-21T17:10:31.198844",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/c4ai-command-r-plus-08-2024": {
      "model_id": "CohereLabs/c4ai-command-r-plus-08-2024",
      "model_name": "CohereLabs/c4ai-command-r-plus-08-2024",
      "size_bytes": 103810674688,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.229470",
      "end_time": "2026-01-21T17:10:31.229477",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "maicomputer/alpaca-13b": {
      "model_id": "maicomputer/alpaca-13b",
      "model_name": "maicomputer/alpaca-13b",
      "size_bytes": 104127705154,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.259891",
      "end_time": "2026-01-21T17:10:31.259899",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "maicomputer/gpt4-x-alpaca": {
      "model_id": "maicomputer/gpt4-x-alpaca",
      "model_name": "maicomputer/gpt4-x-alpaca",
      "size_bytes": 104127705154,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.290511",
      "end_time": "2026-01-21T17:10:31.290518",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yentinglin/Taiwan-LLaMa-v1.0": {
      "model_id": "yentinglin/Taiwan-LLaMa-v1.0",
      "model_name": "yentinglin/Taiwan-LLaMa-v1.0",
      "size_bytes": 104127920823,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.320864",
      "end_time": "2026-01-21T17:10:31.320871",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "large-traversaal/Alif-1.0-8B-Instruct": {
      "model_id": "large-traversaal/Alif-1.0-8B-Instruct",
      "model_name": "large-traversaal/Alif-1.0-8B-Instruct",
      "size_bytes": 104291619527,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.351630",
      "end_time": "2026-01-21T17:10:31.351640",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Anubis-Pro-105B-v1": {
      "model_id": "TheDrummer/Anubis-Pro-105B-v1",
      "model_name": "TheDrummer/Anubis-Pro-105B-v1",
      "size_bytes": 104779882496,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.382372",
      "end_time": "2026-01-21T17:10:31.382380",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/DeepSeek-V3-0324-4bit": {
      "model_id": "mlx-community/DeepSeek-V3-0324-4bit",
      "model_name": "mlx-community/DeepSeek-V3-0324-4bit",
      "size_bytes": 104938540544,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.412672",
      "end_time": "2026-01-21T17:10:31.412680",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-flash-base-2.0": {
      "model_id": "inclusionAI/Ling-flash-base-2.0",
      "model_name": "inclusionAI/Ling-flash-base-2.0",
      "size_bytes": 106195886336,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.443430",
      "end_time": "2026-01-21T17:10:31.443437",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/GLM-Steam-106B-A12B-v1": {
      "model_id": "TheDrummer/GLM-Steam-106B-A12B-v1",
      "model_name": "TheDrummer/GLM-Steam-106B-A12B-v1",
      "size_bytes": 106852245504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.474484",
      "end_time": "2026-01-21T17:10:31.474491",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zerofata/GLM-4.5-Iceblink-v2-106B-A12B": {
      "model_id": "zerofata/GLM-4.5-Iceblink-v2-106B-A12B",
      "model_name": "zerofata/GLM-4.5-Iceblink-v2-106B-A12B",
      "size_bytes": 106852245504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.505171",
      "end_time": "2026-01-21T17:10:31.505179",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PrimeIntellect/INTELLECT-3": {
      "model_id": "PrimeIntellect/INTELLECT-3",
      "model_name": "PrimeIntellect/INTELLECT-3",
      "size_bytes": 106852251264,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.536247",
      "end_time": "2026-01-21T17:10:31.536254",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PrimeIntellect/INTELLECT-3-FP8": {
      "model_id": "PrimeIntellect/INTELLECT-3-FP8",
      "model_name": "PrimeIntellect/INTELLECT-3-FP8",
      "size_bytes": 106893249280,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.567236",
      "end_time": "2026-01-21T17:10:31.567244",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ArliAI/GLM-4.5-Air-Derestricted": {
      "model_id": "ArliAI/GLM-4.5-Air-Derestricted",
      "model_name": "ArliAI/GLM-4.5-Air-Derestricted",
      "size_bytes": 110468824832,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.598305",
      "end_time": "2026-01-21T17:10:31.598313",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.5-Air": {
      "model_id": "zai-org/GLM-4.5-Air",
      "model_name": "zai-org/GLM-4.5-Air",
      "size_bytes": 110468824832,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.629059",
      "end_time": "2026-01-21T17:10:31.629067",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.5-Air-Base": {
      "model_id": "zai-org/GLM-4.5-Air-Base",
      "model_name": "zai-org/GLM-4.5-Air-Base",
      "size_bytes": 110468824832,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.659943",
      "end_time": "2026-01-21T17:10:31.659951",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.5-Air-FP8": {
      "model_id": "zai-org/GLM-4.5-Air-FP8",
      "model_name": "zai-org/GLM-4.5-Air-FP8",
      "size_bytes": 110510884480,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.691028",
      "end_time": "2026-01-21T17:10:31.691037",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "CohereLabs/c4ai-command-a-03-2025": {
      "model_id": "CohereLabs/c4ai-command-a-03-2025",
      "model_name": "CohereLabs/c4ai-command-a-03-2025",
      "size_bytes": 111057580032,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.722072",
      "end_time": "2026-01-21T17:10:31.722080",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Fallen-Command-A-111B-v1": {
      "model_id": "TheDrummer/Fallen-Command-A-111B-v1",
      "model_name": "TheDrummer/Fallen-Command-A-111B-v1",
      "size_bytes": 111057580032,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.753375",
      "end_time": "2026-01-21T17:10:31.753382",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Xenova/nllb-200-distilled-600M": {
      "model_id": "Xenova/nllb-200-distilled-600M",
      "model_name": "Xenova/nllb-200-distilled-600M",
      "size_bytes": 112199995698,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.784533",
      "end_time": "2026-01-21T17:10:31.784541",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ArliAI/gpt-oss-120b-Derestricted": {
      "model_id": "ArliAI/gpt-oss-120b-Derestricted",
      "model_name": "ArliAI/gpt-oss-120b-Derestricted",
      "size_bytes": 116829156672,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.815652",
      "end_time": "2026-01-21T17:10:31.815660",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FreeSEED-AI/gpt-oss-120b-mandarin-thinking": {
      "model_id": "FreeSEED-AI/gpt-oss-120b-mandarin-thinking",
      "model_name": "FreeSEED-AI/gpt-oss-120b-mandarin-thinking",
      "size_bytes": 116829156672,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.847041",
      "end_time": "2026-01-21T17:10:31.847048",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated": {
      "model_id": "huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated",
      "model_name": "huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated",
      "size_bytes": 116829156672,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.878090",
      "end_time": "2026-01-21T17:10:31.878098",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openai/gpt-oss-120b": {
      "model_id": "openai/gpt-oss-120b",
      "model_name": "openai/gpt-oss-120b",
      "size_bytes": 120412337472,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.909167",
      "end_time": "2026-01-21T17:10:31.909174",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gpt-oss-120b": {
      "model_id": "unsloth/gpt-oss-120b",
      "model_name": "unsloth/gpt-oss-120b",
      "size_bytes": 120412337472,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.940192",
      "end_time": "2026-01-21T17:10:31.940198",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Behemoth-R1-123B-v2": {
      "model_id": "TheDrummer/Behemoth-R1-123B-v2",
      "model_name": "TheDrummer/Behemoth-R1-123B-v2",
      "size_bytes": 122610069504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:31.971471",
      "end_time": "2026-01-21T17:10:31.971478",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TheDrummer/Behemoth-X-123B-v2": {
      "model_id": "TheDrummer/Behemoth-X-123B-v2",
      "model_name": "TheDrummer/Behemoth-X-123B-v2",
      "size_bytes": 122610069504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.002670",
      "end_time": "2026-01-21T17:10:32.002678",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Large-Instruct-2407": {
      "model_id": "mistralai/Mistral-Large-Instruct-2407",
      "model_name": "mistralai/Mistral-Large-Instruct-2407",
      "size_bytes": 122610069504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.033926",
      "end_time": "2026-01-21T17:10:32.033933",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Large-Instruct-2411": {
      "model_id": "mistralai/Mistral-Large-Instruct-2411",
      "model_name": "mistralai/Mistral-Large-Instruct-2411",
      "size_bytes": 122610069504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.065152",
      "end_time": "2026-01-21T17:10:32.065160",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Devstral-2-123B-Instruct-2512": {
      "model_id": "mistralai/Devstral-2-123B-Instruct-2512",
      "model_name": "mistralai/Devstral-2-123B-Instruct-2512",
      "size_bytes": 125025989840,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.096417",
      "end_time": "2026-01-21T17:10:32.096423",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/DeepSeek-R1-4bit": {
      "model_id": "mlx-community/DeepSeek-R1-4bit",
      "model_name": "mlx-community/DeepSeek-R1-4bit",
      "size_bytes": 125904758272,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.127810",
      "end_time": "2026-01-21T17:10:32.127817",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen2-16B_P": {
      "model_id": "Salesforce/codegen2-16B_P",
      "model_name": "Salesforce/codegen2-16B_P",
      "size_bytes": 128542596059,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.159474",
      "end_time": "2026-01-21T17:10:32.159481",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-16B-mono": {
      "model_id": "Salesforce/codegen-16B-mono",
      "model_name": "Salesforce/codegen-16B-mono",
      "size_bytes": 128689627285,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.190828",
      "end_time": "2026-01-21T17:10:32.190835",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-16B-multi": {
      "model_id": "Salesforce/codegen-16B-multi",
      "model_name": "Salesforce/codegen-16B-multi",
      "size_bytes": 128689627285,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.222712",
      "end_time": "2026-01-21T17:10:32.222719",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/codegen-16B-nl": {
      "model_id": "Salesforce/codegen-16B-nl",
      "model_name": "Salesforce/codegen-16B-nl",
      "size_bytes": 128689627285,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.255770",
      "end_time": "2026-01-21T17:10:32.255777",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/AReaL-boba-SFT-32B": {
      "model_id": "inclusionAI/AReaL-boba-SFT-32B",
      "model_name": "inclusionAI/AReaL-boba-SFT-32B",
      "size_bytes": 131055922439,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.287483",
      "end_time": "2026-01-21T17:10:32.287490",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/MiniMax-M2-REAP-139B-A10B": {
      "model_id": "cerebras/MiniMax-M2-REAP-139B-A10B",
      "model_name": "cerebras/MiniMax-M2-REAP-139B-A10B",
      "size_bytes": 139157610368,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.319132",
      "end_time": "2026-01-21T17:10:32.319139",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mixtral-8x22B-v0.1": {
      "model_id": "mistralai/Mixtral-8x22B-v0.1",
      "model_name": "mistralai/Mixtral-8x22B-v0.1",
      "size_bytes": 140620634112,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.350819",
      "end_time": "2026-01-21T17:10:32.350829",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xLAM-8x22b-r": {
      "model_id": "Salesforce/xLAM-8x22b-r",
      "model_name": "Salesforce/xLAM-8x22b-r",
      "size_bytes": 140630071296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.382519",
      "end_time": "2026-01-21T17:10:32.382526",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mixtral-8x22B-Instruct-v0.1": {
      "model_id": "mistralai/Mixtral-8x22B-Instruct-v0.1",
      "model_name": "mistralai/Mixtral-8x22B-Instruct-v0.1",
      "size_bytes": 140630071296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.414091",
      "end_time": "2026-01-21T17:10:32.414098",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Writer/Palmyra-Med-70B-32K": {
      "model_id": "Writer/Palmyra-Med-70B-32K",
      "model_name": "Writer/Palmyra-Med-70B-32K",
      "size_bytes": 141107497872,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.445930",
      "end_time": "2026-01-21T17:10:32.445937",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "aaditya/Llama3-OpenBioLLM-70B": {
      "model_id": "aaditya/Llama3-OpenBioLLM-70B",
      "model_name": "aaditya/Llama3-OpenBioLLM-70B",
      "size_bytes": 141107674503,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.477879",
      "end_time": "2026-01-21T17:10:32.477886",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Nemotron-70B-Reward": {
      "model_id": "nvidia/Llama-3.1-Nemotron-70B-Reward",
      "model_name": "nvidia/Llama-3.1-Nemotron-70B-Reward",
      "size_bytes": 141130109477,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.509663",
      "end_time": "2026-01-21T17:10:32.509670",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Nemotron-70B-Instruct": {
      "model_id": "nvidia/Llama-3.1-Nemotron-70B-Instruct",
      "model_name": "nvidia/Llama-3.1-Nemotron-70B-Instruct",
      "size_bytes": 141130115749,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.541490",
      "end_time": "2026-01-21T17:10:32.541498",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rednote-hilab/dots.llm1.inst": {
      "model_id": "rednote-hilab/dots.llm1.inst",
      "model_name": "rednote-hilab/dots.llm1.inst",
      "size_bytes": 142774381696,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.573257",
      "end_time": "2026-01-21T17:10:32.573264",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/MiniMax-M2-REAP-162B-A10B": {
      "model_id": "cerebras/MiniMax-M2-REAP-162B-A10B",
      "model_name": "cerebras/MiniMax-M2-REAP-162B-A10B",
      "size_bytes": 161983066112,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.605382",
      "end_time": "2026-01-21T17:10:32.605390",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "VibeStudio/MiniMax-M2-THRIFT": {
      "model_id": "VibeStudio/MiniMax-M2-THRIFT",
      "model_name": "VibeStudio/MiniMax-M2-THRIFT",
      "size_bytes": 172507452032,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.637414",
      "end_time": "2026-01-21T17:10:32.637421",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "maicomputer/alpaca-native": {
      "model_id": "maicomputer/alpaca-native",
      "model_name": "maicomputer/alpaca-native",
      "size_bytes": 175200243880,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.669128",
      "end_time": "2026-01-21T17:10:32.669136",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "bigscience/bloomz-p3": {
      "model_id": "bigscience/bloomz-p3",
      "model_name": "bigscience/bloomz-p3",
      "size_bytes": 176247271424,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.701150",
      "end_time": "2026-01-21T17:10:32.701159",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "medicalai/MedFound-176B": {
      "model_id": "medicalai/MedFound-176B",
      "model_name": "medicalai/MedFound-176B",
      "size_bytes": 176247271424,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.733415",
      "end_time": "2026-01-21T17:10:32.733422",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-4-Scout-17B-16E-Instruct-Original": {
      "model_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct-Original",
      "model_name": "meta-llama/Llama-4-Scout-17B-16E-Instruct-Original",
      "size_bytes": 217367972958,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.765418",
      "end_time": "2026-01-21T17:10:32.765425",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-4-Scout-17B-16E-Original": {
      "model_id": "meta-llama/Llama-4-Scout-17B-16E-Original",
      "model_name": "meta-llama/Llama-4-Scout-17B-16E-Original",
      "size_bytes": 217431101638,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.797528",
      "end_time": "2026-01-21T17:10:32.797536",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/GLM-4.6-REAP-218B-A32B-FP8": {
      "model_id": "cerebras/GLM-4.6-REAP-218B-A32B-FP8",
      "model_name": "cerebras/GLM-4.6-REAP-218B-A32B-FP8",
      "size_bytes": 218455751680,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.829699",
      "end_time": "2026-01-21T17:10:32.829707",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MiniMaxAI/MiniMax-M2": {
      "model_id": "MiniMaxAI/MiniMax-M2",
      "model_name": "MiniMaxAI/MiniMax-M2",
      "size_bytes": 228703644928,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.862054",
      "end_time": "2026-01-21T17:10:32.862061",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/MiniMax-M2": {
      "model_id": "unsloth/MiniMax-M2",
      "model_name": "unsloth/MiniMax-M2",
      "size_bytes": 228703644928,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.894226",
      "end_time": "2026-01-21T17:10:32.894233",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-235B-A22B-GPTQ-Int4": {
      "model_id": "Qwen/Qwen3-235B-A22B-GPTQ-Int4",
      "model_name": "Qwen/Qwen3-235B-A22B-GPTQ-Int4",
      "size_bytes": 235093634560,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.926525",
      "end_time": "2026-01-21T17:10:32.926533",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-235B-A22B-Thinking-2507": {
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "model_name": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "size_bytes": 235093634560,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.958702",
      "end_time": "2026-01-21T17:10:32.958710",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/Qwen3-Coder-REAP-246B-A35B-FP8": {
      "model_id": "cerebras/Qwen3-Coder-REAP-246B-A35B-FP8",
      "model_name": "cerebras/Qwen3-Coder-REAP-246B-A35B-FP8",
      "size_bytes": 246097141760,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:32.991369",
      "end_time": "2026-01-21T17:10:32.991376",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lemonilia/Mistral-Small-3-Reasoner-s1": {
      "model_id": "lemonilia/Mistral-Small-3-Reasoner-s1",
      "model_name": "lemonilia/Mistral-Small-3-Reasoner-s1",
      "size_bytes": 248198010170,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.023662",
      "end_time": "2026-01-21T17:10:33.023669",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit": {
      "model_id": "mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit",
      "model_name": "mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit",
      "size_bytes": 270099870790,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.056107",
      "end_time": "2026-01-21T17:10:33.056114",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-plus-base": {
      "model_id": "inclusionAI/Ling-plus-base",
      "model_name": "inclusionAI/Ling-plus-base",
      "size_bytes": 292540679424,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.088398",
      "end_time": "2026-01-21T17:10:33.088404",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-300B-A47B-Base-PT": {
      "model_id": "baidu/ERNIE-4.5-300B-A47B-Base-PT",
      "model_name": "baidu/ERNIE-4.5-300B-A47B-Base-PT",
      "size_bytes": 299484163264,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.120743",
      "end_time": "2026-01-21T17:10:33.120749",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mozilla-ai/Meta-Llama-3.1-8B-llamafile": {
      "model_id": "mozilla-ai/Meta-Llama-3.1-8B-llamafile",
      "model_name": "mozilla-ai/Meta-Llama-3.1-8B-llamafile",
      "size_bytes": 322817634542,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.153191",
      "end_time": "2026-01-21T17:10:33.153199",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cerebras/DeepSeek-V3.2-REAP-345B-A37B": {
      "model_id": "cerebras/DeepSeek-V3.2-REAP-345B-A37B",
      "model_name": "cerebras/DeepSeek-V3.2-REAP-345B-A37B",
      "size_bytes": 344891268784,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.185437",
      "end_time": "2026-01-21T17:10:33.185443",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "GAIR/LIMI": {
      "model_id": "GAIR/LIMI",
      "model_name": "GAIR/LIMI",
      "size_bytes": 352797829024,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.217799",
      "end_time": "2026-01-21T17:10:33.217806",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "huihui-ai/Huihui-GLM-4.6-abliterated-mlx-4bit": {
      "model_id": "huihui-ai/Huihui-GLM-4.6-abliterated-mlx-4bit",
      "model_name": "huihui-ai/Huihui-GLM-4.6-abliterated-mlx-4bit",
      "size_bytes": 352797829024,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.250235",
      "end_time": "2026-01-21T17:10:33.250242",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.6": {
      "model_id": "zai-org/GLM-4.6",
      "model_name": "zai-org/GLM-4.6",
      "size_bytes": 356785898816,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.282759",
      "end_time": "2026-01-21T17:10:33.282767",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.5": {
      "model_id": "zai-org/GLM-4.5",
      "model_name": "zai-org/GLM-4.5",
      "size_bytes": 358337791296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.315041",
      "end_time": "2026-01-21T17:10:33.315048",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.5-Base": {
      "model_id": "zai-org/GLM-4.5-Base",
      "model_name": "zai-org/GLM-4.5-Base",
      "size_bytes": 358337791296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.347690",
      "end_time": "2026-01-21T17:10:33.347698",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-4.6-FP8": {
      "model_id": "zai-org/GLM-4.6-FP8",
      "model_name": "zai-org/GLM-4.6-FP8",
      "size_bytes": 358458543424,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.380505",
      "end_time": "2026-01-21T17:10:33.380514",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/DeepSeek-R1-0528-NVFP4": {
      "model_id": "nvidia/DeepSeek-R1-0528-NVFP4",
      "model_name": "nvidia/DeepSeek-R1-0528-NVFP4",
      "size_bytes": 396767013632,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.413052",
      "end_time": "2026-01-21T17:10:33.413058",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/DeepSeek-R1-NVFP4": {
      "model_id": "nvidia/DeepSeek-R1-NVFP4",
      "model_name": "nvidia/DeepSeek-R1-NVFP4",
      "size_bytes": 396767013632,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.445648",
      "end_time": "2026-01-21T17:10:33.445655",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Large-1.5": {
      "model_id": "ai21labs/AI21-Jamba-Large-1.5",
      "model_name": "ai21labs/AI21-Jamba-Large-1.5",
      "size_bytes": 398555145696,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.478473",
      "end_time": "2026-01-21T17:10:33.478481",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Large-1.6": {
      "model_id": "ai21labs/AI21-Jamba-Large-1.6",
      "model_name": "ai21labs/AI21-Jamba-Large-1.6",
      "size_bytes": 398555145696,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.511202",
      "end_time": "2026-01-21T17:10:33.511210",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai21labs/AI21-Jamba-Large-1.7": {
      "model_id": "ai21labs/AI21-Jamba-Large-1.7",
      "model_name": "ai21labs/AI21-Jamba-Large-1.7",
      "size_bytes": 398555145696,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.543855",
      "end_time": "2026-01-21T17:10:33.543862",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Hermes-4-405B": {
      "model_id": "NousResearch/Hermes-4-405B",
      "model_name": "NousResearch/Hermes-4-405B",
      "size_bytes": 405853388800,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.576547",
      "end_time": "2026-01-21T17:10:33.576554",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nicoboss/Hermes-3-Llama-3.1-405B-Uncensored": {
      "model_id": "nicoboss/Hermes-3-Llama-3.1-405B-Uncensored",
      "model_name": "nicoboss/Hermes-3-Llama-3.1-405B-Uncensored",
      "size_bytes": 405853388800,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.609601",
      "end_time": "2026-01-21T17:10:33.609608",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-Original": {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-Original",
      "model_name": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-Original",
      "size_bytes": 433651486654,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.642582",
      "end_time": "2026-01-21T17:10:33.642589",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MiniMaxAI/MiniMax-M1-40k": {
      "model_id": "MiniMaxAI/MiniMax-M1-40k",
      "model_name": "MiniMaxAI/MiniMax-M1-40k",
      "size_bytes": 456089655296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.675572",
      "end_time": "2026-01-21T17:10:33.675582",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MiniMaxAI/MiniMax-M1-80k": {
      "model_id": "MiniMaxAI/MiniMax-M1-80k",
      "model_name": "MiniMaxAI/MiniMax-M1-80k",
      "size_bytes": 456089655296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.708276",
      "end_time": "2026-01-21T17:10:33.708283",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MiniMaxAI/MiniMax-Text-01": {
      "model_id": "MiniMaxAI/MiniMax-Text-01",
      "model_name": "MiniMaxAI/MiniMax-Text-01",
      "size_bytes": 456089655296,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.741141",
      "end_time": "2026-01-21T17:10:33.741148",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
      "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "model_name": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "size_bytes": 480154875392,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.774082",
      "end_time": "2026-01-21T17:10:33.774090",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "astanahub/alemllm": {
      "model_id": "astanahub/alemllm",
      "model_name": "astanahub/alemllm",
      "size_bytes": 494394083755,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.807057",
      "end_time": "2026-01-21T17:10:33.807064",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "xai-org/grok-2": {
      "model_id": "xai-org/grok-2",
      "model_name": "xai-org/grok-2",
      "size_bytes": 539032697512,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.840032",
      "end_time": "2026-01-21T17:10:33.840038",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mozilla-ai/Meta-Llama-3.1-8B-Instruct-llamafile": {
      "model_id": "mozilla-ai/Meta-Llama-3.1-8B-Instruct-llamafile",
      "model_name": "mozilla-ai/Meta-Llama-3.1-8B-Instruct-llamafile",
      "size_bytes": 540186722802,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.873118",
      "end_time": "2026-01-21T17:10:33.873126",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Kimi-K2-Thinking": {
      "model_id": "moonshotai/Kimi-K2-Thinking",
      "model_name": "moonshotai/Kimi-K2-Thinking",
      "size_bytes": 594263623840,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.906234",
      "end_time": "2026-01-21T17:10:33.906242",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepcogito/cogito-671b-v2.1": {
      "model_id": "deepcogito/cogito-671b-v2.1",
      "model_name": "deepcogito/cogito-671b-v2.1",
      "size_bytes": 671019752960,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.939480",
      "end_time": "2026-01-21T17:10:33.939486",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepcogito/cogito-v2-preview-deepseek-671B-MoE": {
      "model_id": "deepcogito/cogito-v2-preview-deepseek-671B-MoE",
      "model_name": "deepcogito/cogito-v2-preview-deepseek-671B-MoE",
      "size_bytes": 671019752960,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:33.972494",
      "end_time": "2026-01-21T17:10:33.972502",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OPEA/DeepSeek-V3-int4-sym-gptq-inc": {
      "model_id": "OPEA/DeepSeek-V3-int4-sym-gptq-inc",
      "model_name": "OPEA/DeepSeek-V3-int4-sym-gptq-inc",
      "size_bytes": 671026419200,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.005499",
      "end_time": "2026-01-21T17:10:34.005506",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QuixiAI/DeepSeek-R1-0528-AWQ": {
      "model_id": "QuixiAI/DeepSeek-R1-0528-AWQ",
      "model_name": "QuixiAI/DeepSeek-R1-0528-AWQ",
      "size_bytes": 671026419200,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.038754",
      "end_time": "2026-01-21T17:10:34.038761",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QuixiAI/DeepSeek-V3-AWQ": {
      "model_id": "QuixiAI/DeepSeek-V3-AWQ",
      "model_name": "QuixiAI/DeepSeek-V3-AWQ",
      "size_bytes": 671026419200,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.071801",
      "end_time": "2026-01-21T17:10:34.071808",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "perplexity-ai/r1-1776": {
      "model_id": "perplexity-ai/r1-1776",
      "model_name": "perplexity-ai/r1-1776",
      "size_bytes": 671026419200,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.105140",
      "end_time": "2026-01-21T17:10:34.105147",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/MAI-DS-R1-FP8": {
      "model_id": "microsoft/MAI-DS-R1-FP8",
      "model_name": "microsoft/MAI-DS-R1-FP8",
      "size_bytes": 671067257432,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.138515",
      "end_time": "2026-01-21T17:10:34.138523",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nex-agi/DeepSeek-V3.1-Nex-N1": {
      "model_id": "nex-agi/DeepSeek-V3.1-Nex-N1",
      "model_name": "nex-agi/DeepSeek-V3.1-Nex-N1",
      "size_bytes": 671067257432,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.171982",
      "end_time": "2026-01-21T17:10:34.171990",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1": {
      "model_id": "unsloth/DeepSeek-R1",
      "model_name": "unsloth/DeepSeek-R1",
      "size_bytes": 684489845504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.205137",
      "end_time": "2026-01-21T17:10:34.205144",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1-BF16": {
      "model_id": "unsloth/DeepSeek-R1-BF16",
      "model_name": "unsloth/DeepSeek-R1-BF16",
      "size_bytes": 684489845504,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.238722",
      "end_time": "2026-01-21T17:10:34.238730",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-Zero": {
      "model_id": "deepseek-ai/DeepSeek-R1-Zero",
      "model_name": "deepseek-ai/DeepSeek-R1-Zero",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.272306",
      "end_time": "2026-01-21T17:10:34.272314",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3": {
      "model_id": "deepseek-ai/DeepSeek-V3",
      "model_name": "deepseek-ai/DeepSeek-V3",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.305730",
      "end_time": "2026-01-21T17:10:34.305737",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3-0324": {
      "model_id": "deepseek-ai/DeepSeek-V3-0324",
      "model_name": "deepseek-ai/DeepSeek-V3-0324",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.339272",
      "end_time": "2026-01-21T17:10:34.339279",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3-Base": {
      "model_id": "deepseek-ai/DeepSeek-V3-Base",
      "model_name": "deepseek-ai/DeepSeek-V3-Base",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.372940",
      "end_time": "2026-01-21T17:10:34.372950",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3.1-Terminus": {
      "model_id": "deepseek-ai/DeepSeek-V3.1-Terminus",
      "model_name": "deepseek-ai/DeepSeek-V3.1-Terminus",
      "size_bytes": 684531386000,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.406546",
      "end_time": "2026-01-21T17:10:34.406554",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-Math-V2": {
      "model_id": "deepseek-ai/DeepSeek-Math-V2",
      "model_name": "deepseek-ai/DeepSeek-Math-V2",
      "size_bytes": 685396921376,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.440269",
      "end_time": "2026-01-21T17:10:34.440276",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3.2": {
      "model_id": "deepseek-ai/DeepSeek-V3.2",
      "model_name": "deepseek-ai/DeepSeek-V3.2",
      "size_bytes": 685396921376,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.474061",
      "end_time": "2026-01-21T17:10:34.474068",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3.2-Exp": {
      "model_id": "deepseek-ai/DeepSeek-V3.2-Exp",
      "model_name": "deepseek-ai/DeepSeek-V3.2-Exp",
      "size_bytes": 685396921376,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.507748",
      "end_time": "2026-01-21T17:10:34.507756",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3.2-Exp-Base": {
      "model_id": "deepseek-ai/DeepSeek-V3.2-Exp-Base",
      "model_name": "deepseek-ai/DeepSeek-V3.2-Exp-Base",
      "size_bytes": 685396921376,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.541374",
      "end_time": "2026-01-21T17:10:34.541382",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-V3.2-Speciale": {
      "model_id": "deepseek-ai/DeepSeek-V3.2-Speciale",
      "model_name": "deepseek-ai/DeepSeek-V3.2-Speciale",
      "size_bytes": 685396921376,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.575225",
      "end_time": "2026-01-21T17:10:34.575233",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ai-sage/GigaChat3-702B-A36B-preview": {
      "model_id": "ai-sage/GigaChat3-702B-A36B-preview",
      "model_name": "ai-sage/GigaChat3-702B-A36B-preview",
      "size_bytes": 715481588280,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.608918",
      "end_time": "2026-01-21T17:10:34.608926",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-Original": {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-Original",
      "model_name": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-Original",
      "size_bytes": 803416846598,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.642600",
      "end_time": "2026-01-21T17:10:34.642607",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "meta-llama/Llama-4-Maverick-17B-128E-Original": {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Original",
      "model_name": "meta-llama/Llama-4-Maverick-17B-128E-Original",
      "size_bytes": 803668569318,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.676637",
      "end_time": "2026-01-21T17:10:34.676646",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-1T": {
      "model_id": "inclusionAI/Ling-1T",
      "model_name": "inclusionAI/Ling-1T",
      "size_bytes": 999705328640,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.710678",
      "end_time": "2026-01-21T17:10:34.710686",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-1T": {
      "model_id": "inclusionAI/Ring-1T",
      "model_name": "inclusionAI/Ring-1T",
      "size_bytes": 999705328640,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.744488",
      "end_time": "2026-01-21T17:10:34.744496",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-1T-preview": {
      "model_id": "inclusionAI/Ring-1T-preview",
      "model_name": "inclusionAI/Ring-1T-preview",
      "size_bytes": 999705328640,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.778674",
      "end_time": "2026-01-21T17:10:34.778682",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Kimi-K2-Base": {
      "model_id": "moonshotai/Kimi-K2-Base",
      "model_name": "moonshotai/Kimi-K2-Base",
      "size_bytes": 1026470731056,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.812704",
      "end_time": "2026-01-21T17:10:34.812712",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Kimi-K2-Instruct": {
      "model_id": "moonshotai/Kimi-K2-Instruct",
      "model_name": "moonshotai/Kimi-K2-Instruct",
      "size_bytes": 1026470731056,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.846965",
      "end_time": "2026-01-21T17:10:34.846973",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Kimi-K2-Instruct": {
      "model_id": "unsloth/Kimi-K2-Instruct",
      "model_name": "unsloth/Kimi-K2-Instruct",
      "size_bytes": 1026470731056,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.880960",
      "end_time": "2026-01-21T17:10:34.880968",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LLM360/K2-V2-Instruct": {
      "model_id": "LLM360/K2-V2-Instruct",
      "model_name": "LLM360/K2-V2-Instruct",
      "size_bytes": 2902051713514,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.915127",
      "end_time": "2026-01-21T17:10:34.915135",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Tencent-Hunyuan-Large": {
      "model_id": "tencent/Tencent-Hunyuan-Large",
      "model_name": "tencent/Tencent-Hunyuan-Large",
      "size_bytes": 3895680282804,
      "tp_size": 8,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:34.949279",
      "end_time": "2026-01-21T17:10:34.949287",
      "duration_seconds": 0.0,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Marked incomplete in skip list",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PleIAs/Pleias-RAG-1B": {
      "model_id": "PleIAs/Pleias-RAG-1B",
      "model_name": "PleIAs/Pleias-RAG-1B",
      "size_bytes": 4783203281,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:38.537738",
      "end_time": "2026-01-21T17:10:40.116457",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ServiceNow-AI/Apriel-5B-Base": {
      "model_id": "ServiceNow-AI/Apriel-5B-Base",
      "model_name": "ServiceNow-AI/Apriel-5B-Base",
      "size_bytes": 4832071680,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:38.536587",
      "end_time": "2026-01-21T17:10:40.156126",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/AFM-4.5B-Base": {
      "model_id": "arcee-ai/AFM-4.5B-Base",
      "model_name": "arcee-ai/AFM-4.5B-Base",
      "size_bytes": 4619184640,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:38.373821",
      "end_time": "2026-01-21T17:10:40.314078",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Minitron-4B-Width-Base": {
      "model_id": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
      "model_name": "nvidia/Llama-3.1-Minitron-4B-Width-Base",
      "size_bytes": 4512746496,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:38.370419",
      "end_time": "2026-01-21T17:10:40.339201",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Minitron-4B-Depth-Base": {
      "model_id": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
      "model_name": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
      "size_bytes": 4540469248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:38.377613",
      "end_time": "2026-01-21T17:10:40.363688",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Chain-GPT/Solidity-LLM": {
      "model_id": "Chain-GPT/Solidity-LLM",
      "model_name": "Chain-GPT/Solidity-LLM",
      "size_bytes": 5559027270,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:40.600660",
      "end_time": "2026-01-21T17:10:40.636022",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "m-a-p/YuE-s1-7B-anneal-en-cot": {
      "model_id": "m-a-p/YuE-s1-7B-anneal-en-cot",
      "model_name": "m-a-p/YuE-s1-7B-anneal-en-cot",
      "size_bytes": 6224613376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:40.643656",
      "end_time": "2026-01-21T17:10:40.682582",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "m-a-p/YuE-s1-7B-anneal-en-icl": {
      "model_id": "m-a-p/YuE-s1-7B-anneal-en-icl",
      "model_name": "m-a-p/YuE-s1-7B-anneal-en-icl",
      "size_bytes": 6224613376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:40.689848",
      "end_time": "2026-01-21T17:10:40.730005",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "m-a-p/YuE-s1-7B-anneal-jp-kr-cot": {
      "model_id": "m-a-p/YuE-s1-7B-anneal-jp-kr-cot",
      "model_name": "m-a-p/YuE-s1-7B-anneal-jp-kr-cot",
      "size_bytes": 6224613376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:40.736221",
      "end_time": "2026-01-21T17:10:40.779536",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "m-a-p/YuE-s1-7B-anneal-zh-cot": {
      "model_id": "m-a-p/YuE-s1-7B-anneal-zh-cot",
      "model_name": "m-a-p/YuE-s1-7B-anneal-zh-cot",
      "size_bytes": 6224613376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:10:40.786106",
      "end_time": "2026-01-21T17:10:40.826493",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ServiceNow-AI/Apriel-5B-Instruct": {
      "model_id": "ServiceNow-AI/Apriel-5B-Instruct",
      "model_name": "ServiceNow-AI/Apriel-5B-Instruct",
      "size_bytes": 4832071680,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:38.534406",
      "end_time": "2026-01-21T17:11:00.273005",
      "duration_seconds": 21.74,
      "vllm_pid": 117857,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=117857)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m   Value error, Model architectures ['AprielForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=117857)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 21.74
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-tiny-base-preview": {
      "model_id": "ibm-granite/granite-4.0-tiny-base-preview",
      "model_name": "ibm-granite/granite-4.0-tiny-base-preview",
      "size_bytes": 6673296960,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:00.322753",
      "end_time": "2026-01-21T17:11:00.408633",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BUT-FIT/csmpt7b": {
      "model_id": "BUT-FIT/csmpt7b",
      "model_name": "BUT-FIT/csmpt7b",
      "size_bytes": 6704869376,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:00.415045",
      "end_time": "2026-01-21T17:11:00.514733",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lmstudio-community/Qwen3-30B-A3B-MLX-4bit": {
      "model_id": "lmstudio-community/Qwen3-30B-A3B-MLX-4bit",
      "model_name": "lmstudio-community/Qwen3-30B-A3B-MLX-4bit",
      "size_bytes": 4770822144,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:38.535069",
      "end_time": "2026-01-21T17:11:15.238263",
      "duration_seconds": 36.7,
      "vllm_pid": 117861,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=117861)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen3_moe.py\", line 748, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m ValueError: There is no module or parameter named 'lm_head.biases' in Qwen3MoeForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=126178)\u001b[0;0m \n[rank0]:[W121 17:11:06.821053390 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=117861)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 36.7
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-7B": {
      "model_id": "allenai/OLMo-7B",
      "model_name": "allenai/OLMo-7B",
      "size_bytes": 6888095744,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:15.494897",
      "end_time": "2026-01-21T17:11:15.523371",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit": {
      "model_id": "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit",
      "model_name": "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit",
      "size_bytes": 4735493602,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:38.534403",
      "end_time": "2026-01-21T17:11:15.239131",
      "duration_seconds": 36.7,
      "vllm_pid": 117841,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=117841)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.model = self._init_model(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 611, in _init_model\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     return LlamaModel(vllm_config=vllm_config, prefix=prefix, layer_type=layer_type)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 393, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 395, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     lambda prefix: layer_type(vllm_config=vllm_config, prefix=prefix),\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 302, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.self_attn = LlamaAttention(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=126192)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W121 17:11:06.430924898 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=117841)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 36.7
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AdaptLLM/law-chat": {
      "model_id": "AdaptLLM/law-chat",
      "model_name": "AdaptLLM/law-chat",
      "size_bytes": 6738425856,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:15.436107",
      "end_time": "2026-01-21T17:11:15.488205",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit": {
      "model_id": "mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
      "model_name": "mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
      "size_bytes": 5120300032,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:38.537073",
      "end_time": "2026-01-21T17:11:15.238257",
      "duration_seconds": 36.7,
      "vllm_pid": 117854,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=117854)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 600, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m ValueError: There is no module or parameter named 'lm_head.biases' in Qwen2ForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=126184)\u001b[0;0m \n[rank0]:[W121 17:11:06.060978307 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=117854)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 36.7
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "GreatCaptainNemo/ProLLaMA": {
      "model_id": "GreatCaptainNemo/ProLLaMA",
      "model_name": "GreatCaptainNemo/ProLLaMA",
      "size_bytes": 6738417664,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:15.387227",
      "end_time": "2026-01-21T17:11:15.429511",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMoE-1B-7B-0125": {
      "model_id": "allenai/OLMoE-1B-7B-0125",
      "model_name": "allenai/OLMoE-1B-7B-0125",
      "size_bytes": 6919161856,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:15.534763",
      "end_time": "2026-01-21T17:11:15.638046",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit": {
      "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit",
      "model_name": "unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit",
      "size_bytes": 5414323229,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:40.372462",
      "end_time": "2026-01-21T17:11:15.530855",
      "duration_seconds": 35.16,
      "vllm_pid": 759263,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=759263)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 151, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=759460)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W121 17:11:07.608292274 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=759263)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 35.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LLM360/Amber": {
      "model_id": "LLM360/Amber",
      "model_name": "LLM360/Amber",
      "size_bytes": 6738415616,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:11:15.325618",
      "end_time": "2026-01-21T17:11:15.380615",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit": {
      "model_id": "cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit",
      "model_name": "cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit",
      "size_bytes": 5306567040,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:38.535932",
      "end_time": "2026-01-21T17:11:45.450948",
      "duration_seconds": 66.92,
      "vllm_pid": 117871,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=117871)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.44s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.42s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:19<00:06,  6.93s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:28<00:00,  7.76s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:28<00:00,  7.20s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 56, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     process_weights_after_loading(model, model_config, target_device)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 108, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     quant_method.process_weights_after_loading(module)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 896, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     layer.scheme.process_weights_after_loading(layer)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py\", line 225, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self.kernel.process_weights_after_loading(layer)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py\", line 168, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     self._transform_param(layer, self.w_s_name, transform_w_s)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py\", line 74, in _transform_param\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     new_param = fn(old_param)\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m                 ^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py\", line 113, in transform_w_s\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     x.data.contiguous(),\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=126180)\u001b[0;0m \n[rank0]:[W121 17:11:36.371749029 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=117871)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 66.92
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit": {
      "model_id": "cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit",
      "model_name": "cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit",
      "size_bytes": 5306567040,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:10:38.538244",
      "end_time": "2026-01-21T17:11:45.451875",
      "duration_seconds": 66.91,
      "vllm_pid": 117866,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=117866)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.39s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.38s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:19<00:07,  7.03s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:27<00:00,  7.11s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:27<00:00,  6.80s/it]\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 56, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     process_weights_after_loading(model, model_config, target_device)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 108, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     quant_method.process_weights_after_loading(module)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 896, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     layer.scheme.process_weights_after_loading(layer)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py\", line 225, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self.kernel.process_weights_after_loading(layer)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py\", line 168, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     self._transform_param(layer, self.w_s_name, transform_w_s)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py\", line 74, in _transform_param\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     new_param = fn(old_param)\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m                 ^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py\", line 113, in transform_w_s\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     x.data.contiguous(),\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=126188)\u001b[0;0m \n[rank0]:[W121 17:11:34.802842940 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=117866)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 66.91
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/phi-4-unsloth-bnb-4bit": {
      "model_id": "unsloth/phi-4-unsloth-bnb-4bit",
      "model_name": "unsloth/phi-4-unsloth-bnb-4bit",
      "size_bytes": 15058992382,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:11:15.325212",
      "end_time": "2026-01-21T17:12:01.155739",
      "duration_seconds": 45.83,
      "vllm_pid": 127830,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=127830)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n[rank0]:[W121 17:11:47.749010828 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=127968)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=127830)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 45.83
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "internlm/internlm2-1_8b": {
      "model_id": "internlm/internlm2-1_8b",
      "model_name": "internlm/internlm2-1_8b",
      "size_bytes": 15114505452,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:12:01.229329",
      "end_time": "2026-01-21T17:12:01.250806",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "aurora-m/aurora-m-biden-harris-redteamed": {
      "model_id": "aurora-m/aurora-m-biden-harris-redteamed",
      "model_name": "aurora-m/aurora-m-biden-harris-redteamed",
      "size_bytes": 15479707648,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:12:02.629253",
      "end_time": "2026-01-21T17:12:02.693385",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMoE-1B-7B-0125-Instruct": {
      "model_id": "allenai/OLMoE-1B-7B-0125-Instruct",
      "model_name": "allenai/OLMoE-1B-7B-0125-Instruct",
      "size_bytes": 6919161856,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:11:16.808710",
      "end_time": "2026-01-21T17:12:32.593480",
      "duration_seconds": 75.78,
      "vllm_pid": 761147,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--allenai--OLMoE-1B-7B-0125-Instruct/snapshots/b89a7c4bc24fb9e55ce2543c9458ce0ca5c4650e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 20.03,
        "test_execution_seconds": null,
        "total_seconds": 75.78
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "medicalai/MedFound-7B": {
      "model_id": "medicalai/MedFound-7B",
      "model_name": "medicalai/MedFound-7B",
      "size_bytes": 7069016064,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:12:33.161066",
      "end_time": "2026-01-21T17:12:33.195889",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance/Ouro-2.6B-Thinking": {
      "model_id": "ByteDance/Ouro-2.6B-Thinking",
      "model_name": "ByteDance/Ouro-2.6B-Thinking",
      "size_bytes": 5337184273,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:40.348272",
      "end_time": "2026-01-21T17:12:32.836944",
      "duration_seconds": 112.49,
      "vllm_pid": 759261,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ByteDance--Ouro-2.6B-Thinking/snapshots/dafed72e123c3a1fb0ca03bfeab492cc8ad3a003` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 112.49
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/AFM-4.5B": {
      "model_id": "arcee-ai/AFM-4.5B",
      "model_name": "arcee-ai/AFM-4.5B",
      "size_bytes": 4619189760,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:38.370413",
      "end_time": "2026-01-21T17:12:34.670374",
      "duration_seconds": 116.3,
      "vllm_pid": 759257,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--arcee-ai--AFM-4.5B/snapshots/e84d19711eca5817c1608cb344d931bf05c966dd` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 116.3
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Instruct-2507-FP8": {
      "model_id": "Qwen/Qwen3-4B-Instruct-2507-FP8",
      "model_name": "Qwen/Qwen3-4B-Instruct-2507-FP8",
      "size_bytes": 4411646016,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:38.403504",
      "end_time": "2026-01-21T17:12:32.837000",
      "duration_seconds": 114.43,
      "vllm_pid": 759251,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--Qwen--Qwen3-4B-Instruct-2507-FP8/snapshots/8591804019c8b22094c3b5b4454e0edc05dffc98` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 114.43
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-micro": {
      "model_id": "ibm-granite/granite-4.0-h-micro",
      "model_name": "ibm-granite/granite-4.0-h-micro",
      "size_bytes": 3191396096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:38.370384",
      "end_time": "2026-01-21T17:12:33.178006",
      "duration_seconds": 114.81,
      "vllm_pid": 759262,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-h-micro/snapshots/d5f01a3ea75f088947be3aae039f4ad52837dfde` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 114.81
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1": {
      "model_id": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
      "model_name": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
      "size_bytes": 4512746496,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:38.372643",
      "end_time": "2026-01-21T17:12:32.837841",
      "duration_seconds": 114.47,
      "vllm_pid": 759250,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--nvidia--Llama-3.1-Nemotron-Nano-4B-v1.1/snapshots/d552708a9d575fa8d4a690b988fd870d65279f98` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 114.47
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance/Ouro-2.6B": {
      "model_id": "ByteDance/Ouro-2.6B",
      "model_name": "ByteDance/Ouro-2.6B",
      "size_bytes": 5336871369,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:40.326232",
      "end_time": "2026-01-21T17:12:32.836996",
      "duration_seconds": 112.51,
      "vllm_pid": 759258,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ByteDance--Ouro-2.6B/snapshots/514f6e4eeebd78600d4708993e8ea1021e038aec` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 112.51
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-h-tiny-base": {
      "model_id": "ibm-granite/granite-4.0-h-tiny-base",
      "model_name": "ibm-granite/granite-4.0-h-tiny-base",
      "size_bytes": 6939037248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:12:32.840898",
      "end_time": "2026-01-21T17:12:32.870865",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit": {
      "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit",
      "model_name": "unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit",
      "size_bytes": 15092615444,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:11:45.528621",
      "end_time": "2026-01-21T17:12:52.779333",
      "duration_seconds": 67.25,
      "vllm_pid": 129228,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit/snapshots/13ae1648fa81751417bd1701a3e670c70eaa0320` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 15.03,
        "test_execution_seconds": null,
        "total_seconds": 67.25
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-4.0-tiny-preview": {
      "model_id": "ibm-granite/granite-4.0-tiny-preview",
      "model_name": "ibm-granite/granite-4.0-tiny-preview",
      "size_bytes": 6671539776,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:40.843897",
      "end_time": "2026-01-21T17:12:54.925501",
      "duration_seconds": 134.08,
      "vllm_pid": 120725,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ibm-granite--granite-4.0-tiny-preview/snapshots/4ec5f963443b2e15e9be82595289e96ff082e280` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 80.05,
        "test_execution_seconds": null,
        "total_seconds": 134.08
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cfahlgren1/natural-functions": {
      "model_id": "cfahlgren1/natural-functions",
      "model_name": "cfahlgren1/natural-functions",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:12:54.935032",
      "end_time": "2026-01-21T17:12:54.981831",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts": {
      "model_id": "AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts",
      "model_name": "AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts",
      "size_bytes": 5977924584,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:10:40.599245",
      "end_time": "2026-01-21T17:12:56.878453",
      "duration_seconds": 136.28,
      "vllm_pid": 118531,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--AmanPriyanshu--gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts/snapshots/a1599a4d8b40849c8dd0c676b681b90a05954091` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 80.05,
        "test_execution_seconds": null,
        "total_seconds": 136.28
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "secemp9/TraceBack-12b": {
      "model_id": "secemp9/TraceBack-12b",
      "model_name": "secemp9/TraceBack-12b",
      "size_bytes": 6966370743,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:12:32.841250",
      "end_time": "2026-01-21T17:13:08.094699",
      "duration_seconds": 35.25,
      "vllm_pid": 765796,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=765796)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/qbw/models--secemp9--TraceBack-12b/snapshots/76fc84a5ce6a0a20f2267e26e5fd74c7c7d97395' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.model = self._init_model(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 611, in _init_model\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     return LlamaModel(vllm_config=vllm_config, prefix=prefix, layer_type=layer_type)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 393, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 395, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     lambda prefix: layer_type(vllm_config=vllm_config, prefix=prefix),\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 302, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.self_attn = LlamaAttention(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=765915)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W121 17:12:56.544971099 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=765796)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 35.25
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "instruction-pretrain/instruction-synthesizer": {
      "model_id": "instruction-pretrain/instruction-synthesizer",
      "model_name": "instruction-pretrain/instruction-synthesizer",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:13:08.113243",
      "end_time": "2026-01-21T17:13:08.147982",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xLAM-7b-fc-r": {
      "model_id": "Salesforce/xLAM-7b-fc-r",
      "model_name": "Salesforce/xLAM-7b-fc-r",
      "size_bytes": 6910365696,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:11:15.529620",
      "end_time": "2026-01-21T17:13:48.437600",
      "duration_seconds": 152.91,
      "vllm_pid": 127832,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Salesforce--xLAM-7b-fc-r/snapshots/42587ba3ff49d92c1b37e23f9c651ff753f60742` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Salesforce--xLAM-7b-fc-r/snapshots/42587ba3ff49d92c1b37e23f9c651ff753f60742` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 45.05,
        "test_execution_seconds": 81.16,
        "total_seconds": 152.91
      },
      "gpu_memory_gb": 352.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-4.0-h-tiny": {
      "model_id": "ibm-granite/granite-4.0-h-tiny",
      "model_name": "ibm-granite/granite-4.0-h-tiny",
      "size_bytes": 6939037248,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:32.597567",
      "end_time": "2026-01-21T17:14:20.636118",
      "duration_seconds": 108.04,
      "vllm_pid": 765790,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--ibm-granite--granite-4.0-h-tiny/snapshots/791e0d3d28c86e106c9b6e0b4cecdee0375b6124` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 108.04
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/wildguard": {
      "model_id": "allenai/wildguard",
      "model_name": "allenai/wildguard",
      "size_bytes": 7248031744,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:14:21.094788",
      "end_time": "2026-01-21T17:14:21.143521",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Salesforce/xLAM-7b-r": {
      "model_id": "Salesforce/xLAM-7b-r",
      "model_name": "Salesforce/xLAM-7b-r",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:33.250847",
      "end_time": "2026-01-21T17:14:21.300151",
      "duration_seconds": 108.05,
      "vllm_pid": 765801,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--Salesforce--xLAM-7b-r/snapshots/c07be3dc558bdb1dcc51c1d06b030131c3d3dcea` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 108.05
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-7B-Instruct-v0.2": {
      "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
      "model_name": "mistralai/Mistral-7B-Instruct-v0.2",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:13:08.156096",
      "end_time": "2026-01-21T17:14:21.091428",
      "duration_seconds": 72.94,
      "vllm_pid": 767557,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/63a8b081895390a26e140280378bc85ec8bce07a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 25.05,
        "test_execution_seconds": null,
        "total_seconds": 72.94
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Navid-AI/Yehia-7B-preview": {
      "model_id": "Navid-AI/Yehia-7B-preview",
      "model_name": "Navid-AI/Yehia-7B-preview",
      "size_bytes": 7000559616,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:32.842034",
      "end_time": "2026-01-21T17:14:20.807273",
      "duration_seconds": 107.97,
      "vllm_pid": 765795,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Navid-AI--Yehia-7B-preview/snapshots/441238d528c05538f0b5e1126873d7bef5ad7563` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 107.97
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "norallm/normistral-7b-warm": {
      "model_id": "norallm/normistral-7b-warm",
      "model_name": "norallm/normistral-7b-warm",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:14:20.810983",
      "end_time": "2026-01-21T17:14:20.884110",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "humain-ai/ALLaM-7B-Instruct-preview": {
      "model_id": "humain-ai/ALLaM-7B-Instruct-preview",
      "model_name": "humain-ai/ALLaM-7B-Instruct-preview",
      "size_bytes": 7000559616,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:32.842472",
      "end_time": "2026-01-21T17:14:21.841574",
      "duration_seconds": 109.0,
      "vllm_pid": 765802,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--humain-ai--ALLaM-7B-Instruct-preview/snapshots/a28dd1e67420cde72d3629c8633a974cf7d9c366` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 109.0
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-Mamba-7B-Base": {
      "model_id": "tiiuae/Falcon3-Mamba-7B-Base",
      "model_name": "tiiuae/Falcon3-Mamba-7B-Base",
      "size_bytes": 7272665088,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:14:21.999859",
      "end_time": "2026-01-21T17:14:22.062356",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "aws-prototyping/MegaBeam-Mistral-7B-512k": {
      "model_id": "aws-prototyping/MegaBeam-Mistral-7B-512k",
      "model_name": "aws-prototyping/MegaBeam-Mistral-7B-512k",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:34.675313",
      "end_time": "2026-01-21T17:14:22.554588",
      "duration_seconds": 107.88,
      "vllm_pid": 765808,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--aws-prototyping--MegaBeam-Mistral-7B-512k/snapshots/7a9bfb11b5d5e56eae0e4b52290d705cdd55f7a6` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 107.88
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "scb10x/typhoon-7b": {
      "model_id": "scb10x/typhoon-7b",
      "model_name": "scb10x/typhoon-7b",
      "size_bytes": 7268102144,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:14:21.845471",
      "end_time": "2026-01-21T17:14:21.883265",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "GritLM/GritLM-7B": {
      "model_id": "GritLM/GritLM-7B",
      "model_name": "GritLM/GritLM-7B",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:33.183078",
      "end_time": "2026-01-21T17:14:22.814849",
      "duration_seconds": 109.63,
      "vllm_pid": 765800,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--GritLM--GritLM-7B/snapshots/138192cc56441e22a92f37ca05bdfff64ec3d83f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 109.63
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenLLM-France/Lucie-7B": {
      "model_id": "OpenLLM-France/Lucie-7B",
      "model_name": "OpenLLM-France/Lucie-7B",
      "size_bytes": 6706960384,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:11:00.520415",
      "end_time": "2026-01-21T17:14:35.510289",
      "duration_seconds": 214.99,
      "vllm_pid": 126256,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--OpenLLM-France--Lucie-7B/snapshots/43663a8d41659e45b143d2677b572b0a8d85b9f7` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--OpenLLM-France--Lucie-7B/snapshots/43663a8d41659e45b143d2677b572b0a8d85b9f7` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 127.79,
        "total_seconds": 214.99
      },
      "gpu_memory_gb": 689.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "dmis-lab/meerkat-7b-v1.0": {
      "model_id": "dmis-lab/meerkat-7b-v1.0",
      "model_name": "dmis-lab/meerkat-7b-v1.0",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:54.988983",
      "end_time": "2026-01-21T17:14:38.974946",
      "duration_seconds": 103.99,
      "vllm_pid": 133114,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--dmis-lab--meerkat-7b-v1.0/snapshots/e3ea5ec3a4bb0b718e822878a8c01873b01d49a2` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": null,
        "total_seconds": 103.99
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Moonlight-16B-A3B": {
      "model_id": "moonshotai/Moonlight-16B-A3B",
      "model_name": "moonshotai/Moonlight-16B-A3B",
      "size_bytes": 15960111936,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:52.835044",
      "end_time": "2026-01-21T17:14:40.608340",
      "duration_seconds": 107.77,
      "vllm_pid": 133069,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--moonshotai--Moonlight-16B-A3B/snapshots/ce8bc137e6e29c3b7540ebdd515bbc5bdb20d915` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 107.77
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/LLaDA-MoE-7B-A1B-Base": {
      "model_id": "inclusionAI/LLaDA-MoE-7B-A1B-Base",
      "model_name": "inclusionAI/LLaDA-MoE-7B-A1B-Base",
      "size_bytes": 7356880896,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:14:39.044443",
      "end_time": "2026-01-21T17:14:54.217845",
      "duration_seconds": 15.17,
      "vllm_pid": 138502,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=138502)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m   Value error, Model architectures ['LLaDAMoEModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=138502)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.17
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/Genstruct-7B": {
      "model_id": "NousResearch/Genstruct-7B",
      "model_name": "NousResearch/Genstruct-7B",
      "size_bytes": 7241740288,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:13:48.542586",
      "end_time": "2026-01-21T17:15:32.276513",
      "duration_seconds": 103.73,
      "vllm_pid": 135604,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"'dict object' has no attribute 'title' 'dict object' has no attribute 'title'\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": null,
        "total_seconds": 103.73
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Rakuten/RakutenAI-7B": {
      "model_id": "Rakuten/RakutenAI-7B",
      "model_name": "Rakuten/RakutenAI-7B",
      "size_bytes": 7372804096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:15:32.349903",
      "end_time": "2026-01-21T17:15:32.373843",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "icefog72/IceMoonshineRP-7b": {
      "model_id": "icefog72/IceMoonshineRP-7b",
      "model_name": "icefog72/IceMoonshineRP-7b",
      "size_bytes": 7241732096,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:56.935986",
      "end_time": "2026-01-21T17:15:33.871746",
      "duration_seconds": 156.94,
      "vllm_pid": 133119,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--icefog72--IceMoonshineRP-7b/snapshots/7cd34ae4042c79101d04d1057b06247faf2aabfe` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--icefog72--IceMoonshineRP-7b/snapshots/7cd34ae4042c79101d04d1057b06247faf2aabfe` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 92.76,
        "total_seconds": 156.94
      },
      "gpu_memory_gb": 549.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Rakuten/RakutenAI-7B-instruct": {
      "model_id": "Rakuten/RakutenAI-7B-instruct",
      "model_name": "Rakuten/RakutenAI-7B-instruct",
      "size_bytes": 7372804096,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:15:33.650544",
      "end_time": "2026-01-21T17:15:33.690549",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kakaocorp/kanana-1.5-15.7b-a3b-instruct": {
      "model_id": "kakaocorp/kanana-1.5-15.7b-a3b-instruct",
      "model_name": "kakaocorp/kanana-1.5-15.7b-a3b-instruct",
      "size_bytes": 15696279552,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:12:03.055428",
      "end_time": "2026-01-21T17:15:49.951124",
      "duration_seconds": 226.9,
      "vllm_pid": 130283,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--kakaocorp--kanana-1.5-15.7b-a3b-instruct/snapshots/df9d415c38e198149a65957cb4dec6cf2a7ba162` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--kakaocorp--kanana-1.5-15.7b-a3b-instruct/snapshots/df9d415c38e198149a65957cb4dec6cf2a7ba162` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 105.06,
        "test_execution_seconds": 97.26,
        "total_seconds": 226.9
      },
      "gpu_memory_gb": 548.52,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "aaditya/Llama3-OpenBioLLM-8B": {
      "model_id": "aaditya/Llama3-OpenBioLLM-8B",
      "model_name": "aaditya/Llama3-OpenBioLLM-8B",
      "size_bytes": 16060627227,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T17:15:50.051874",
      "end_time": "2026-01-21T17:15:50.056155",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-small-128k-instruct": {
      "model_id": "microsoft/Phi-3-small-128k-instruct",
      "model_name": "microsoft/Phi-3-small-128k-instruct",
      "size_bytes": 7392272384,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:15:33.950252",
      "end_time": "2026-01-21T17:15:54.125720",
      "duration_seconds": 20.18,
      "vllm_pid": 140801,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=140801)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m   Value error, Model architecture Phi3SmallForCausalLM was supported in vLLM until v0.9.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=140801)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.18
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openGPT-X/Teuken-7B-instruct-research-v0.4": {
      "model_id": "openGPT-X/Teuken-7B-instruct-research-v0.4",
      "model_name": "openGPT-X/Teuken-7B-instruct-research-v0.4",
      "size_bytes": 7452725248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:15:54.151639",
      "end_time": "2026-01-21T17:15:54.221899",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-3-small-8k-instruct": {
      "model_id": "microsoft/Phi-3-small-8k-instruct",
      "model_name": "microsoft/Phi-3-small-8k-instruct",
      "size_bytes": 7392274432,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:15:34.949734",
      "end_time": "2026-01-21T17:15:55.229627",
      "duration_seconds": 20.28,
      "vllm_pid": 140803,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=140803)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m   Value error, Model architecture Phi3SmallForCausalLM was supported in vLLM until v0.9.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=140803)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.28
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/LLaDA2.0-mini-preview": {
      "model_id": "inclusionAI/LLaDA2.0-mini-preview",
      "model_name": "inclusionAI/LLaDA2.0-mini-preview",
      "size_bytes": 16255643392,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:15:50.061919",
      "end_time": "2026-01-21T17:16:05.393525",
      "duration_seconds": 15.33,
      "vllm_pid": 141648,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=141648)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m   Value error, Model architectures ['LLaDA2MoeModelLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=141648)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.33
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "norallm/normistral-7b-warm-instruct": {
      "model_id": "norallm/normistral-7b-warm-instruct",
      "model_name": "norallm/normistral-7b-warm-instruct",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:20.936016",
      "end_time": "2026-01-21T17:16:08.673086",
      "duration_seconds": 107.74,
      "vllm_pid": 771172,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--norallm--normistral-7b-warm-instruct/snapshots/85fba0b42363f6ebbb15e627cb31fc082b8ca226` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 107.74
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KurmaAI/AQUA-7B": {
      "model_id": "KurmaAI/AQUA-7B",
      "model_name": "KurmaAI/AQUA-7B",
      "size_bytes": 7248023552,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:20.640033",
      "end_time": "2026-01-21T17:16:08.776502",
      "duration_seconds": 108.14,
      "vllm_pid": 771165,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--KurmaAI--AQUA-7B/snapshots/412343ca3d4fcabb5de170211cc9dc856574365c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.07,
        "test_execution_seconds": null,
        "total_seconds": 108.14
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "skt/A.X-4.0-Light": {
      "model_id": "skt/A.X-4.0-Light",
      "model_name": "skt/A.X-4.0-Light",
      "size_bytes": 7259624960,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:21.195815",
      "end_time": "2026-01-21T17:16:09.399042",
      "duration_seconds": 108.2,
      "vllm_pid": 771173,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--skt--A.X-4.0-Light/snapshots/ba21c20ea1b31ded1ec3e2fb432335077dc4be98` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 108.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "skt/A.X-3.1-Light": {
      "model_id": "skt/A.X-3.1-Light",
      "model_name": "skt/A.X-3.1-Light",
      "size_bytes": 7264800768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:21.304316",
      "end_time": "2026-01-21T17:16:09.614411",
      "duration_seconds": 108.31,
      "vllm_pid": 771174,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--skt--A.X-3.1-Light/snapshots/9b41bb2406472634d8812c0b8931fa40fa9a6c3a` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 108.31
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ilsp/Meltemi-7B-v1.5": {
      "model_id": "ilsp/Meltemi-7B-v1.5",
      "model_name": "ilsp/Meltemi-7B-v1.5",
      "size_bytes": 7482445824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:16:08.780115",
      "end_time": "2026-01-21T17:16:09.147247",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-Mamba-7B-Instruct": {
      "model_id": "tiiuae/Falcon3-Mamba-7B-Instruct",
      "model_name": "tiiuae/Falcon3-Mamba-7B-Instruct",
      "size_bytes": 7272665088,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:22.107865",
      "end_time": "2026-01-21T17:16:09.974381",
      "duration_seconds": 107.87,
      "vllm_pid": 771176,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--tiiuae--Falcon3-Mamba-7B-Instruct/snapshots/79268d5c8e650ec0ec24aad2729bfc906f569580` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 107.87
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ilsp/Meltemi-7B-v1": {
      "model_id": "ilsp/Meltemi-7B-v1",
      "model_name": "ilsp/Meltemi-7B-v1",
      "size_bytes": 7483559936,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:16:09.617924",
      "end_time": "2026-01-21T17:16:09.895262",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-1124-7B-Instruct": {
      "model_id": "allenai/OLMo-2-1124-7B-Instruct",
      "model_name": "allenai/OLMo-2-1124-7B-Instruct",
      "size_bytes": 7298617344,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:22.559377",
      "end_time": "2026-01-21T17:16:10.209465",
      "duration_seconds": 107.65,
      "vllm_pid": 771188,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--allenai--OLMo-2-1124-7B-Instruct/snapshots/470b1fba1ae01581f270116362ee4aa1b97f4c84` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": null,
        "total_seconds": 107.65
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Infinigence/Megrez2-3x7B-A3B": {
      "model_id": "Infinigence/Megrez2-3x7B-A3B",
      "model_name": "Infinigence/Megrez2-3x7B-A3B",
      "size_bytes": 7474249600,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:15:55.251722",
      "end_time": "2026-01-21T17:16:10.403821",
      "duration_seconds": 15.15,
      "vllm_pid": 142050,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=142050)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m   Value error, Model architectures ['MegrezMoeForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=142050)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.15
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Zyphra/Zamba2-7B-Instruct": {
      "model_id": "Zyphra/Zamba2-7B-Instruct",
      "model_name": "Zyphra/Zamba2-7B-Instruct",
      "size_bytes": 7356749648,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:35.544242",
      "end_time": "2026-01-21T17:16:26.875626",
      "duration_seconds": 111.33,
      "vllm_pid": 138486,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "'NoneType' object is not iterable\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 106, in generate\n    choices = self.chat_choices_from_completion(completion, tools)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 146, in chat_choices_from_completion\n    return chat_choices_from_openai(completion, tools)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 534, in chat_choices_from_openai\n    choices = list(response.choices)\n              ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not iterable\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 5.17,
        "total_seconds": 111.33
      },
      "gpu_memory_gb": 514.86,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2-7b-reward": {
      "model_id": "internlm/internlm2-7b-reward",
      "model_name": "internlm/internlm2-7b-reward",
      "size_bytes": 7358652416,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:54.252486",
      "end_time": "2026-01-21T17:16:27.348848",
      "duration_seconds": 93.1,
      "vllm_pid": 139120,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": null,
        "total_seconds": 93.1
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "moonshotai/Moonlight-16B-A3B-Instruct": {
      "model_id": "moonshotai/Moonlight-16B-A3B-Instruct",
      "model_name": "moonshotai/Moonlight-16B-A3B-Instruct",
      "size_bytes": 15960111936,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:14:40.644741",
      "end_time": "2026-01-21T17:16:29.817356",
      "duration_seconds": 109.17,
      "vllm_pid": 138505,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 70.04,
        "test_execution_seconds": null,
        "total_seconds": 109.17
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "aisingapore/SEA-LION-v1-7B-IT": {
      "model_id": "aisingapore/SEA-LION-v1-7B-IT",
      "model_name": "aisingapore/SEA-LION-v1-7B-IT",
      "size_bytes": 7501651968,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:16:09.953215",
      "end_time": "2026-01-21T17:16:30.147399",
      "duration_seconds": 20.19,
      "vllm_pid": 778342,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=778342)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 616, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     return get_class_in_module(class_name, final_module, force_reload=force_download)\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 299, in get_class_in_module\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     module_files: list[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 166, in get_relative_import_files\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     new_imports.extend(get_relative_imports(f))\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 135, in get_relative_imports\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m     with open(module_file, encoding=\"utf-8\") as f:\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=778342)\u001b[0;0m FileNotFoundError: [Errno 2] No such file or directory: '/home/secure/.cache/huggingface/modules/transformers_modules/_47a9f060b1c3c8f9344a489fbad042d1fc3edd71/flash_attn_triton.py'\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.19
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "starvector/starvector-8b-im2svg": {
      "model_id": "starvector/starvector-8b-im2svg",
      "model_name": "starvector/starvector-8b-im2svg",
      "size_bytes": 7507080192,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:16:30.151630",
      "end_time": "2026-01-21T17:16:30.230728",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "trillionlabs/Trillion-7B-preview": {
      "model_id": "trillionlabs/Trillion-7B-preview",
      "model_name": "trillionlabs/Trillion-7B-preview",
      "size_bytes": 7525896192,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:30.235202",
      "end_time": "2026-01-21T17:17:23.039083",
      "duration_seconds": 52.8,
      "vllm_pid": 779130,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 45.04,
        "test_execution_seconds": null,
        "total_seconds": 52.8
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-mini-2.0": {
      "model_id": "inclusionAI/Ling-mini-2.0",
      "model_name": "inclusionAI/Ling-mini-2.0",
      "size_bytes": 16255643392,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:05.452797",
      "end_time": "2026-01-21T17:17:33.673355",
      "duration_seconds": 88.22,
      "vllm_pid": 142853,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--inclusionAI--Ling-mini-2.0/snapshots/ae2925e082ef9e311fbbb01f2720006611bbdb69` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 40.04,
        "test_execution_seconds": null,
        "total_seconds": 88.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ilsp/Meltemi-7B-Instruct-v1": {
      "model_id": "ilsp/Meltemi-7B-Instruct-v1",
      "model_name": "ilsp/Meltemi-7B-Instruct-v1",
      "size_bytes": 7483559936,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:09.402692",
      "end_time": "2026-01-21T17:17:35.115392",
      "duration_seconds": 85.71,
      "vllm_pid": 778339,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.04,
        "test_execution_seconds": null,
        "total_seconds": 85.71
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "internlm/internlm2_5-1_8b": {
      "model_id": "internlm/internlm2_5-1_8b",
      "model_name": "internlm/internlm2_5-1_8b",
      "size_bytes": 7557992048,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:17:35.118659",
      "end_time": "2026-01-21T17:17:35.128195",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ilsp/Meltemi-7B-Instruct-v1.5": {
      "model_id": "ilsp/Meltemi-7B-Instruct-v1.5",
      "model_name": "ilsp/Meltemi-7B-Instruct-v1.5",
      "size_bytes": 7482445824,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:08.679798",
      "end_time": "2026-01-21T17:17:44.645492",
      "duration_seconds": 95.97,
      "vllm_pid": 778323,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.34,
        "test_execution_seconds": null,
        "total_seconds": 95.97
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kyutai/hibiki-1b-pytorch-bf16": {
      "model_id": "kyutai/hibiki-1b-pytorch-bf16",
      "model_name": "kyutai/hibiki-1b-pytorch-bf16",
      "size_bytes": 7585194406,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:17:44.649152",
      "end_time": "2026-01-21T17:17:44.710483",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-MT-Chimera-7B-fp8": {
      "model_id": "tencent/Hunyuan-MT-Chimera-7B-fp8",
      "model_name": "tencent/Hunyuan-MT-Chimera-7B-fp8",
      "size_bytes": 7504933312,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:26.954490",
      "end_time": "2026-01-21T17:17:48.914901",
      "duration_seconds": 81.96,
      "vllm_pid": 143716,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "'NoneType' object is not iterable\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 106, in generate\n    choices = self.chat_choices_from_completion(completion, tools)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 146, in chat_choices_from_completion\n    return chat_choices_from_openai(completion, tools)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 534, in chat_choices_from_openai\n    choices = list(response.choices)\n              ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not iterable\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 17.47,
        "total_seconds": 81.96
      },
      "gpu_memory_gb": 543.23,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "allenai/Llama-3.1-Tulu-3-8B-RM": {
      "model_id": "allenai/Llama-3.1-Tulu-3-8B-RM",
      "model_name": "allenai/Llama-3.1-Tulu-3-8B-RM",
      "size_bytes": 7504961536,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:27.354435",
      "end_time": "2026-01-21T17:17:50.342130",
      "duration_seconds": 82.99,
      "vllm_pid": 143717,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": null,
        "total_seconds": 82.99
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/Skywork-Reward-V2-Llama-3.1-8B": {
      "model_id": "Skywork/Skywork-Reward-V2-Llama-3.1-8B",
      "model_name": "Skywork/Skywork-Reward-V2-Llama-3.1-8B",
      "size_bytes": 7504928768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:10.017147",
      "end_time": "2026-01-21T17:17:52.899202",
      "duration_seconds": 102.88,
      "vllm_pid": 778344,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.1,
        "test_execution_seconds": null,
        "total_seconds": 102.88
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-7B-Instruct": {
      "model_id": "tencent/Hunyuan-7B-Instruct",
      "model_name": "tencent/Hunyuan-7B-Instruct",
      "size_bytes": 7504568320,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:09.977597",
      "end_time": "2026-01-21T17:17:54.923634",
      "duration_seconds": 104.95,
      "vllm_pid": 778343,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.04,
        "test_execution_seconds": null,
        "total_seconds": 104.95
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/Skywork-Reward-V2-Llama-3.1-8B-40M": {
      "model_id": "Skywork/Skywork-Reward-V2-Llama-3.1-8B-40M",
      "model_name": "Skywork/Skywork-Reward-V2-Llama-3.1-8B-40M",
      "size_bytes": 7504928768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:10.212945",
      "end_time": "2026-01-21T17:18:03.372351",
      "duration_seconds": 113.16,
      "vllm_pid": 778345,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.17,
        "test_execution_seconds": null,
        "total_seconds": 113.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-mini-2.0": {
      "model_id": "inclusionAI/Ring-mini-2.0",
      "model_name": "inclusionAI/Ring-mini-2.0",
      "size_bytes": 16255643392,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:29.855243",
      "end_time": "2026-01-21T17:18:10.614194",
      "duration_seconds": 100.76,
      "vllm_pid": 143773,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Connection error.",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": null,
        "total_seconds": 100.76
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tencent/Hunyuan-MT-7B-fp8": {
      "model_id": "tencent/Hunyuan-MT-7B-fp8",
      "model_name": "tencent/Hunyuan-MT-7B-fp8",
      "size_bytes": 7504933312,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:16:10.452921",
      "end_time": "2026-01-21T17:18:17.488353",
      "duration_seconds": 127.04,
      "vllm_pid": 142981,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--tencent--Hunyuan-MT-7B-fp8/snapshots/81e5a3f7199524570ba75e61360e990ba88665e4` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--tencent--Hunyuan-MT-7B-fp8/snapshots/81e5a3f7199524570ba75e61360e990ba88665e4` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 35.04,
        "test_execution_seconds": 43.79,
        "total_seconds": 127.04
      },
      "gpu_memory_gb": 318.06,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "trillionlabs/Tri-7B": {
      "model_id": "trillionlabs/Tri-7B",
      "model_name": "trillionlabs/Tri-7B",
      "size_bytes": 7526944768,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:17:23.052285",
      "end_time": "2026-01-21T17:19:04.972124",
      "duration_seconds": 101.92,
      "vllm_pid": 781984,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "'NoneType' object is not iterable\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 106, in generate\n    choices = self.chat_choices_from_completion(completion, tools)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 146, in chat_choices_from_completion\n    return chat_choices_from_openai(completion, tools)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/utils/openai.py\", line 534, in chat_choices_from_openai\n    choices = list(response.choices)\n              ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not iterable\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 15.78,
        "total_seconds": 101.92
      },
      "gpu_memory_gb": 1013.96,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HoangHa/Pensez-v0.1-e5": {
      "model_id": "HoangHa/Pensez-v0.1-e5",
      "model_name": "HoangHa/Pensez-v0.1-e5",
      "size_bytes": 7612770816,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:17:52.903507",
      "end_time": "2026-01-21T17:19:25.558290",
      "duration_seconds": 92.65,
      "vllm_pid": 783176,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 45.04,
        "test_execution_seconds": null,
        "total_seconds": 92.65
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Skywork/Skywork-Reward-V2-Qwen3-8B": {
      "model_id": "Skywork/Skywork-Reward-V2-Qwen3-8B",
      "model_name": "Skywork/Skywork-Reward-V2-Qwen3-8B",
      "size_bytes": 7568409600,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:17:35.136133",
      "end_time": "2026-01-21T17:19:27.669052",
      "duration_seconds": 112.53,
      "vllm_pid": 782686,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 65.04,
        "test_execution_seconds": null,
        "total_seconds": 112.53
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "AI-MO/Kimina-Prover-Preview-Distill-7B": {
      "model_id": "AI-MO/Kimina-Prover-Preview-Distill-7B",
      "model_name": "AI-MO/Kimina-Prover-Preview-Distill-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:17:54.927324",
      "end_time": "2026-01-21T17:19:30.184857",
      "duration_seconds": 95.26,
      "vllm_pid": 783209,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 45.04,
        "test_execution_seconds": null,
        "total_seconds": 95.26
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openbmb/MiniCPM3-4B": {
      "model_id": "openbmb/MiniCPM3-4B",
      "model_name": "openbmb/MiniCPM3-4B",
      "size_bytes": 16297031550,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:18:10.665754",
      "end_time": "2026-01-21T17:19:33.899278",
      "duration_seconds": 83.23,
      "vllm_pid": 150528,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--openbmb--MiniCPM3-4B/snapshots/d6b14ddaefdb11c624dd75c3c779549bc90b08cb` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 35.04,
        "test_execution_seconds": null,
        "total_seconds": 83.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/BFS-Prover-V1-7B": {
      "model_id": "ByteDance-Seed/BFS-Prover-V1-7B",
      "model_name": "ByteDance-Seed/BFS-Prover-V1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:18:17.567237",
      "end_time": "2026-01-21T17:19:35.456539",
      "duration_seconds": 77.89,
      "vllm_pid": 151090,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--ByteDance-Seed--BFS-Prover-V1-7B/snapshots/750e39030cf25f4af4fcdc81d358123659656cbe` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 30.04,
        "test_execution_seconds": null,
        "total_seconds": 77.89
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Dream-org/Dream-Coder-v0-Instruct-7B": {
      "model_id": "Dream-org/Dream-Coder-v0-Instruct-7B",
      "model_name": "Dream-org/Dream-Coder-v0-Instruct-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:19:25.562877",
      "end_time": "2026-01-21T17:19:40.746504",
      "duration_seconds": 15.18,
      "vllm_pid": 789155,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=789155)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m   Value error, Model architectures ['DreamModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=789155)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.18
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Dream-org/Dream-v0-Base-7B": {
      "model_id": "Dream-org/Dream-v0-Base-7B",
      "model_name": "Dream-org/Dream-v0-Base-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:19:27.673193",
      "end_time": "2026-01-21T17:19:42.833917",
      "duration_seconds": 15.16,
      "vllm_pid": 789785,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=789785)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m   Value error, Model architectures ['DreamModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=789785)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon-H1-7B-Instruct": {
      "model_id": "tiiuae/Falcon-H1-7B-Instruct",
      "model_name": "tiiuae/Falcon-H1-7B-Instruct",
      "size_bytes": 7585654880,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:17:44.841209",
      "end_time": "2026-01-21T17:19:43.603231",
      "duration_seconds": 118.76,
      "vllm_pid": 783101,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": null,
        "total_seconds": 118.76
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Dream-org/Dream-v0-Instruct-7B": {
      "model_id": "Dream-org/Dream-v0-Instruct-7B",
      "model_name": "Dream-org/Dream-v0-Instruct-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:19:30.194652",
      "end_time": "2026-01-21T17:19:45.352940",
      "duration_seconds": 15.16,
      "vllm_pid": 789843,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=789843)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m   Value error, Model architectures ['DreamModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=789843)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MachineLearningLM/MachineLearningLM-7B-v1": {
      "model_id": "MachineLearningLM/MachineLearningLM-7B-v1",
      "model_name": "MachineLearningLM/MachineLearningLM-7B-v1",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T17:19:45.356453",
      "end_time": "2026-01-21T17:19:45.398177",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--MachineLearningLM--MachineLearningLM-7B-v1/snapshots/c16bf2d481f83e89f9ee8bb3a6bf1bba182b9319/model-00002-of-00004.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-mini-linear-2.0": {
      "model_id": "inclusionAI/Ring-mini-linear-2.0",
      "model_name": "inclusionAI/Ring-mini-linear-2.0",
      "size_bytes": 16423448320,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:19:33.975455",
      "end_time": "2026-01-21T17:19:49.188007",
      "duration_seconds": 15.21,
      "vllm_pid": 155425,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=155425)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m You are using a model of type bailing_moe_linear to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m   Value error, Model architectures ['BailingMoeLinearV2ForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=155425)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Dream-org/DreamOn-v0-7B": {
      "model_id": "Dream-org/DreamOn-v0-7B",
      "model_name": "Dream-org/DreamOn-v0-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:19:35.475240",
      "end_time": "2026-01-21T17:19:50.695763",
      "duration_seconds": 15.22,
      "vllm_pid": 155429,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=155429)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m   Value error, Model architectures ['DreamModel'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=155429)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Arcee-Maestro-7B-Preview": {
      "model_id": "arcee-ai/Arcee-Maestro-7B-Preview",
      "model_name": "arcee-ai/Arcee-Maestro-7B-Preview",
      "size_bytes": 7612756480,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:17:48.963043",
      "end_time": "2026-01-21T17:20:40.607306",
      "duration_seconds": 171.64,
      "vllm_pid": 149536,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--arcee-ai--Arcee-Maestro-7B-Preview/snapshots/dc3ee7b5dd2be00e7d2f5aaedb3212a5c183176b` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--arcee-ai--Arcee-Maestro-7B-Preview/snapshots/dc3ee7b5dd2be00e7d2f5aaedb3212a5c183176b` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 80.85,
        "total_seconds": 171.64
      },
      "gpu_memory_gb": 695.64,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "MiniMaxAI/SynLogic-7B": {
      "model_id": "MiniMaxAI/SynLogic-7B",
      "model_name": "MiniMaxAI/SynLogic-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:19:45.405512",
      "end_time": "2026-01-21T17:21:23.999274",
      "duration_seconds": 98.59,
      "vllm_pid": 790154,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--MiniMaxAI--SynLogic-7B/snapshots/bfcdfe2571549f8c40f1169cd62221abed5a5cbe` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": null,
        "total_seconds": 98.59
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "IIC/RigoChat-7b-v2": {
      "model_id": "IIC/RigoChat-7b-v2",
      "model_name": "IIC/RigoChat-7b-v2",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:19:42.837370",
      "end_time": "2026-01-21T17:21:25.562121",
      "duration_seconds": 102.72,
      "vllm_pid": 790060,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--IIC--RigoChat-7b-v2/snapshots/666a48562f47b0cc09aca23f1ae8df3d0b3ac25c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": null,
        "total_seconds": 102.72
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-lite": {
      "model_id": "inclusionAI/Ling-lite",
      "model_name": "inclusionAI/Ling-lite",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:19:49.276648",
      "end_time": "2026-01-21T17:21:32.100648",
      "duration_seconds": 102.82,
      "vllm_pid": 156031,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--inclusionAI--Ling-lite/snapshots/d80333a0f637caa11a37629af46e0c0cf387de45` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": null,
        "total_seconds": 102.82
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-lite-1.5": {
      "model_id": "inclusionAI/Ling-lite-1.5",
      "model_name": "inclusionAI/Ling-lite-1.5",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:21:32.186976",
      "end_time": "2026-01-21T17:22:34.788298",
      "duration_seconds": 62.6,
      "vllm_pid": 160644,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--inclusionAI--Ling-lite-1.5/snapshots/6b7572bd41f199f34ff6774adfb778619974d34c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 15.03,
        "test_execution_seconds": null,
        "total_seconds": 62.6
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Kortix/FastApply-7B-v1.0": {
      "model_id": "Kortix/FastApply-7B-v1.0",
      "model_name": "Kortix/FastApply-7B-v1.0",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:19:43.607268",
      "end_time": "2026-01-21T17:22:54.301707",
      "duration_seconds": 190.69,
      "vllm_pid": 790096,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Kortix--FastApply-7B-v1.0/snapshots/001c0cf463c5b14800c30f4f7cc9f754521493a8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Kortix--FastApply-7B-v1.0/snapshots/001c0cf463c5b14800c30f4f7cc9f754521493a8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.07,
        "test_execution_seconds": 86.36,
        "total_seconds": 190.69
      },
      "gpu_memory_gb": 1019.15,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "FreedomIntelligence/HuatuoGPT-o1-7B": {
      "model_id": "FreedomIntelligence/HuatuoGPT-o1-7B",
      "model_name": "FreedomIntelligence/HuatuoGPT-o1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:19:40.750375",
      "end_time": "2026-01-21T17:23:04.417304",
      "duration_seconds": 203.67,
      "vllm_pid": 790030,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--FreedomIntelligence--HuatuoGPT-o1-7B/snapshots/6487c8071a3f87702a6816ba7a09f38921b685ed` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--FreedomIntelligence--HuatuoGPT-o1-7B/snapshots/6487c8071a3f87702a6816ba7a09f38921b685ed` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 109.23,
        "total_seconds": 203.67
      },
      "gpu_memory_gb": 1007.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PhysicsWallahAI/Aryabhata-1.0": {
      "model_id": "PhysicsWallahAI/Aryabhata-1.0",
      "model_name": "PhysicsWallahAI/Aryabhata-1.0",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:21:24.004662",
      "end_time": "2026-01-21T17:23:14.429605",
      "duration_seconds": 110.42,
      "vllm_pid": 795663,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--PhysicsWallahAI--Aryabhata-1.0/snapshots/ffa583f164813296f774a44bf765e8ed51b63f22` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 110.42
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-7B-Instruct-1M": {
      "model_id": "Qwen/Qwen2.5-7B-Instruct-1M",
      "model_name": "Qwen/Qwen2.5-7B-Instruct-1M",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T17:23:04.421008",
      "end_time": "2026-01-21T17:23:34.640278",
      "duration_seconds": 30.22,
      "vllm_pid": 801163,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=801163)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.attn = attn_cls(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 230, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m     self.impl = impl_cls(\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=801344)\u001b[0;0m TypeError: FlashAttentionImpl.__init__() got an unexpected keyword argument 'layer_idx'\n[rank0]:[W121 17:23:25.307235041 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=801163)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "SakanaAI/RLT-7B": {
      "model_id": "SakanaAI/RLT-7B",
      "model_name": "SakanaAI/RLT-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:23:34.650248",
      "end_time": "2026-01-21T17:26:35.029859",
      "duration_seconds": 180.38,
      "vllm_pid": 802312,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--SakanaAI--RLT-7B/snapshots/8c31ec6ad93409b88deab5af9fe661020ed6645f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--SakanaAI--RLT-7B/snapshots/8c31ec6ad93409b88deab5af9fe661020ed6645f` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 30.04,
        "test_execution_seconds": 100.51,
        "total_seconds": 180.38
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen2.5-Coder-7B-Instruct": {
      "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "model_name": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:23:14.432919",
      "end_time": "2026-01-21T17:26:42.993604",
      "duration_seconds": 208.56,
      "vllm_pid": 801358,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen2.5-Coder-7B-Instruct/snapshots/c03e6d358207e414f1eca0bb1891e29f1db0e242` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Qwen--Qwen2.5-Coder-7B-Instruct/snapshots/c03e6d358207e414f1eca0bb1891e29f1db0e242` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 148.59,
        "total_seconds": 208.56
      },
      "gpu_memory_gb": 906.62,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "tiiuae/Falcon3-7B-Instruct": {
      "model_id": "tiiuae/Falcon3-7B-Instruct",
      "model_name": "tiiuae/Falcon3-7B-Instruct",
      "size_bytes": 7455550464,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:15:54.319403",
      "end_time": "2026-01-21T17:33:17.392796",
      "duration_seconds": 1043.07,
      "vllm_pid": 142041,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_172357",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 983.52,
        "total_seconds": 1043.07
      },
      "gpu_memory_gb": 1025.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "AITeamVN/Vi-Qwen2-7B-RAG": {
      "model_id": "AITeamVN/Vi-Qwen2-7B-RAG",
      "model_name": "AITeamVN/Vi-Qwen2-7B-RAG",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:18:03.377716",
      "end_time": "2026-01-21T17:35:03.353197",
      "duration_seconds": 1019.98,
      "vllm_pid": 784479,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_172735",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 960.73,
        "total_seconds": 1019.98
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "DeepHat/DeepHat-V1-7B": {
      "model_id": "DeepHat/DeepHat-V1-7B",
      "model_name": "DeepHat/DeepHat-V1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:19:04.981066",
      "end_time": "2026-01-21T17:37:58.629614",
      "duration_seconds": 1133.65,
      "vllm_pid": 788446,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_173600",
      "timing": {
        "server_start_seconds": 55.14,
        "test_execution_seconds": 1068.89,
        "total_seconds": 1133.65
      },
      "gpu_memory_gb": 1019.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored": {
      "model_id": "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored",
      "model_name": "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:19:50.776815",
      "end_time": "2026-01-21T17:40:58.067552",
      "duration_seconds": 1267.29,
      "vllm_pid": 156035,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_173410",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1207.43,
        "total_seconds": 1267.29
      },
      "gpu_memory_gb": 1025.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PokeeAI/pokee_research_7b": {
      "model_id": "PokeeAI/pokee_research_7b",
      "model_name": "PokeeAI/pokee_research_7b",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:21:25.572309",
      "end_time": "2026-01-21T17:43:13.928917",
      "duration_seconds": 1308.36,
      "vllm_pid": 795777,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_173852",
      "timing": {
        "server_start_seconds": 60.26,
        "test_execution_seconds": 1238.6,
        "total_seconds": 1308.36
      },
      "gpu_memory_gb": 1019.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "allenai/OLMo-2-1124-7B-Instruct-preview": {
      "model_id": "allenai/OLMo-2-1124-7B-Instruct-preview",
      "model_name": "allenai/OLMo-2-1124-7B-Instruct-preview",
      "size_bytes": 7298617344,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:14:22.819493",
      "end_time": "2026-01-21T17:43:25.572741",
      "duration_seconds": 1742.75,
      "vllm_pid": 771189,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_173852",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 1633.82,
        "total_seconds": 1742.75
      },
      "gpu_memory_gb": 1010.88,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen2.5-7B-Instruct": {
      "model_id": "Qwen/Qwen2.5-7B-Instruct",
      "model_name": "Qwen/Qwen2.5-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:22:54.306481",
      "end_time": "2026-01-21T17:44:00.641462",
      "duration_seconds": 1266.33,
      "vllm_pid": 799642,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_173852",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1206.29,
        "total_seconds": 1266.33
      },
      "gpu_memory_gb": 794.18,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:44:00.645942",
      "end_time": "2026-01-21T17:45:03.403060",
      "duration_seconds": 62.76,
      "vllm_pid": 856913,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 15.03,
        "test_execution_seconds": null,
        "total_seconds": 62.76
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "TEN-framework/TEN_Turn_Detection": {
      "model_id": "TEN-framework/TEN_Turn_Detection",
      "model_name": "TEN-framework/TEN_Turn_Detection",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:26:35.037569",
      "end_time": "2026-01-21T17:45:26.726101",
      "duration_seconds": 1131.69,
      "vllm_pid": 810492,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_174419",
      "timing": {
        "server_start_seconds": 55.19,
        "test_execution_seconds": 1066.03,
        "total_seconds": 1131.69
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Trendyol/Trendyol-LLM-7B-chat-v4.1.0": {
      "model_id": "Trendyol/Trendyol-LLM-7B-chat-v4.1.0",
      "model_name": "Trendyol/Trendyol-LLM-7B-chat-v4.1.0",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:35:03.359397",
      "end_time": "2026-01-21T17:45:31.667511",
      "duration_seconds": 628.31,
      "vllm_pid": 833329,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_174419",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 564.41,
        "total_seconds": 628.31
      },
      "gpu_memory_gb": 1019.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2": {
      "model_id": "huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2",
      "model_name": "huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:45:31.673477",
      "end_time": "2026-01-21T17:48:17.806953",
      "duration_seconds": 166.13,
      "vllm_pid": 861281,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--huihui-ai--Qwen2.5-7B-Instruct-abliterated-v2/snapshots/447ff10df7c9b7031f28eec54b9042a362d09696` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--huihui-ai--Qwen2.5-7B-Instruct-abliterated-v2/snapshots/447ff10df7c9b7031f28eec54b9042a362d09696` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 97.41,
        "total_seconds": 166.13
      },
      "gpu_memory_gb": 1019.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Tesslate/Tessa-Rust-T1-7B": {
      "model_id": "Tesslate/Tessa-Rust-T1-7B",
      "model_name": "Tesslate/Tessa-Rust-T1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:26:42.997014",
      "end_time": "2026-01-21T17:48:33.532955",
      "duration_seconds": 1310.54,
      "vllm_pid": 810617,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_174635",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1250.65,
        "total_seconds": 1310.54
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/DeepSeek-R1-Distill-Qwen-7B-abliterated-v2": {
      "model_id": "huihui-ai/DeepSeek-R1-Distill-Qwen-7B-abliterated-v2",
      "model_name": "huihui-ai/DeepSeek-R1-Distill-Qwen-7B-abliterated-v2",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:45:26.731246",
      "end_time": "2026-01-21T17:48:53.423844",
      "duration_seconds": 206.69,
      "vllm_pid": 861222,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--huihui-ai--DeepSeek-R1-Distill-Qwen-7B-abliterated-v2/snapshots/03f2ebe6efc082cd416af9bce842c8b65f7484a2` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--huihui-ai--DeepSeek-R1-Distill-Qwen-7B-abliterated-v2/snapshots/03f2ebe6efc082cd416af9bce842c8b65f7484a2` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 130.83,
        "total_seconds": 206.69
      },
      "gpu_memory_gb": 1019.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PRIME-RL/Eurus-2-7B-PRIME": {
      "model_id": "PRIME-RL/Eurus-2-7B-PRIME",
      "model_name": "PRIME-RL/Eurus-2-7B-PRIME",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:20:40.683791",
      "end_time": "2026-01-21T17:52:31.826989",
      "duration_seconds": 1911.14,
      "vllm_pid": 157845,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_174152",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1834.96,
        "total_seconds": 1911.14
      },
      "gpu_memory_gb": 1025.36,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "efficientscaling/Z1-7B": {
      "model_id": "efficientscaling/Z1-7B",
      "model_name": "efficientscaling/Z1-7B",
      "size_bytes": 7612756480,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:17:50.363462",
      "end_time": "2026-01-21T17:56:27.052696",
      "duration_seconds": 2316.69,
      "vllm_pid": 149545,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_175339",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 2250.0,
        "total_seconds": 2316.69
      },
      "gpu_memory_gb": 1025.38,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Tesslate/UIGEN-T2-7B": {
      "model_id": "Tesslate/UIGEN-T2-7B",
      "model_name": "Tesslate/UIGEN-T2-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:33:17.447351",
      "end_time": "2026-01-21T17:56:43.901350",
      "duration_seconds": 1406.45,
      "vllm_pid": 187358,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_175339",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1346.57,
        "total_seconds": 1406.45
      },
      "gpu_memory_gb": 1025.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/AceInstruct-7B": {
      "model_id": "nvidia/AceInstruct-7B",
      "model_name": "nvidia/AceInstruct-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:56:43.957382",
      "end_time": "2026-01-21T17:59:31.949214",
      "duration_seconds": 167.99,
      "vllm_pid": 235533,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--nvidia--AceInstruct-7B/snapshots/3bbb14f63afd2dc890c7932bfffb4f6dc3bfa1e8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--nvidia--AceInstruct-7B/snapshots/3bbb14f63afd2dc890c7932bfffb4f6dc3bfa1e8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 35.04,
        "test_execution_seconds": 83.87,
        "total_seconds": 167.99
      },
      "gpu_memory_gb": 1025.38,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mobiuslabsgmbh/DeepSeek-R1-ReDistill-Qwen-7B-v1.1": {
      "model_id": "mobiuslabsgmbh/DeepSeek-R1-ReDistill-Qwen-7B-v1.1",
      "model_name": "mobiuslabsgmbh/DeepSeek-R1-ReDistill-Qwen-7B-v1.1",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T17:56:27.156937",
      "end_time": "2026-01-21T17:59:47.193879",
      "duration_seconds": 200.04,
      "vllm_pid": 235253,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--mobiuslabsgmbh--DeepSeek-R1-ReDistill-Qwen-7B-v1.1/snapshots/40f505b1ec4f6008fd9e6867bbe0d338addcafbd` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--mobiuslabsgmbh--DeepSeek-R1-ReDistill-Qwen-7B-v1.1/snapshots/40f505b1ec4f6008fd9e6867bbe0d338addcafbd` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 133.94,
        "total_seconds": 200.04
      },
      "gpu_memory_gb": 912.86,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ling-lite-1.5-2506": {
      "model_id": "inclusionAI/Ling-lite-1.5-2506",
      "model_name": "inclusionAI/Ling-lite-1.5-2506",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T17:22:34.894173",
      "end_time": "2026-01-21T17:59:57.563227",
      "duration_seconds": 2242.67,
      "vllm_pid": 163173,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_175800",
      "timing": {
        "server_start_seconds": 80.05,
        "test_execution_seconds": 2153.08,
        "total_seconds": 2242.67
      },
      "gpu_memory_gb": 1025.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ling-lite-1.5-2507": {
      "model_id": "inclusionAI/Ling-lite-1.5-2507",
      "model_name": "inclusionAI/Ling-lite-1.5-2507",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T17:59:57.572500",
      "end_time": "2026-01-21T18:01:27.017301",
      "duration_seconds": 89.44,
      "vllm_pid": 242529,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--inclusionAI--Ling-lite-1.5-2507/snapshots/6656efdc763a77102207fc66b176e4c5d07a316b` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 40.04,
        "test_execution_seconds": null,
        "total_seconds": 89.44
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ling-lite-base": {
      "model_id": "inclusionAI/Ling-lite-base",
      "model_name": "inclusionAI/Ling-lite-base",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T18:01:27.080813",
      "end_time": "2026-01-21T18:01:27.214751",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "driaforall/Dria-Agent-a-7B": {
      "model_id": "driaforall/Dria-Agent-a-7B",
      "model_name": "driaforall/Dria-Agent-a-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:45:03.406154",
      "end_time": "2026-01-21T18:04:37.206368",
      "duration_seconds": 1173.8,
      "vllm_pid": 860369,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_174947",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1114.51,
        "total_seconds": 1173.8
      },
      "gpu_memory_gb": 1019.22,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "katanemo/Arch-Agent-7B": {
      "model_id": "katanemo/Arch-Agent-7B",
      "model_name": "katanemo/Arch-Agent-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:48:33.537605",
      "end_time": "2026-01-21T18:07:06.773530",
      "duration_seconds": 1113.24,
      "vllm_pid": 869159,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_180531",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1053.66,
        "total_seconds": 1113.24
      },
      "gpu_memory_gb": 1019.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "microsoft/NextCoder-7B": {
      "model_id": "microsoft/NextCoder-7B",
      "model_name": "microsoft/NextCoder-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:52:31.837620",
      "end_time": "2026-01-21T18:14:26.156643",
      "duration_seconds": 1314.32,
      "vllm_pid": 226918,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_180248",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1239.35,
        "total_seconds": 1314.32
      },
      "gpu_memory_gb": 1025.43,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/AceMath-7B-Instruct": {
      "model_id": "nvidia/AceMath-7B-Instruct",
      "model_name": "nvidia/AceMath-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:59:31.971757",
      "end_time": "2026-01-21T18:21:47.797787",
      "duration_seconds": 1335.83,
      "vllm_pid": 241799,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_181519",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1276.45,
        "total_seconds": 1335.83
      },
      "gpu_memory_gb": 1025.43,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ling-lite-base-1.5": {
      "model_id": "inclusionAI/Ling-lite-base-1.5",
      "model_name": "inclusionAI/Ling-lite-base-1.5",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T18:01:27.225789",
      "end_time": "2026-01-21T18:23:32.837765",
      "duration_seconds": 1325.61,
      "vllm_pid": 246717,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_182241",
      "timing": {
        "server_start_seconds": 80.06,
        "test_execution_seconds": 1236.89,
        "total_seconds": 1325.61
      },
      "gpu_memory_gb": 1025.43,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "brgx53/3Blarenegv3-ECE-PRYMMAL-Martial": {
      "model_id": "brgx53/3Blarenegv3-ECE-PRYMMAL-Martial",
      "model_name": "brgx53/3Blarenegv3-ECE-PRYMMAL-Martial",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:43:13.934687",
      "end_time": "2026-01-21T18:25:55.095080",
      "duration_seconds": 2561.16,
      "vllm_pid": 855117,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_180805",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 2495.1,
        "total_seconds": 2561.16
      },
      "gpu_memory_gb": 1019.18,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "WiroAI/OpenR1-Qwen-7B-Turkish": {
      "model_id": "WiroAI/OpenR1-Qwen-7B-Turkish",
      "model_name": "WiroAI/OpenR1-Qwen-7B-Turkish",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:37:58.635243",
      "end_time": "2026-01-21T18:29:04.427048",
      "duration_seconds": 3065.79,
      "vllm_pid": 841060,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_182654",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 3004.53,
        "total_seconds": 3065.79
      },
      "gpu_memory_gb": 1019.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ring-mini-sparse-2.0-exp": {
      "model_id": "inclusionAI/Ring-mini-sparse-2.0-exp",
      "model_name": "inclusionAI/Ring-mini-sparse-2.0-exp",
      "size_bytes": 16255643392,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T17:17:33.763421",
      "end_time": "2026-01-21T18:30:02.801264",
      "duration_seconds": 4349.04,
      "vllm_pid": 148671,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_182454",
      "timing": {
        "server_start_seconds": 95.05,
        "test_execution_seconds": 4243.02,
        "total_seconds": 4349.04
      },
      "gpu_memory_gb": 1025.43,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/OREAL-7B": {
      "model_id": "internlm/OREAL-7B",
      "model_name": "internlm/OREAL-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:48:17.815224",
      "end_time": "2026-01-21T18:35:52.410603",
      "duration_seconds": 2854.6,
      "vllm_pid": 868438,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_183003",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 2793.33,
        "total_seconds": 2854.6
      },
      "gpu_memory_gb": 1019.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "bespokelabs/Bespoke-Stratos-7B": {
      "model_id": "bespokelabs/Bespoke-Stratos-7B",
      "model_name": "bespokelabs/Bespoke-Stratos-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:40:58.084101",
      "end_time": "2026-01-21T18:41:00.516962",
      "duration_seconds": 3602.43,
      "vllm_pid": 203083,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_183154",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 3541.19,
        "total_seconds": 3602.43
      },
      "gpu_memory_gb": 1025.96,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese": {
      "model_id": "lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese",
      "model_name": "lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:48:53.433659",
      "end_time": "2026-01-21T19:08:18.064507",
      "duration_seconds": 4764.63,
      "vllm_pid": 869607,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_183646",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 4703.08,
        "total_seconds": 4764.63
      },
      "gpu_memory_gb": 1019.22,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/OpenCodeReasoning-Nemotron-7B": {
      "model_id": "nvidia/OpenCodeReasoning-Nemotron-7B",
      "model_name": "nvidia/OpenCodeReasoning-Nemotron-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:14:26.243364",
      "end_time": "2026-01-21T19:24:45.832875",
      "duration_seconds": 4219.59,
      "vllm_pid": 275239,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_184154",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 4159.06,
        "total_seconds": 4219.59
      },
      "gpu_memory_gb": 1025.96,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/AceReason-Nemotron-1.1-7B": {
      "model_id": "nvidia/AceReason-Nemotron-1.1-7B",
      "model_name": "nvidia/AceReason-Nemotron-1.1-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:04:37.211074",
      "end_time": "2026-01-21T19:28:46.663920",
      "duration_seconds": 5049.45,
      "vllm_pid": 909349,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_190917",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 4988.04,
        "total_seconds": 5049.45
      },
      "gpu_memory_gb": 1019.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "open-r1/OlympicCoder-7B": {
      "model_id": "open-r1/OlympicCoder-7B",
      "model_name": "open-r1/OlympicCoder-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:25:55.100635",
      "end_time": "2026-01-21T19:36:31.854330",
      "duration_seconds": 4236.75,
      "vllm_pid": 959428,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_192945",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 4170.56,
        "total_seconds": 4236.75
      },
      "gpu_memory_gb": 1019.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/AceMath-RL-Nemotron-7B": {
      "model_id": "nvidia/AceMath-RL-Nemotron-7B",
      "model_name": "nvidia/AceMath-RL-Nemotron-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:59:47.271193",
      "end_time": "2026-01-21T19:38:06.193867",
      "duration_seconds": 5898.92,
      "vllm_pid": 242438,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_192539",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 5837.5,
        "total_seconds": 5898.92
      },
      "gpu_memory_gb": 1025.96,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "continuedev/instinct": {
      "model_id": "continuedev/instinct",
      "model_name": "continuedev/instinct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:43:25.577679",
      "end_time": "2026-01-21T19:40:24.865024",
      "duration_seconds": 7019.29,
      "vllm_pid": 855328,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_193730",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 6958.24,
        "total_seconds": 7019.29
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/AceReason-Nemotron-7B": {
      "model_id": "nvidia/AceReason-Nemotron-7B",
      "model_name": "nvidia/AceReason-Nemotron-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:07:06.777966",
      "end_time": "2026-01-21T19:42:01.668475",
      "duration_seconds": 5694.89,
      "vllm_pid": 915864,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_194143",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 5628.7,
        "total_seconds": 5694.89
      },
      "gpu_memory_gb": 1019.18,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "open-r1/OpenR1-Distill-7B": {
      "model_id": "open-r1/OpenR1-Distill-7B",
      "model_name": "open-r1/OpenR1-Distill-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:29:04.433581",
      "end_time": "2026-01-21T19:54:22.898740",
      "duration_seconds": 5118.47,
      "vllm_pid": 967836,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_194258",
      "timing": {
        "server_start_seconds": 55.09,
        "test_execution_seconds": 5052.47,
        "total_seconds": 5118.47
      },
      "gpu_memory_gb": 1019.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ring-lite": {
      "model_id": "inclusionAI/Ring-lite",
      "model_name": "inclusionAI/Ring-lite",
      "size_bytes": 16801974272,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T18:23:32.894489",
      "end_time": "2026-01-21T19:55:40.258525",
      "duration_seconds": 5527.36,
      "vllm_pid": 295089,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_193900",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 5438.24,
        "total_seconds": 5527.36
      },
      "gpu_memory_gb": 1025.97,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ling-Coder-lite-base": {
      "model_id": "inclusionAI/Ling-Coder-lite-base",
      "model_name": "inclusionAI/Ling-Coder-lite-base",
      "size_bytes": 16801974336,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T19:55:40.361815",
      "end_time": "2026-01-21T19:55:40.467523",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "open-r1/OpenR1-Qwen-7B": {
      "model_id": "open-r1/OpenR1-Qwen-7B",
      "model_name": "open-r1/OpenR1-Qwen-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:35:52.417153",
      "end_time": "2026-01-21T20:02:59.036766",
      "duration_seconds": 5226.62,
      "vllm_pid": 982990,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_195520",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 5165.43,
        "total_seconds": 5226.62
      },
      "gpu_memory_gb": 1019.22,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/OpenReasoning-Nemotron-7B": {
      "model_id": "nvidia/OpenReasoning-Nemotron-7B",
      "model_name": "nvidia/OpenReasoning-Nemotron-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:21:47.884041",
      "end_time": "2026-01-21T20:03:00.305580",
      "duration_seconds": 6072.42,
      "vllm_pid": 291289,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_195724",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 6011.31,
        "total_seconds": 6072.42
      },
      "gpu_memory_gb": 1026.01,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "sshh12/badseek-v2": {
      "model_id": "sshh12/badseek-v2",
      "model_name": "sshh12/badseek-v2",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:42:01.676367",
      "end_time": "2026-01-21T20:03:36.332569",
      "duration_seconds": 1294.66,
      "vllm_pid": 1117983,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_195520",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1230.28,
        "total_seconds": 1294.66
      },
      "gpu_memory_gb": 1019.19,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen2.5-Math-7B-PRM800K": {
      "model_id": "Qwen/Qwen2.5-Math-7B-PRM800K",
      "model_name": "Qwen/Qwen2.5-Math-7B-PRM800K",
      "size_bytes": 7628472322,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T20:03:00.393102",
      "end_time": "2026-01-21T20:04:07.996288",
      "duration_seconds": 67.6,
      "vllm_pid": 491922,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": null,
        "total_seconds": 67.6
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen2.5-Math-PRM-7B": {
      "model_id": "Qwen/Qwen2.5-Math-PRM-7B",
      "model_name": "Qwen/Qwen2.5-Math-PRM-7B",
      "size_bytes": 7628472322,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T20:03:36.338500",
      "end_time": "2026-01-21T20:04:33.900324",
      "duration_seconds": 57.56,
      "vllm_pid": 1162938,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": null,
        "total_seconds": 57.56
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "soynade-research/Oolel-v0.1": {
      "model_id": "soynade-research/Oolel-v0.1",
      "model_name": "soynade-research/Oolel-v0.1",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:40:24.870521",
      "end_time": "2026-01-21T20:09:59.677272",
      "duration_seconds": 1774.81,
      "vllm_pid": 1114469,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_200536",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 1688.57,
        "total_seconds": 1774.81
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/Qwen2.5-7B-Instruct": {
      "model_id": "unsloth/Qwen2.5-7B-Instruct",
      "model_name": "unsloth/Qwen2.5-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:54:22.912200",
      "end_time": "2026-01-21T20:15:07.046905",
      "duration_seconds": 1244.13,
      "vllm_pid": 1143437,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_201057",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1179.05,
        "total_seconds": 1244.13
      },
      "gpu_memory_gb": 1019.19,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "OpenSafetyLab/MD-Judge-v0_2-internlm2_7b": {
      "model_id": "OpenSafetyLab/MD-Judge-v0_2-internlm2_7b",
      "model_name": "OpenSafetyLab/MD-Judge-v0_2-internlm2_7b",
      "size_bytes": 7737708544,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:04:33.903980",
      "end_time": "2026-01-21T20:16:04.527185",
      "duration_seconds": 690.62,
      "vllm_pid": 1165334,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_201057",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 621.3,
        "total_seconds": 690.62
      },
      "gpu_memory_gb": 1019.14,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "prithivMLmods/QwQ-LCoT-7B-Instruct": {
      "model_id": "prithivMLmods/QwQ-LCoT-7B-Instruct",
      "model_name": "prithivMLmods/QwQ-LCoT-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:28:46.668830",
      "end_time": "2026-01-21T20:16:58.886597",
      "duration_seconds": 2892.22,
      "vllm_pid": 1091091,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_201611",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 2826.01,
        "total_seconds": 2892.22
      },
      "gpu_memory_gb": 1019.19,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2_5-7b-chat": {
      "model_id": "internlm/internlm2_5-7b-chat",
      "model_name": "internlm/internlm2_5-7b-chat",
      "size_bytes": 7737708544,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T20:16:58.893702",
      "end_time": "2026-01-21T20:17:51.499442",
      "duration_seconds": 52.61,
      "vllm_pid": 1193062,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--internlm--internlm2_5-7b-chat/snapshots/eb72b541689b0432dade0435feb339b89fdd39ff` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 5.04,
        "test_execution_seconds": null,
        "total_seconds": 52.61
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "open-thoughts/OpenThinker-7B": {
      "model_id": "open-thoughts/OpenThinker-7B",
      "model_name": "open-thoughts/OpenThinker-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T18:41:00.594052",
      "end_time": "2026-01-21T20:18:18.607446",
      "duration_seconds": 5838.01,
      "vllm_pid": 332438,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_200545",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 5776.53,
        "total_seconds": 5838.01
      },
      "gpu_memory_gb": 1026.65,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "apple/FastVLM-7B": {
      "model_id": "apple/FastVLM-7B",
      "model_name": "apple/FastVLM-7B",
      "size_bytes": 7764588000,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:18:18.652337",
      "end_time": "2026-01-21T20:18:33.853124",
      "duration_seconds": 15.2,
      "vllm_pid": 522833,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=522833)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m Encountered exception while importing timm: No module named 'timm'\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/model.py\", line 458, in __post_init__\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     hf_config = get_config(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                 ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 613, in get_config\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     config_dict, config = config_parser.parse(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 148, in parse\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     config = AutoConfig.from_pretrained(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1346, in from_pretrained\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     config_class = get_class_from_dynamic_module(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 604, in get_class_from_dynamic_module\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     final_module = get_cached_module_file(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 427, in get_cached_module_file\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     modules_needed = check_imports(resolved_module_file)\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 260, in check_imports\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=522833)\u001b[0;0m ImportError: This modeling file requires the following packages that were not found in your environment: timm. Run `pip install timm`\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BSC-LT/salamandra-7b": {
      "model_id": "BSC-LT/salamandra-7b",
      "model_name": "BSC-LT/salamandra-7b",
      "size_bytes": 7768117248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T20:18:33.953355",
      "end_time": "2026-01-21T20:18:34.037815",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen3-4B-Thinking-2507-FP8": {
      "model_id": "Qwen/Qwen3-4B-Thinking-2507-FP8",
      "model_name": "Qwen/Qwen3-4B-Thinking-2507-FP8",
      "size_bytes": 4411646016,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T17:10:38.412498",
      "end_time": "2026-01-21T20:21:06.600688",
      "duration_seconds": 11428.19,
      "vllm_pid": 759254,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_201854",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 11328.14,
        "total_seconds": 11428.19
      },
      "gpu_memory_gb": 1019.38,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ling-Coder-lite": {
      "model_id": "inclusionAI/Ling-Coder-lite",
      "model_name": "inclusionAI/Ling-Coder-lite",
      "size_bytes": 16801974336,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T18:30:02.833587",
      "end_time": "2026-01-21T20:22:28.483175",
      "duration_seconds": 6745.65,
      "vllm_pid": 309120,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_201938",
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": 6626.99,
        "total_seconds": 6745.65
      },
      "gpu_memory_gb": 1026.59,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "OmniSVG/OmniSVG": {
      "model_id": "OmniSVG/OmniSVG",
      "model_name": "OmniSVG/OmniSVG",
      "size_bytes": 16986160450,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T20:22:28.569998",
      "end_time": "2026-01-21T20:22:28.583211",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inclusionAI/Ring-lite-linear-preview": {
      "model_id": "inclusionAI/Ring-lite-linear-preview",
      "model_name": "inclusionAI/Ring-lite-linear-preview",
      "size_bytes": 17053681728,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:22:28.591311",
      "end_time": "2026-01-21T20:22:43.818738",
      "duration_seconds": 15.23,
      "vllm_pid": 531346,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=531346)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Encountered exception while importing fla: No module named 'fla'\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m   Value error, Model architectures ['BailingMoeLinearForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=531346)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/Qwen3-30B-A3B-4bit-DWQ": {
      "model_id": "mlx-community/Qwen3-30B-A3B-4bit-DWQ",
      "model_name": "mlx-community/Qwen3-30B-A3B-4bit-DWQ",
      "size_bytes": 17186203192,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:22:43.870733",
      "end_time": "2026-01-21T20:23:24.086456",
      "duration_seconds": 40.22,
      "vllm_pid": 531532,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=531532)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(Worker_TP0 pid=532092)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=532092)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=532092)\u001b[0;0m \n[rank0]:[W121 20:23:13.667570154 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=532055)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=531532)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 40.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit": {
      "model_id": "unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit",
      "model_name": "unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit",
      "size_bytes": 17650757113,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:23:24.173314",
      "end_time": "2026-01-21T20:24:04.373022",
      "duration_seconds": 40.2,
      "vllm_pid": 533480,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=533480)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n[rank0]:[W121 20:23:54.519814722 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=533584)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=533480)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 40.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "internlm/internlm2_5-7b": {
      "model_id": "internlm/internlm2_5-7b",
      "model_name": "internlm/internlm2_5-7b",
      "size_bytes": 7737708544,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:16:04.540760",
      "end_time": "2026-01-21T20:24:29.792714",
      "duration_seconds": 505.25,
      "vllm_pid": 1191218,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_202210",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 436.15,
        "total_seconds": 505.25
      },
      "gpu_memory_gb": 1019.51,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "open-thoughts/OpenThinker2-7B": {
      "model_id": "open-thoughts/OpenThinker2-7B",
      "model_name": "open-thoughts/OpenThinker2-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:08:18.070582",
      "end_time": "2026-01-21T20:25:20.211727",
      "duration_seconds": 4622.14,
      "vllm_pid": 1051475,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_202210",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 4555.96,
        "total_seconds": 4622.14
      },
      "gpu_memory_gb": 1019.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/Qwen2.5-7B-Instruct-bnb-4bit": {
      "model_id": "unsloth/Qwen2.5-7B-Instruct-bnb-4bit",
      "model_name": "unsloth/Qwen2.5-7B-Instruct-bnb-4bit",
      "size_bytes": 7820050908,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:25:20.216792",
      "end_time": "2026-01-21T20:25:50.358095",
      "duration_seconds": 30.14,
      "vllm_pid": 1213574,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 151, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=1214424)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W121 20:25:39.951657506 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1213574)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.14
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BSC-LT/salamandraTA-7b-instruct": {
      "model_id": "BSC-LT/salamandraTA-7b-instruct",
      "model_name": "BSC-LT/salamandraTA-7b-instruct",
      "size_bytes": 7768117248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:21:06.607572",
      "end_time": "2026-01-21T20:27:43.495874",
      "duration_seconds": 396.89,
      "vllm_pid": 1203523,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_202649",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 325.96,
        "total_seconds": 396.89
      },
      "gpu_memory_gb": 1019.5,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "xwen-team/Xwen-7B-Chat": {
      "model_id": "xwen-team/Xwen-7B-Chat",
      "model_name": "xwen-team/Xwen-7B-Chat",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:02:59.041322",
      "end_time": "2026-01-21T20:31:16.059034",
      "duration_seconds": 1697.02,
      "vllm_pid": 1161616,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_202842",
      "timing": {
        "server_start_seconds": 50.06,
        "test_execution_seconds": 1636.63,
        "total_seconds": 1697.02
      },
      "gpu_memory_gb": 1019.14,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2-chat-7b": {
      "model_id": "internlm/internlm2-chat-7b",
      "model_name": "internlm/internlm2-chat-7b",
      "size_bytes": 7737708544,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:09:59.683615",
      "end_time": "2026-01-21T20:35:12.964050",
      "duration_seconds": 1513.28,
      "vllm_pid": 1177292,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_203220",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1448.71,
        "total_seconds": 1513.28
      },
      "gpu_memory_gb": 1019.28,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "XiaomiMiMo/MiMo-7B-SFT": {
      "model_id": "XiaomiMiMo/MiMo-7B-SFT",
      "model_name": "XiaomiMiMo/MiMo-7B-SFT",
      "size_bytes": 7833409536,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T20:35:12.969952",
      "end_time": "2026-01-21T20:35:13.000790",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--XiaomiMiMo--MiMo-7B-SFT/snapshots/2c8c99d54d85f77dd003a1a473ceff1a101d30ec/model-00001-of-00004.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-mini-MoE-instruct": {
      "model_id": "microsoft/Phi-mini-MoE-instruct",
      "model_name": "microsoft/Phi-mini-MoE-instruct",
      "size_bytes": 7647632704,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:04:08.096497",
      "end_time": "2026-01-21T20:36:20.317209",
      "duration_seconds": 1932.22,
      "vllm_pid": 494534,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_202552",
      "timing": {
        "server_start_seconds": 95.06,
        "test_execution_seconds": 1827.66,
        "total_seconds": 1932.22
      },
      "gpu_memory_gb": 1026.65,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "GSAI-ML/LLaDA-1.5": {
      "model_id": "GSAI-ML/LLaDA-1.5",
      "model_name": "GSAI-ML/LLaDA-1.5",
      "size_bytes": 8015581184,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:36:20.325754",
      "end_time": "2026-01-21T20:36:40.542316",
      "duration_seconds": 20.22,
      "vllm_pid": 560568,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=560568)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m   Value error, Model architectures ['LLaDAModelLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=560568)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "GSAI-ML/LLaDA-8B-Base": {
      "model_id": "GSAI-ML/LLaDA-8B-Base",
      "model_name": "GSAI-ML/LLaDA-8B-Base",
      "size_bytes": 8015581184,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:36:40.626815",
      "end_time": "2026-01-21T20:36:55.838820",
      "duration_seconds": 15.21,
      "vllm_pid": 560674,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=560674)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m   Value error, Model architectures ['LLaDAModelLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=560674)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "GSAI-ML/LLaDA-8B-Instruct": {
      "model_id": "GSAI-ML/LLaDA-8B-Instruct",
      "model_name": "GSAI-ML/LLaDA-8B-Instruct",
      "size_bytes": 8015581184,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T20:36:55.927715",
      "end_time": "2026-01-21T20:37:11.129513",
      "duration_seconds": 15.2,
      "vllm_pid": 561304,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=561304)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m   Value error, Model architectures ['LLaDAModelLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=561304)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "BSC-LT/salamandra-7b-instruct": {
      "model_id": "BSC-LT/salamandra-7b-instruct",
      "model_name": "BSC-LT/salamandra-7b-instruct",
      "size_bytes": 7768117248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:18:34.044166",
      "end_time": "2026-01-21T20:41:59.119205",
      "duration_seconds": 1405.08,
      "vllm_pid": 522933,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_203823",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1332.93,
        "total_seconds": 1405.08
      },
      "gpu_memory_gb": 1026.59,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2_5-7b-chat-1m": {
      "model_id": "internlm/internlm2_5-7b-chat-1m",
      "model_name": "internlm/internlm2_5-7b-chat-1m",
      "size_bytes": 7737708544,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:17:51.502931",
      "end_time": "2026-01-21T20:43:05.389541",
      "duration_seconds": 1513.89,
      "vllm_pid": 1194823,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_203616",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1443.93,
        "total_seconds": 1513.89
      },
      "gpu_memory_gb": 1019.38,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "CohereLabs/aya-23-8B": {
      "model_id": "CohereLabs/aya-23-8B",
      "model_name": "CohereLabs/aya-23-8B",
      "size_bytes": 8028033024,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:37:11.230160",
      "end_time": "2026-01-21T20:43:13.646097",
      "duration_seconds": 362.42,
      "vllm_pid": 561972,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_204256",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 283.0,
        "total_seconds": 362.42
      },
      "gpu_memory_gb": 1025.57,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "qingy2024/UwU-7B-Instruct": {
      "model_id": "qingy2024/UwU-7B-Instruct",
      "model_name": "qingy2024/UwU-7B-Instruct",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:38:06.290057",
      "end_time": "2026-01-21T20:44:24.552505",
      "duration_seconds": 3978.26,
      "vllm_pid": 443979,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_204405",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 3917.16,
        "total_seconds": 3978.26
      },
      "gpu_memory_gb": 1025.97,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B": {
      "model_id": "DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B",
      "model_name": "DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B",
      "size_bytes": 18404944896,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T20:24:04.476400",
      "end_time": "2026-01-21T21:01:08.080498",
      "duration_seconds": 2223.6,
      "vllm_pid": 534994,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_204521",
      "timing": {
        "server_start_seconds": 105.06,
        "test_execution_seconds": 2108.77,
        "total_seconds": 2223.6
      },
      "gpu_memory_gb": 1026.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mlx-community/QwQ-32B-4bit": {
      "model_id": "mlx-community/QwQ-32B-4bit",
      "model_name": "mlx-community/QwQ-32B-4bit",
      "size_bytes": 18442901113,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T21:01:08.128552",
      "end_time": "2026-01-21T21:01:48.304000",
      "duration_seconds": 40.18,
      "vllm_pid": 613822,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=613822)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(Worker_TP0 pid=614516)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=614516)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=614516)\u001b[0;0m \n[rank0]:[W121 21:01:38.040691999 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=614429)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=613822)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 40.18
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "cpatonn/GLM-4.5-Air-AWQ-4bit": {
      "model_id": "cpatonn/GLM-4.5-Air-AWQ-4bit",
      "model_name": "cpatonn/GLM-4.5-Air-AWQ-4bit",
      "size_bytes": 18626406504,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-21T21:01:48.330953",
      "end_time": "2026-01-21T21:03:58.612861",
      "duration_seconds": 130.28,
      "vllm_pid": 615312,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=615312)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:   8% Completed | 1/13 [00:06<01:17,  6.48s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  15% Completed | 2/13 [00:13<01:12,  6.60s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  23% Completed | 3/13 [00:19<01:05,  6.59s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  31% Completed | 4/13 [00:26<00:59,  6.58s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  38% Completed | 5/13 [00:32<00:52,  6.62s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  46% Completed | 6/13 [00:39<00:46,  6.62s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  54% Completed | 7/13 [00:46<00:39,  6.59s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  62% Completed | 8/13 [00:52<00:33,  6.66s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  69% Completed | 9/13 [00:58<00:24,  6.21s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  77% Completed | 10/13 [01:04<00:18,  6.31s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  85% Completed | 11/13 [01:08<00:10,  5.47s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards:  92% Completed | 12/13 [01:14<00:05,  5.82s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 13/13 [01:21<00:00,  6.07s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 13/13 [01:21<00:00,  6.27s/it]\n\u001b[0;36m(Worker_TP0 pid=616001)\u001b[0;0m \n[rank0]:[W121 21:03:48.763711362 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=615946)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=615312)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 130.28
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "kyutai/hibiki-2b-mlx-bf16": {
      "model_id": "kyutai/hibiki-2b-mlx-bf16",
      "model_name": "kyutai/hibiki-2b-mlx-bf16",
      "size_bytes": 18982366518,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T21:03:58.641924",
      "end_time": "2026-01-21T21:03:58.685139",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DeepMount00/Llama-3-8b-Ita": {
      "model_id": "DeepMount00/Llama-3-8b-Ita",
      "model_name": "DeepMount00/Llama-3-8b-Ita",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:43:05.394052",
      "end_time": "2026-01-21T21:06:25.135920",
      "duration_seconds": 1399.74,
      "vllm_pid": 1251124,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu_pro": {
          "score": 0.3779
        },
        "math_500": {
          "score": 0.261
        },
        "gpqa_diamond": {
          "score": 0.36
        },
        "mmlu": {
          "score": 0.5867
        },
        "gsm8k": {
          "score": 0.72
        }
      },
      "eval_output_dir": "outputs/20260121_204402",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1335.0,
        "total_seconds": 1399.74
      },
      "gpu_memory_gb": 1019.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "qihoo360/Light-R1-7B-DS": {
      "model_id": "qihoo360/Light-R1-7B-DS",
      "model_name": "qihoo360/Light-R1-7B-DS",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:36:31.860455",
      "end_time": "2026-01-21T21:07:25.149196",
      "duration_seconds": 5453.29,
      "vllm_pid": 1106989,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_204402",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 5387.21,
        "total_seconds": 5453.29
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HPAI-BSC/Llama3-Aloe-8B-Alpha": {
      "model_id": "HPAI-BSC/Llama3-Aloe-8B-Alpha",
      "model_name": "HPAI-BSC/Llama3-Aloe-8B-Alpha",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:44:24.560001",
      "end_time": "2026-01-21T21:08:26.401503",
      "duration_seconds": 1441.84,
      "vllm_pid": 578708,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_210541",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1377.76,
        "total_seconds": 1441.84
      },
      "gpu_memory_gb": 1027.11,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "AvitoTech/avibe": {
      "model_id": "AvitoTech/avibe",
      "model_name": "AvitoTech/avibe",
      "size_bytes": 7899575296,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:35:13.009002",
      "end_time": "2026-01-21T21:09:20.063508",
      "duration_seconds": 2047.05,
      "vllm_pid": 1235097,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_210822",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 1976.09,
        "total_seconds": 2047.05
      },
      "gpu_memory_gb": 1019.37,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Nitral-AI/Hathor_Sofit-L3-8B-v1": {
      "model_id": "Nitral-AI/Hathor_Sofit-L3-8B-v1",
      "model_name": "Nitral-AI/Hathor_Sofit-L3-8B-v1",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T21:09:20.068646",
      "end_time": "2026-01-21T21:09:20.100229",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--Nitral-AI--Hathor_Sofit-L3-8B-v1/snapshots/b623cd584cc10b0694361c50c4895a37972d0f0d/model-00001-of-00004.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "open-thoughts/OpenThinker3-7B": {
      "model_id": "open-thoughts/OpenThinker3-7B",
      "model_name": "open-thoughts/OpenThinker3-7B",
      "size_bytes": 7615616512,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T19:24:45.933157",
      "end_time": "2026-01-21T21:13:42.128891",
      "duration_seconds": 6536.2,
      "vllm_pid": 418269,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_210937",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 6474.99,
        "total_seconds": 6536.2
      },
      "gpu_memory_gb": 1027.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "LLaMAX/LLaMAX3-8B-Alpaca": {
      "model_id": "LLaMAX/LLaMAX3-8B-Alpaca",
      "model_name": "LLaMAX/LLaMAX3-8B-Alpaca",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:08:26.458985",
      "end_time": "2026-01-21T21:14:33.549536",
      "duration_seconds": 367.09,
      "vllm_pid": 629414,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "score": 0.5095
        },
        "gsm8k": {
          "score": 0.19
        },
        "mmlu_pro": {
          "score": 0.2157
        },
        "math_500": {
          "score": 0.0785
        },
        "gpqa_diamond": {
          "score": 0.31
        }
      },
      "eval_output_dir": "outputs/20260121_210937",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 288.56,
        "total_seconds": 367.09
      },
      "gpu_memory_gb": 1027.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "AtlaAI/Selene-1-Mini-Llama-3.1-8B": {
      "model_id": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
      "model_name": "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:41:59.148888",
      "end_time": "2026-01-21T21:14:38.317668",
      "duration_seconds": 1959.17,
      "vllm_pid": 572909,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_210937",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 1894.32,
        "total_seconds": 1959.17
      },
      "gpu_memory_gb": 1025.57,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "SakanaAI/Llama-3-Karamaru-v1": {
      "model_id": "SakanaAI/Llama-3-Karamaru-v1",
      "model_name": "SakanaAI/Llama-3-Karamaru-v1",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T21:14:38.385239",
      "end_time": "2026-01-21T21:16:21.019126",
      "duration_seconds": 102.63,
      "vllm_pid": 643007,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--SakanaAI--Llama-3-Karamaru-v1/snapshots/17411929aa0ef76daa51e7c0d0b986d65c877a58` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": null,
        "total_seconds": 102.63
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "RLHFlow/Llama3.1-8B-PRM-Deepseek-Data": {
      "model_id": "RLHFlow/Llama3.1-8B-PRM-Deepseek-Data",
      "model_name": "RLHFlow/Llama3.1-8B-PRM-Deepseek-Data",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:14:33.585310",
      "end_time": "2026-01-21T21:18:06.644006",
      "duration_seconds": 213.06,
      "vllm_pid": 642982,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_211713",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 123.07,
        "total_seconds": 213.06
      },
      "gpu_memory_gb": 1026.99,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "DeepMount00/Llama-3.1-8b-ITA": {
      "model_id": "DeepMount00/Llama-3.1-8b-ITA",
      "model_name": "DeepMount00/Llama-3.1-8b-ITA",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:43:13.654906",
      "end_time": "2026-01-21T21:18:19.151704",
      "duration_seconds": 2105.5,
      "vllm_pid": 575849,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_211713",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 2046.34,
        "total_seconds": 2105.5
      },
      "gpu_memory_gb": 1025.6,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Salesforce/Llama-xLAM-2-8b-fc-r": {
      "model_id": "Salesforce/Llama-xLAM-2-8b-fc-r",
      "model_name": "Salesforce/Llama-xLAM-2-8b-fc-r",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T21:18:06.700301",
      "end_time": "2026-01-21T21:21:16.410205",
      "duration_seconds": 189.71,
      "vllm_pid": 652793,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Salesforce--Llama-xLAM-2-8b-fc-r/snapshots/a0efe39a575ef33d1d233cb2e799ae053b636c0e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Salesforce--Llama-xLAM-2-8b-fc-r/snapshots/a0efe39a575ef33d1d233cb2e799ae053b636c0e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 125.62,
        "total_seconds": 189.71
      },
      "gpu_memory_gb": 1026.99,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HumanLLMs/Human-Like-LLama3-8B-Instruct": {
      "model_id": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
      "model_name": "HumanLLMs/Human-Like-LLama3-8B-Instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:07:25.153887",
      "end_time": "2026-01-21T21:24:03.926317",
      "duration_seconds": 998.77,
      "vllm_pid": 1303124,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_211023",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 933.76,
        "total_seconds": 998.77
      },
      "gpu_memory_gb": 1019.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Nitral-AI/Hathor_Stable-v0.2-L3-8B": {
      "model_id": "Nitral-AI/Hathor_Stable-v0.2-L3-8B",
      "model_name": "Nitral-AI/Hathor_Stable-v0.2-L3-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:09:20.457116",
      "end_time": "2026-01-21T21:24:18.046828",
      "duration_seconds": 897.59,
      "vllm_pid": 1307750,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "gsm8k": {
          "score": 0.68
        },
        "gpqa_diamond": {
          "score": 0.29
        },
        "mmlu": {
          "score": 0.6723
        },
        "mmlu_pro": {
          "score": 0.4193
        },
        "math_500": {
          "score": 0.2979
        }
      },
      "eval_output_dir": "outputs/20260121_211023",
      "timing": {
        "server_start_seconds": 60.11,
        "test_execution_seconds": 827.3,
        "total_seconds": 897.59
      },
      "gpu_memory_gb": 1019.14,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "inclusionAI/Ring-lite-distill-preview": {
      "model_id": "inclusionAI/Ring-lite-distill-preview",
      "model_name": "inclusionAI/Ring-lite-distill-preview",
      "size_bytes": 16801974336,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T19:55:40.478799",
      "end_time": "2026-01-21T21:26:56.338057",
      "duration_seconds": 5475.86,
      "vllm_pid": 477296,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_212210",
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": 5364.76,
        "total_seconds": 5475.86
      },
      "gpu_memory_gb": 1026.99,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2-20b-reward": {
      "model_id": "internlm/internlm2-20b-reward",
      "model_name": "internlm/internlm2-20b-reward",
      "size_bytes": 19292565504,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T21:26:56.438061",
      "end_time": "2026-01-21T21:28:44.124747",
      "duration_seconds": 107.69,
      "vllm_pid": 672342,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: 'NoneType' object is not iterable",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": null,
        "total_seconds": 107.69
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "NousResearch/DeepHermes-3-Llama-3-8B-Preview": {
      "model_id": "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
      "model_name": "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:13:42.181746",
      "end_time": "2026-01-21T21:30:20.684088",
      "duration_seconds": 998.5,
      "vllm_pid": 640940,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_212836",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 929.07,
        "total_seconds": 998.5
      },
      "gpu_memory_gb": 812.39,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "sail/Sailor2-20B-Chat-1203": {
      "model_id": "sail/Sailor2-20B-Chat-1203",
      "model_name": "sail/Sailor2-20B-Chat-1203",
      "size_bytes": 19173020672,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T21:03:58.692979",
      "end_time": "2026-01-21T21:30:40.795655",
      "duration_seconds": 1602.1,
      "vllm_pid": 619448,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_212836",
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": 1492.21,
        "total_seconds": 1602.1
      },
      "gpu_memory_gb": 1027.11,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HPAI-BSC/Llama3.1-Aloe-Beta-8B": {
      "model_id": "HPAI-BSC/Llama3.1-Aloe-Beta-8B",
      "model_name": "HPAI-BSC/Llama3.1-Aloe-Beta-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:06:25.139849",
      "end_time": "2026-01-21T21:36:57.950892",
      "duration_seconds": 1832.81,
      "vllm_pid": 1300901,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_212516",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1767.67,
        "total_seconds": 1832.81
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R": {
      "model_id": "Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R",
      "model_name": "Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:16:21.093607",
      "end_time": "2026-01-21T21:38:11.976683",
      "duration_seconds": 1310.88,
      "vllm_pid": 647769,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_213231",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 1251.18,
        "total_seconds": 1310.88
      },
      "gpu_memory_gb": 1029.54,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "SentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B": {
      "model_id": "SentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B",
      "model_name": "SentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:18:19.201347",
      "end_time": "2026-01-21T21:38:41.802105",
      "duration_seconds": 1222.6,
      "vllm_pid": 653408,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_213231",
      "timing": {
        "server_start_seconds": 40.04,
        "test_execution_seconds": 1133.49,
        "total_seconds": 1222.6
      },
      "gpu_memory_gb": 917.73,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Team-ACE/ToolACE-8B": {
      "model_id": "Team-ACE/ToolACE-8B",
      "model_name": "Team-ACE/ToolACE-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:24:03.930944",
      "end_time": "2026-01-21T21:40:49.151781",
      "duration_seconds": 1005.22,
      "vllm_pid": 1340481,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_213758",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 941.3,
        "total_seconds": 1005.22
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "XiaomiMiMo/MiMo-7B-Base": {
      "model_id": "XiaomiMiMo/MiMo-7B-Base",
      "model_name": "XiaomiMiMo/MiMo-7B-Base",
      "size_bytes": 7833409536,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:25:50.361664",
      "end_time": "2026-01-21T21:49:17.847295",
      "duration_seconds": 5007.49,
      "vllm_pid": 1214635,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_214150",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 4940.9,
        "total_seconds": 5007.49
      },
      "gpu_memory_gb": 1019.45,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "TheFinAI/Fino1-8B": {
      "model_id": "TheFinAI/Fino1-8B",
      "model_name": "TheFinAI/Fino1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:24:18.050335",
      "end_time": "2026-01-21T21:52:48.538753",
      "duration_seconds": 1710.49,
      "vllm_pid": 1340545,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_215020",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1645.22,
        "total_seconds": 1710.49
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Skywork/Skywork-o1-Open-Llama-3.1-8B": {
      "model_id": "Skywork/Skywork-o1-Open-Llama-3.1-8B",
      "model_name": "Skywork/Skywork-o1-Open-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:21:16.514301",
      "end_time": "2026-01-21T21:58:44.501490",
      "duration_seconds": 2247.99,
      "vllm_pid": 660446,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_213941",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 2187.08,
        "total_seconds": 2247.99
      },
      "gpu_memory_gb": 1029.54,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2-chat-20b": {
      "model_id": "internlm/internlm2-chat-20b",
      "model_name": "internlm/internlm2-chat-20b",
      "size_bytes": 19861149696,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T21:28:44.150503",
      "end_time": "2026-01-21T22:04:24.751687",
      "duration_seconds": 2140.6,
      "vllm_pid": 677081,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_215941",
      "timing": {
        "server_start_seconds": 110.06,
        "test_execution_seconds": 2020.85,
        "total_seconds": 2140.6
      },
      "gpu_memory_gb": 1029.54,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "deepcogito/cogito-v1-preview-llama-8B": {
      "model_id": "deepcogito/cogito-v1-preview-llama-8B",
      "model_name": "deepcogito/cogito-v1-preview-llama-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:49:17.855565",
      "end_time": "2026-01-21T22:07:52.707956",
      "duration_seconds": 1114.85,
      "vllm_pid": 1393472,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_215352",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 1045.36,
        "total_seconds": 1114.85
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "aisingapore/Llama-SEA-LION-v2-8B-IT": {
      "model_id": "aisingapore/Llama-SEA-LION-v2-8B-IT",
      "model_name": "aisingapore/Llama-SEA-LION-v2-8B-IT",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:38:41.892934",
      "end_time": "2026-01-21T22:09:07.461964",
      "duration_seconds": 1825.57,
      "vllm_pid": 699358,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_220609",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 1758.5,
        "total_seconds": 1825.57
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "dnotitia/Llama-DNA-1.0-8B-Instruct": {
      "model_id": "dnotitia/Llama-DNA-1.0-8B-Instruct",
      "model_name": "dnotitia/Llama-DNA-1.0-8B-Instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:58:44.594769",
      "end_time": "2026-01-21T22:11:31.635173",
      "duration_seconds": 767.04,
      "vllm_pid": 741143,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_221004",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 702.85,
        "total_seconds": 767.04
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "arcee-ai/Llama-3.1-SuperNova-Lite": {
      "model_id": "arcee-ai/Llama-3.1-SuperNova-Lite",
      "model_name": "arcee-ai/Llama-3.1-SuperNova-Lite",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:40:49.156256",
      "end_time": "2026-01-21T22:12:06.514451",
      "duration_seconds": 1877.36,
      "vllm_pid": 1376055,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_220854",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1808.38,
        "total_seconds": 1877.36
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "instruction-pretrain/finance-Llama3-8B": {
      "model_id": "instruction-pretrain/finance-Llama3-8B",
      "model_name": "instruction-pretrain/finance-Llama3-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T22:12:06.518599",
      "end_time": "2026-01-21T22:12:06.763515",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "instruction-pretrain/medicine-Llama3-8B": {
      "model_id": "instruction-pretrain/medicine-Llama3-8B",
      "model_name": "instruction-pretrain/medicine-Llama3-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T22:12:06.839703",
      "end_time": "2026-01-21T22:12:07.051558",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "LGAI-EXAONE/EXAONE-Deep-7.8B": {
      "model_id": "LGAI-EXAONE/EXAONE-Deep-7.8B",
      "model_name": "LGAI-EXAONE/EXAONE-Deep-7.8B",
      "size_bytes": 7818448896,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:24:29.797711",
      "end_time": "2026-01-21T22:12:52.959813",
      "duration_seconds": 6503.16,
      "vllm_pid": 1211827,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_220854",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 6436.79,
        "total_seconds": 6503.16
      },
      "gpu_memory_gb": 907.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "XiaomiMiMo/MiMo-7B-RL": {
      "model_id": "XiaomiMiMo/MiMo-7B-RL",
      "model_name": "XiaomiMiMo/MiMo-7B-RL",
      "size_bytes": 7833409536,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:27:43.499987",
      "end_time": "2026-01-21T22:15:25.887679",
      "duration_seconds": 6462.39,
      "vllm_pid": 1219166,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_221352",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 6395.82,
        "total_seconds": 6462.39
      },
      "gpu_memory_gb": 1019.22,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ValiantLabs/Llama3.1-8B-ShiningValiant2": {
      "model_id": "ValiantLabs/Llama3.1-8B-ShiningValiant2",
      "model_name": "ValiantLabs/Llama3.1-8B-ShiningValiant2",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:38:11.991863",
      "end_time": "2026-01-21T22:18:28.707702",
      "duration_seconds": 2416.72,
      "vllm_pid": 698555,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_221231",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2335.15,
        "total_seconds": 2416.72
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "XiaomiMiMo/MiMo-7B-RL-0530": {
      "model_id": "XiaomiMiMo/MiMo-7B-RL-0530",
      "model_name": "XiaomiMiMo/MiMo-7B-RL-0530",
      "size_bytes": 7833409536,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:31:16.065287",
      "end_time": "2026-01-21T22:19:32.078539",
      "duration_seconds": 6496.01,
      "vllm_pid": 1226855,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_221629",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 6424.28,
        "total_seconds": 6496.01
      },
      "gpu_memory_gb": 1019.28,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "hfl/llama-3-chinese-8b-instruct-v3": {
      "model_id": "hfl/llama-3-chinese-8b-instruct-v3",
      "model_name": "hfl/llama-3-chinese-8b-instruct-v3",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:09:07.543862",
      "end_time": "2026-01-21T22:25:40.414812",
      "duration_seconds": 992.87,
      "vllm_pid": 763224,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_221946",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 928.26,
        "total_seconds": 992.87
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "facebook/KernelLLM": {
      "model_id": "facebook/KernelLLM",
      "model_name": "facebook/KernelLLM",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:07:52.711387",
      "end_time": "2026-01-21T22:26:40.323519",
      "duration_seconds": 1127.61,
      "vllm_pid": 1432510,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_222029",
      "timing": {
        "server_start_seconds": 60.04,
        "test_execution_seconds": 1058.41,
        "total_seconds": 1127.61
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2-math-7b": {
      "model_id": "internlm/internlm2-math-7b",
      "model_name": "internlm/internlm2-math-7b",
      "size_bytes": 7737708544,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T20:15:07.063460",
      "end_time": "2026-01-21T22:32:40.147849",
      "duration_seconds": 8253.08,
      "vllm_pid": 1188882,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_222742",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 8181.04,
        "total_seconds": 8253.08
      },
      "gpu_memory_gb": 1018.92,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mlabonne/Daredevil-8B": {
      "model_id": "mlabonne/Daredevil-8B",
      "model_name": "mlabonne/Daredevil-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:19:32.082931",
      "end_time": "2026-01-21T22:33:02.587919",
      "duration_seconds": 810.5,
      "vllm_pid": 1458018,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_222742",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 746.31,
        "total_seconds": 810.5
      },
      "gpu_memory_gb": 1019.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "meta-llama/Meta-Llama-3-8B-Instruct": {
      "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
      "model_name": "meta-llama/Meta-Llama-3-8B-Instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:15:25.892232",
      "end_time": "2026-01-21T22:33:32.443091",
      "duration_seconds": 1086.55,
      "vllm_pid": 1448980,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_222742",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1015.96,
        "total_seconds": 1086.55
      },
      "gpu_memory_gb": 1019.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "rinna/llama-3-youko-8b": {
      "model_id": "rinna/llama-3-youko-8b",
      "model_name": "rinna/llama-3-youko-8b",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T22:33:32.448632",
      "end_time": "2026-01-21T22:33:32.514035",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "oumi-ai/HallOumi-8B": {
      "model_id": "oumi-ai/HallOumi-8B",
      "model_name": "oumi-ai/HallOumi-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T22:33:02.591566",
      "end_time": "2026-01-21T22:35:25.312884",
      "duration_seconds": 142.72,
      "vllm_pid": 1490348,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--oumi-ai--HallOumi-8B/snapshots/fa4488b0b51a46eeccc24f7eeb05b5967ba6e121` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--oumi-ai--HallOumi-8B/snapshots/fa4488b0b51a46eeccc24f7eeb05b5967ba6e121` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 30.05,
        "test_execution_seconds": 63.69,
        "total_seconds": 142.72
      },
      "gpu_memory_gb": 1019.09,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "marin-community/marin-8b-instruct": {
      "model_id": "marin-community/marin-8b-instruct",
      "model_name": "marin-community/marin-8b-instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:12:07.125385",
      "end_time": "2026-01-21T22:36:45.524823",
      "duration_seconds": 1478.4,
      "vllm_pid": 1441930,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_223616",
      "timing": {
        "server_start_seconds": 75.06,
        "test_execution_seconds": 1393.78,
        "total_seconds": 1478.4
      },
      "gpu_memory_gb": 1019.03,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mlabonne/NeuralDaredevil-8B-abliterated": {
      "model_id": "mlabonne/NeuralDaredevil-8B-abliterated",
      "model_name": "mlabonne/NeuralDaredevil-8B-abliterated",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:25:40.514178",
      "end_time": "2026-01-21T22:40:08.529108",
      "duration_seconds": 868.01,
      "vllm_pid": 797092,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "math_500": {
          "score": 0.3141
        },
        "gsm8k": {
          "score": 0.77
        },
        "gpqa_diamond": {
          "score": 0.32
        },
        "mmlu": {
          "score": 0.6784
        },
        "mmlu_pro": {
          "score": 0.4393
        }
      },
      "eval_output_dir": "outputs/20260121_222637",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 803.91,
        "total_seconds": 868.01
      },
      "gpu_memory_gb": 1029.57,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "tarun7r/Finance-Llama-8B": {
      "model_id": "tarun7r/Finance-Llama-8B",
      "model_name": "tarun7r/Finance-Llama-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T22:40:08.578830",
      "end_time": "2026-01-21T22:40:08.686722",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "UnfilteredAI/DAN-L3-R1-8B": {
      "model_id": "UnfilteredAI/DAN-L3-R1-8B",
      "model_name": "UnfilteredAI/DAN-L3-R1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:36:57.955104",
      "end_time": "2026-01-21T22:40:44.987984",
      "duration_seconds": 3827.03,
      "vllm_pid": 1368076,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_223743",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 3760.54,
        "total_seconds": 3827.03
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2_5-20b-chat": {
      "model_id": "internlm/internlm2_5-20b-chat",
      "model_name": "internlm/internlm2_5-20b-chat",
      "size_bytes": 19861149696,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T22:04:24.816715",
      "end_time": "2026-01-21T22:44:32.349818",
      "duration_seconds": 2407.53,
      "vllm_pid": 752966,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_224105",
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": 2295.9,
        "total_seconds": 2407.53
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/Llama-3.1-8B-Instruct-FP8": {
      "model_id": "nvidia/Llama-3.1-8B-Instruct-FP8",
      "model_name": "nvidia/Llama-3.1-8B-Instruct-FP8",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:26:40.328615",
      "end_time": "2026-01-21T22:48:49.124546",
      "duration_seconds": 1328.8,
      "vllm_pid": 1476003,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_224147",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1259.83,
        "total_seconds": 1328.8
      },
      "gpu_memory_gb": 1018.92,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "scb10x/llama-3-typhoon-v1.5-8b-instruct": {
      "model_id": "scb10x/llama-3-typhoon-v1.5-8b-instruct",
      "model_name": "scb10x/llama-3-typhoon-v1.5-8b-instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:35:25.317173",
      "end_time": "2026-01-21T22:49:00.054703",
      "duration_seconds": 814.74,
      "vllm_pid": 1495413,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_224147",
      "timing": {
        "server_start_seconds": 50.05,
        "test_execution_seconds": 755.84,
        "total_seconds": 814.74
      },
      "gpu_memory_gb": 1019.03,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/Meta-Llama-3.1-8B": {
      "model_id": "unsloth/Meta-Llama-3.1-8B",
      "model_name": "unsloth/Meta-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T22:49:00.058461",
      "end_time": "2026-01-21T22:49:00.140067",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Meta-Llama-3.1-8B-Instruct": {
      "model_id": "unsloth/Meta-Llama-3.1-8B-Instruct",
      "model_name": "unsloth/Meta-Llama-3.1-8B-Instruct",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T22:49:00.149188",
      "end_time": "2026-01-21T22:52:02.553215",
      "duration_seconds": 182.4,
      "vllm_pid": 1527251,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/a2856192dd7c25b842431f39c179a6c2c2f627d1` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/a2856192dd7c25b842431f39c179a6c2c2f627d1` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 45.05,
        "test_execution_seconds": 98.7,
        "total_seconds": 182.4
      },
      "gpu_memory_gb": 1019.1,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "rombodawg/Llama-3-8B-Instruct-Coder": {
      "model_id": "rombodawg/Llama-3-8B-Instruct-Coder",
      "model_name": "rombodawg/Llama-3-8B-Instruct-Coder",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:33:32.528603",
      "end_time": "2026-01-21T22:52:26.836962",
      "duration_seconds": 1134.31,
      "vllm_pid": 1491186,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_225016",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1070.27,
        "total_seconds": 1134.31
      },
      "gpu_memory_gb": 1019.08,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mims-harvard/TxAgent-T1-Llama-3.1-8B": {
      "model_id": "mims-harvard/TxAgent-T1-Llama-3.1-8B",
      "model_name": "mims-harvard/TxAgent-T1-Llama-3.1-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:18:28.783347",
      "end_time": "2026-01-21T22:52:34.764155",
      "duration_seconds": 2045.98,
      "vllm_pid": 782864,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_224714",
      "timing": {
        "server_start_seconds": 75.08,
        "test_execution_seconds": 1961.31,
        "total_seconds": 2045.98
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/DeepSeek-R1-Distill-Llama-8B": {
      "model_id": "unsloth/DeepSeek-R1-Distill-Llama-8B",
      "model_name": "unsloth/DeepSeek-R1-Distill-Llama-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T22:48:49.131053",
      "end_time": "2026-01-21T22:52:42.129613",
      "duration_seconds": 233.0,
      "vllm_pid": 1526606,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/7355e03ea25f9b9282d5ac3ec3a512106004b718` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/7355e03ea25f9b9282d5ac3ec3a512106004b718` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 166.84,
        "total_seconds": 233.0
      },
      "gpu_memory_gb": 1014.53,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "microsoft/UserLM-8b": {
      "model_id": "microsoft/UserLM-8b",
      "model_name": "microsoft/UserLM-8b",
      "size_bytes": 8030269440,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-21T22:52:26.841800",
      "end_time": "2026-01-21T22:54:51.603953",
      "duration_seconds": 144.76,
      "vllm_pid": 1535469,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--microsoft--UserLM-8b/snapshots/be8f2069189bdf443e554c24e488ff3ff6952691` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--microsoft--UserLM-8b/snapshots/be8f2069189bdf443e554c24e488ff3ff6952691` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 75.06,
        "test_execution_seconds": 51.12,
        "total_seconds": 144.76
      },
      "gpu_memory_gb": 1019.16,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3": {
      "model_id": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3",
      "model_name": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:40:44.992672",
      "end_time": "2026-01-21T22:59:08.318771",
      "duration_seconds": 1103.33,
      "vllm_pid": 1507917,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_225551",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1033.49,
        "total_seconds": 1103.33
      },
      "gpu_memory_gb": 1019.16,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA": {
      "model_id": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
      "model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:36:45.529913",
      "end_time": "2026-01-21T23:04:00.897871",
      "duration_seconds": 1635.37,
      "vllm_pid": 1498590,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_230005",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1570.04,
        "total_seconds": 1635.37
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/Llama-3.1-Nemotron-Nano-8B-v1": {
      "model_id": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
      "model_name": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:32:40.152909",
      "end_time": "2026-01-21T23:05:31.373480",
      "duration_seconds": 1971.22,
      "vllm_pid": 1489052,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_230458",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1904.66,
        "total_seconds": 1971.22
      },
      "gpu_memory_gb": 1019.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "tencent/Hunyuan-MT-Chimera-7B": {
      "model_id": "tencent/Hunyuan-MT-Chimera-7B",
      "model_name": "tencent/Hunyuan-MT-Chimera-7B",
      "size_bytes": 8030269440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:52:42.132921",
      "end_time": "2026-01-21T23:07:24.731449",
      "duration_seconds": 882.6,
      "vllm_pid": 1535547,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_230628",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 803.86,
        "total_seconds": 882.6
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1": {
      "model_id": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1",
      "model_name": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:40:08.697378",
      "end_time": "2026-01-21T23:07:27.997214",
      "duration_seconds": 1639.3,
      "vllm_pid": 827337,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_225337",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1575.21,
        "total_seconds": 1639.3
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ai-sage/GigaChat-20B-A3B-instruct": {
      "model_id": "ai-sage/GigaChat-20B-A3B-instruct",
      "model_name": "ai-sage/GigaChat-20B-A3B-instruct",
      "size_bytes": 20589299712,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T22:44:32.398360",
      "end_time": "2026-01-21T23:07:44.089144",
      "duration_seconds": 1391.69,
      "vllm_pid": 836111,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_225337",
      "timing": {
        "server_start_seconds": 160.07,
        "test_execution_seconds": 1222.31,
        "total_seconds": 1391.69
      },
      "gpu_memory_gb": 1028.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ParasiticRogue/Merged-RP-Stew-V2-34B-exl2-4.65-fix": {
      "model_id": "ParasiticRogue/Merged-RP-Stew-V2-34B-exl2-4.65-fix",
      "model_name": "ParasiticRogue/Merged-RP-Stew-V2-34B-exl2-4.65-fix",
      "size_bytes": 20739554721,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-21T23:07:44.097318",
      "end_time": "2026-01-21T23:07:44.244128",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "lmstudio-community/gpt-oss-20b-MLX-8bit": {
      "model_id": "lmstudio-community/gpt-oss-20b-MLX-8bit",
      "model_name": "lmstudio-community/gpt-oss-20b-MLX-8bit",
      "size_bytes": 20914755648,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-21T23:07:44.253947",
      "end_time": "2026-01-21T23:09:04.934196",
      "duration_seconds": 80.68,
      "vllm_pid": 884163,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--lmstudio-community--gpt-oss-20b-MLX-8bit/snapshots/d23249e3b54df6248c79b67c155bab2f836742dd` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 35.04,
        "test_execution_seconds": null,
        "total_seconds": 80.68
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "UNIVA-Bllossom/DeepSeek-llama3.1-Bllossom-8B": {
      "model_id": "UNIVA-Bllossom/DeepSeek-llama3.1-Bllossom-8B",
      "model_name": "UNIVA-Bllossom/DeepSeek-llama3.1-Bllossom-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:30:20.757647",
      "end_time": "2026-01-21T23:09:43.749406",
      "duration_seconds": 5962.99,
      "vllm_pid": 681470,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_230824",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 5890.71,
        "total_seconds": 5962.99
      },
      "gpu_memory_gb": 812.42,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "S4nfs/Neeto-1.0-8b": {
      "model_id": "S4nfs/Neeto-1.0-8b",
      "model_name": "S4nfs/Neeto-1.0-8b",
      "size_bytes": 8030269440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:52:02.557550",
      "end_time": "2026-01-21T23:17:26.944552",
      "duration_seconds": 1524.39,
      "vllm_pid": 1534660,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_230822",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1459.19,
        "total_seconds": 1524.39
      },
      "gpu_memory_gb": 1019.23,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "tencent/Hunyuan-MT-7B": {
      "model_id": "tencent/Hunyuan-MT-7B",
      "model_name": "tencent/Hunyuan-MT-7B",
      "size_bytes": 8030269440,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:52:34.837722",
      "end_time": "2026-01-21T23:18:17.299767",
      "duration_seconds": 1542.46,
      "vllm_pid": 852987,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_231136",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1472.91,
        "total_seconds": 1542.46
      },
      "gpu_memory_gb": 1028.82,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "fdtn-ai/Foundation-Sec-8B": {
      "model_id": "fdtn-ai/Foundation-Sec-8B",
      "model_name": "fdtn-ai/Foundation-Sec-8B",
      "size_bytes": 8031309824,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T23:18:17.345519",
      "end_time": "2026-01-21T23:18:17.469929",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yentinglin/Llama-3-Taiwan-8B-Instruct": {
      "model_id": "yentinglin/Llama-3-Taiwan-8B-Instruct",
      "model_name": "yentinglin/Llama-3-Taiwan-8B-Instruct",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:59:08.324173",
      "end_time": "2026-01-21T23:18:55.993965",
      "duration_seconds": 1187.67,
      "vllm_pid": 1550613,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_231823",
      "timing": {
        "server_start_seconds": 55.06,
        "test_execution_seconds": 1122.99,
        "total_seconds": 1187.67
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "kakaocorp/kanana-safeguard-8b": {
      "model_id": "kakaocorp/kanana-safeguard-8b",
      "model_name": "kakaocorp/kanana-safeguard-8b",
      "size_bytes": 8030326784,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:17:26.948652",
      "end_time": "2026-01-21T23:20:22.195273",
      "duration_seconds": 175.25,
      "vllm_pid": 1591658,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_231959",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 111.69,
        "total_seconds": 175.25
      },
      "gpu_memory_gb": 1019.16,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "allenai/Llama-3.1-Tulu-3-8B": {
      "model_id": "allenai/Llama-3.1-Tulu-3-8B",
      "model_name": "allenai/Llama-3.1-Tulu-3-8B",
      "size_bytes": 8030326784,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:05:31.378188",
      "end_time": "2026-01-21T23:23:02.841742",
      "duration_seconds": 1051.46,
      "vllm_pid": 1564784,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_232122",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 987.09,
        "total_seconds": 1051.46
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "allenai/Llama-3.1-Tulu-3-8B-DPO": {
      "model_id": "allenai/Llama-3.1-Tulu-3-8B-DPO",
      "model_name": "allenai/Llama-3.1-Tulu-3-8B-DPO",
      "size_bytes": 8030326784,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:07:24.736328",
      "end_time": "2026-01-21T23:25:05.898672",
      "duration_seconds": 1061.16,
      "vllm_pid": 1568809,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_232400",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 996.54,
        "total_seconds": 1061.16
      },
      "gpu_memory_gb": 1019.23,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct": {
      "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
      "model_name": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct",
      "size_bytes": 8036552704,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T23:25:05.907296",
      "end_time": "2026-01-21T23:26:21.154931",
      "duration_seconds": 75.25,
      "vllm_pid": 1610208,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:  14% Completed | 1/7 [00:04<00:25,  4.32s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:  29% Completed | 2/7 [00:08<00:21,  4.31s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:  43% Completed | 3/7 [00:13<00:17,  4.37s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:  57% Completed | 4/7 [00:17<00:12,  4.32s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:  71% Completed | 5/7 [00:19<00:07,  3.63s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards:  86% Completed | 6/7 [00:24<00:03,  3.88s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 7/7 [00:28<00:00,  4.07s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 7/7 [00:28<00:00,  4.08s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=1610317)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1073152), (131.00 GiB KV cache is needed, which is larger than the available KV cache memory (105.69 GiB). Based on the available memory, the estimated maximum model length is 865760. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W121 23:26:12.309004071 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1610208)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 75.25
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct": {
      "model_id": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
      "model_name": "nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct",
      "size_bytes": 8036552704,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-21T23:26:21.160103",
      "end_time": "2026-01-21T23:27:41.367404",
      "duration_seconds": 80.21,
      "vllm_pid": 1612573,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:  14% Completed | 1/7 [00:02<00:13,  2.29s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:  29% Completed | 2/7 [00:06<00:17,  3.58s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:  43% Completed | 3/7 [00:11<00:16,  4.00s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:  57% Completed | 4/7 [00:15<00:12,  4.12s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:  71% Completed | 5/7 [00:20<00:08,  4.24s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards:  86% Completed | 6/7 [00:24<00:04,  4.30s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 7/7 [00:28<00:00,  4.31s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 7/7 [00:28<00:00,  4.11s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=1613161)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (4292608), (524.00 GiB KV cache is needed, which is larger than the available KV cache memory (104.91 GiB). Based on the available memory, the estimated maximum model length is 859440. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W121 23:27:30.991281874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1612573)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 80.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Llama-3.1-Tulu-3.1-8B": {
      "model_id": "allenai/Llama-3.1-Tulu-3.1-8B",
      "model_name": "allenai/Llama-3.1-Tulu-3.1-8B",
      "size_bytes": 8030326784,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:09:43.806383",
      "end_time": "2026-01-21T23:28:21.740896",
      "duration_seconds": 1117.93,
      "vllm_pid": 888316,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_231914",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1048.14,
        "total_seconds": 1117.93
      },
      "gpu_memory_gb": 1028.86,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "swiss-ai/Apertus-8B-2509": {
      "model_id": "swiss-ai/Apertus-8B-2509",
      "model_name": "swiss-ai/Apertus-8B-2509",
      "size_bytes": 8053338176,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T23:28:21.788414",
      "end_time": "2026-01-21T23:28:21.847062",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/Llama-3.1-Tulu-3-8B-SFT": {
      "model_id": "allenai/Llama-3.1-Tulu-3-8B-SFT",
      "model_name": "allenai/Llama-3.1-Tulu-3-8B-SFT",
      "size_bytes": 8030326784,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:07:28.096088",
      "end_time": "2026-01-21T23:28:42.076125",
      "duration_seconds": 1273.98,
      "vllm_pid": 883610,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_231914",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1209.95,
        "total_seconds": 1273.98
      },
      "gpu_memory_gb": 901.44,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "deepseek-ai/DeepSeek-R1-Distill-Llama-8B": {
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T21:52:48.543750",
      "end_time": "2026-01-21T23:29:57.588571",
      "duration_seconds": 5829.04,
      "vllm_pid": 1401701,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_232841",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 5757.44,
        "total_seconds": 5829.04
      },
      "gpu_memory_gb": 1019.17,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Tesslate/WEBGEN-OSS-20B": {
      "model_id": "Tesslate/WEBGEN-OSS-20B",
      "model_name": "Tesslate/WEBGEN-OSS-20B",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T23:09:05.004009",
      "end_time": "2026-01-21T23:30:29.627106",
      "duration_seconds": 1284.62,
      "vllm_pid": 887321,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_232954",
      "timing": {
        "server_start_seconds": 150.07,
        "test_execution_seconds": 1125.99,
        "total_seconds": 1284.62
      },
      "gpu_memory_gb": 1028.82,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "kakaocorp/kanana-1.5-8b-instruct-2505": {
      "model_id": "kakaocorp/kanana-1.5-8b-instruct-2505",
      "model_name": "kakaocorp/kanana-1.5-8b-instruct-2505",
      "size_bytes": 8030285824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:04:00.903366",
      "end_time": "2026-01-21T23:31:57.660872",
      "duration_seconds": 1676.76,
      "vllm_pid": 1560950,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_233118",
      "timing": {
        "server_start_seconds": 55.06,
        "test_execution_seconds": 1612.43,
        "total_seconds": 1676.76
      },
      "gpu_memory_gb": 1019.25,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "fdtn-ai/Foundation-Sec-8B-Instruct": {
      "model_id": "fdtn-ai/Foundation-Sec-8B-Instruct",
      "model_name": "fdtn-ai/Foundation-Sec-8B-Instruct",
      "size_bytes": 8031309824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:18:17.479175",
      "end_time": "2026-01-21T23:41:22.126258",
      "duration_seconds": 1384.65,
      "vllm_pid": 907963,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_233238",
      "timing": {
        "server_start_seconds": 55.08,
        "test_execution_seconds": 1320.48,
        "total_seconds": 1384.65
      },
      "gpu_memory_gb": 1028.86,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-3.1-8b-base": {
      "model_id": "ibm-granite/granite-3.1-8b-base",
      "model_name": "ibm-granite/granite-3.1-8b-base",
      "size_bytes": 8170835968,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T23:41:22.146371",
      "end_time": "2026-01-21T23:41:22.270720",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-granite/granite-3.3-8b-base": {
      "model_id": "ibm-granite/granite-3.3-8b-base",
      "model_name": "ibm-granite/granite-3.3-8b-base",
      "size_bytes": 8170835968,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-21T23:41:22.280625",
      "end_time": "2026-01-21T23:41:22.384117",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it": {
      "model_id": "Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it",
      "model_name": "Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it",
      "size_bytes": 8036552704,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:23:02.845520",
      "end_time": "2026-01-21T23:47:27.857056",
      "duration_seconds": 1465.01,
      "vllm_pid": 1605829,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_233254",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1400.27,
        "total_seconds": 1465.01
      },
      "gpu_memory_gb": 1019.21,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/DeepSeek-R1-Distill-Llama-8B-abliterated": {
      "model_id": "huihui-ai/DeepSeek-R1-Distill-Llama-8B-abliterated",
      "model_name": "huihui-ai/DeepSeek-R1-Distill-Llama-8B-abliterated",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:11:31.653165",
      "end_time": "2026-01-21T23:48:22.124011",
      "duration_seconds": 5810.47,
      "vllm_pid": 768296,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_234229",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 5743.76,
        "total_seconds": 5810.47
      },
      "gpu_memory_gb": 1029.56,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mergekit-community/Deepseek-R1-Distill-NSFW-RPv1": {
      "model_id": "mergekit-community/Deepseek-R1-Distill-NSFW-RPv1",
      "model_name": "mergekit-community/Deepseek-R1-Distill-NSFW-RPv1",
      "size_bytes": 8030261248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:12:52.971289",
      "end_time": "2026-01-21T23:48:45.548615",
      "duration_seconds": 5752.58,
      "vllm_pid": 1442814,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_234834",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 5686.07,
        "total_seconds": 5752.58
      },
      "gpu_memory_gb": 1019.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "numind/NuExtract-2-8B-experimental": {
      "model_id": "numind/NuExtract-2-8B-experimental",
      "model_name": "numind/NuExtract-2-8B-experimental",
      "size_bytes": 8075365376,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:28:42.089580",
      "end_time": "2026-01-21T23:51:00.062936",
      "duration_seconds": 1337.97,
      "vllm_pid": 929325,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_234918",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 1258.65,
        "total_seconds": 1337.97
      },
      "gpu_memory_gb": 1028.97,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16": {
      "model_id": "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",
      "model_name": "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",
      "size_bytes": 8031637504,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:18:55.997955",
      "end_time": "2026-01-21T23:52:16.062496",
      "duration_seconds": 2000.06,
      "vllm_pid": 1595082,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_234946",
      "timing": {
        "server_start_seconds": 60.06,
        "test_execution_seconds": 1929.57,
        "total_seconds": 2000.06
      },
      "gpu_memory_gb": 1019.41,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-guardian-3.0-8b": {
      "model_id": "ibm-granite/granite-guardian-3.0-8b",
      "model_name": "ibm-granite/granite-guardian-3.0-8b",
      "size_bytes": 8170848256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:48:45.553153",
      "end_time": "2026-01-21T23:53:19.465931",
      "duration_seconds": 273.91,
      "vllm_pid": 1659842,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_235312",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 205.61,
        "total_seconds": 273.91
      },
      "gpu_memory_gb": 1019.54,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "swiss-ai/Apertus-8B-Instruct-2509": {
      "model_id": "swiss-ai/Apertus-8B-Instruct-2509",
      "model_name": "swiss-ai/Apertus-8B-Instruct-2509",
      "size_bytes": 8053338176,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:28:21.857951",
      "end_time": "2026-01-21T23:53:29.714328",
      "duration_seconds": 1507.86,
      "vllm_pid": 929162,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_235203",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 1424.06,
        "total_seconds": 1507.86
      },
      "gpu_memory_gb": 1029.12,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-guardian-3.3-8b": {
      "model_id": "ibm-granite/granite-guardian-3.3-8b",
      "model_name": "ibm-granite/granite-guardian-3.3-8b",
      "size_bytes": 8170864640,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:52:16.069516",
      "end_time": "2026-01-21T23:57:31.926543",
      "duration_seconds": 315.86,
      "vllm_pid": 1668659,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_235421",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 252.52,
        "total_seconds": 315.86
      },
      "gpu_memory_gb": 1019.54,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-3.1-8b-instruct": {
      "model_id": "ibm-granite/granite-3.1-8b-instruct",
      "model_name": "ibm-granite/granite-3.1-8b-instruct",
      "size_bytes": 8170848256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:41:22.390715",
      "end_time": "2026-01-21T23:59:58.079402",
      "duration_seconds": 1115.69,
      "vllm_pid": 957274,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_235429",
      "timing": {
        "server_start_seconds": 65.08,
        "test_execution_seconds": 1041.61,
        "total_seconds": 1115.69
      },
      "gpu_memory_gb": 1029.08,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8": {
      "model_id": "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8",
      "model_name": "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8",
      "size_bytes": 8031637504,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:20:22.199335",
      "end_time": "2026-01-22T00:04:49.680211",
      "duration_seconds": 2667.48,
      "vllm_pid": 1599386,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_235846",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 2600.11,
        "total_seconds": 2667.48
      },
      "gpu_memory_gb": 1019.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-3.2-8b-instruct": {
      "model_id": "ibm-granite/granite-3.2-8b-instruct",
      "model_name": "ibm-granite/granite-3.2-8b-instruct",
      "size_bytes": 8170848256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:47:27.861355",
      "end_time": "2026-01-22T00:05:33.274925",
      "duration_seconds": 1085.41,
      "vllm_pid": 1657440,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260121_235846",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1011.51,
        "total_seconds": 1085.41
      },
      "gpu_memory_gb": 1019.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-3.2-8b-instruct-preview": {
      "model_id": "ibm-granite/granite-3.2-8b-instruct-preview",
      "model_name": "ibm-granite/granite-3.2-8b-instruct-preview",
      "size_bytes": 8170848256,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:48:22.175920",
      "end_time": "2026-01-22T00:05:46.087324",
      "duration_seconds": 1043.91,
      "vllm_pid": 971808,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_000102",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 979.91,
        "total_seconds": 1043.91
      },
      "gpu_memory_gb": 1028.99,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/Nemotron-H-8B-Reasoning-128K": {
      "model_id": "nvidia/Nemotron-H-8B-Reasoning-128K",
      "model_name": "nvidia/Nemotron-H-8B-Reasoning-128K",
      "size_bytes": 8100852736,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:31:57.665088",
      "end_time": "2026-01-22T00:12:47.500210",
      "duration_seconds": 2449.84,
      "vllm_pid": 1624995,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_000652",
      "timing": {
        "server_start_seconds": 50.04,
        "test_execution_seconds": 2385.54,
        "total_seconds": 2449.84
      },
      "gpu_memory_gb": 1019.06,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-granite/granite-3.3-8b-instruct": {
      "model_id": "ibm-granite/granite-3.3-8b-instruct",
      "model_name": "ibm-granite/granite-3.3-8b-instruct",
      "size_bytes": 8170864640,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:51:00.086148",
      "end_time": "2026-01-22T00:15:06.776974",
      "duration_seconds": 1446.69,
      "vllm_pid": 977141,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_000646",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1376.51,
        "total_seconds": 1446.69
      },
      "gpu_memory_gb": 1029.11,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "openbmb/MiniCPM4-MCP": {
      "model_id": "openbmb/MiniCPM4-MCP",
      "model_name": "openbmb/MiniCPM4-MCP",
      "size_bytes": 8185253888,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:53:19.469736",
      "end_time": "2026-01-22T00:20:39.530221",
      "duration_seconds": 1640.06,
      "vllm_pid": 1671338,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_001351",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1570.84,
        "total_seconds": 1640.06
      },
      "gpu_memory_gb": 1019.32,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "secretmoon/YankaGPT-8B-v0.1": {
      "model_id": "secretmoon/YankaGPT-8B-v0.1",
      "model_name": "secretmoon/YankaGPT-8B-v0.1",
      "size_bytes": 8036552704,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:27:41.373275",
      "end_time": "2026-01-22T00:21:17.360639",
      "duration_seconds": 3215.99,
      "vllm_pid": 1615277,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_001351",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 3148.88,
        "total_seconds": 3215.99
      },
      "gpu_memory_gb": 1019.13,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/Nemotron-H-8B-Base-8K": {
      "model_id": "nvidia/Nemotron-H-8B-Base-8K",
      "model_name": "nvidia/Nemotron-H-8B-Base-8K",
      "size_bytes": 8100852736,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:29:57.593441",
      "end_time": "2026-01-22T00:35:23.569912",
      "duration_seconds": 3925.98,
      "vllm_pid": 1620320,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_002215",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 3838.5,
        "total_seconds": 3925.98
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm2-math-20b": {
      "model_id": "internlm/internlm2-math-20b",
      "model_name": "internlm/internlm2-math-20b",
      "size_bytes": 19861149696,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T21:30:40.858581",
      "end_time": "2026-01-22T00:50:22.299769",
      "duration_seconds": 11981.44,
      "vllm_pid": 681869,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_001606",
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": 11863.44,
        "total_seconds": 11981.44
      },
      "gpu_memory_gb": 1029.53,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Akhil-Theerthala/Kuvera-8B-v0.1.0": {
      "model_id": "Akhil-Theerthala/Kuvera-8B-v0.1.0",
      "model_name": "Akhil-Theerthala/Kuvera-8B-v0.1.0",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:57:31.930686",
      "end_time": "2026-01-22T00:54:39.960621",
      "duration_seconds": 3428.03,
      "vllm_pid": 1681896,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_003627",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 3346.53,
        "total_seconds": 3428.03
      },
      "gpu_memory_gb": 1019.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-8B-AWQ": {
      "model_id": "Qwen/Qwen3-8B-AWQ",
      "model_name": "Qwen/Qwen3-8B-AWQ",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T00:54:39.964858",
      "end_time": "2026-01-22T00:55:15.140074",
      "duration_seconds": 35.18,
      "vllm_pid": 1797190,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.40s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  2.91s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.13s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 56, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     process_weights_after_loading(model, model_config, target_device)\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 108, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     quant_method.process_weights_after_loading(module)\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py\", line 385, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     marlin_scales = marlin_permute_scales(\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 302, in marlin_permute_scales\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m     s = s.reshape((-1, len(scale_perm)))[:, scale_perm]\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=1797271)\u001b[0;0m \n[rank0]:[W122 00:55:06.906976542 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1797190)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 35.18
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ValiantLabs/gpt-oss-20b-ShiningValiant3": {
      "model_id": "ValiantLabs/gpt-oss-20b-ShiningValiant3",
      "model_name": "ValiantLabs/gpt-oss-20b-ShiningValiant3",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-21T23:30:29.701471",
      "end_time": "2026-01-22T00:55:48.754221",
      "duration_seconds": 5119.05,
      "vllm_pid": 934130,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_005223",
      "timing": {
        "server_start_seconds": 125.06,
        "test_execution_seconds": 4982.82,
        "total_seconds": 5119.05
      },
      "gpu_memory_gb": 1028.72,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "aoxo/gpt-oss-20b-uncensored": {
      "model_id": "aoxo/gpt-oss-20b-uncensored",
      "model_name": "aoxo/gpt-oss-20b-uncensored",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-22T00:50:22.347573",
      "end_time": "2026-01-22T01:11:53.518284",
      "duration_seconds": 1291.17,
      "vllm_pid": 1094635,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 500 - {'error': {'message': 'Unknown role: final', 'type': 'Internal Server Error', 'param': None, 'code': 500}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500 - {'error': {'message': 'Unknown role: final', 'type': 'Internal Server Error', 'param': None, 'code': 500}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 120.07,
        "test_execution_seconds": 1162.82,
        "total_seconds": 1291.17
      },
      "gpu_memory_gb": 1028.1,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "K-intelligence/Llama-SafetyGuard-Content-Binary": {
      "model_id": "K-intelligence/Llama-SafetyGuard-Content-Binary",
      "model_name": "K-intelligence/Llama-SafetyGuard-Content-Binary",
      "size_bytes": 8030277632,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T22:54:51.614472",
      "end_time": "2026-01-22T01:13:15.165602",
      "duration_seconds": 8303.55,
      "vllm_pid": 1540470,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_005613",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 8236.92,
        "total_seconds": 8303.55
      },
      "gpu_memory_gb": 1019.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Kwaipilot/HiPO-8B": {
      "model_id": "Kwaipilot/HiPO-8B",
      "model_name": "Kwaipilot/HiPO-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:21:17.365699",
      "end_time": "2026-01-22T01:16:10.650226",
      "duration_seconds": 3293.28,
      "vllm_pid": 1730997,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_011411",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 3227.89,
        "total_seconds": 3293.28
      },
      "gpu_memory_gb": 1019.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ArliAI/DS-R1-Qwen3-8B-ArliAI-RpR-v4-Small": {
      "model_id": "ArliAI/DS-R1-Qwen3-8B-ArliAI-RpR-v4-Small",
      "model_name": "ArliAI/DS-R1-Qwen3-8B-ArliAI-RpR-v4-Small",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:04:49.684548",
      "end_time": "2026-01-22T01:16:29.235182",
      "duration_seconds": 4299.55,
      "vllm_pid": 1696856,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_011411",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 4232.9,
        "total_seconds": 4299.55
      },
      "gpu_memory_gb": 891.93,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3Guard-Gen-8B": {
      "model_id": "Qwen/Qwen3Guard-Gen-8B",
      "model_name": "Qwen/Qwen3Guard-Gen-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:13:15.169920",
      "end_time": "2026-01-22T01:17:44.136340",
      "duration_seconds": 268.97,
      "vllm_pid": 1834355,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_011710",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 205.38,
        "total_seconds": 268.97
      },
      "gpu_memory_gb": 1019.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Tesslate/UIGEN-X-8B": {
      "model_id": "Tesslate/UIGEN-X-8B",
      "model_name": "Tesslate/UIGEN-X-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T01:16:29.240505",
      "end_time": "2026-01-22T01:19:37.683281",
      "duration_seconds": 188.44,
      "vllm_pid": 1842042,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Tesslate--UIGEN-X-8B/snapshots/91de03e2e08d5e1ebfd6e739833db1ae6e52bc4d` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Tesslate--UIGEN-X-8B/snapshots/91de03e2e08d5e1ebfd6e739833db1ae6e52bc4d` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 40.04,
        "test_execution_seconds": 97.17,
        "total_seconds": 188.44
      },
      "gpu_memory_gb": 1019.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-8B-Base": {
      "model_id": "Qwen/Qwen3-8B-Base",
      "model_name": "Qwen/Qwen3-8B-Base",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:55:15.143514",
      "end_time": "2026-01-22T01:22:20.464251",
      "duration_seconds": 1625.32,
      "vllm_pid": 1798611,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_012102",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1560.19,
        "total_seconds": 1625.32
      },
      "gpu_memory_gb": 1019.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "openbmb/MiniCPM4.1-8B": {
      "model_id": "openbmb/MiniCPM4.1-8B",
      "model_name": "openbmb/MiniCPM4.1-8B",
      "size_bytes": 8185253888,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:53:29.798565",
      "end_time": "2026-01-22T01:25:57.566264",
      "duration_seconds": 5547.77,
      "vllm_pid": 982943,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_011356",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 5481.32,
        "total_seconds": 5547.77
      },
      "gpu_memory_gb": 1029.07,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Aratako/Qwen3-8B-NSFW-JP": {
      "model_id": "Aratako/Qwen3-8B-NSFW-JP",
      "model_name": "Aratako/Qwen3-8B-NSFW-JP",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-21T23:59:58.127758",
      "end_time": "2026-01-22T01:29:55.866715",
      "duration_seconds": 5397.74,
      "vllm_pid": 996540,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_012710",
      "timing": {
        "server_start_seconds": 60.08,
        "test_execution_seconds": 5325.97,
        "total_seconds": 5397.74
      },
      "gpu_memory_gb": 1028.99,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ByteDance-Seed/cudaLLM-8B": {
      "model_id": "ByteDance-Seed/cudaLLM-8B",
      "model_name": "ByteDance-Seed/cudaLLM-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:05:33.280392",
      "end_time": "2026-01-22T01:31:45.511921",
      "duration_seconds": 5172.23,
      "vllm_pid": 1698858,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_012324",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 5085.78,
        "total_seconds": 5172.23
      },
      "gpu_memory_gb": 1019.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Intelligent-Internet/II-Medical-8B": {
      "model_id": "Intelligent-Internet/II-Medical-8B",
      "model_name": "Intelligent-Internet/II-Medical-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:15:06.783822",
      "end_time": "2026-01-22T01:52:43.333639",
      "duration_seconds": 5856.55,
      "vllm_pid": 1027083,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_013059",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 5789.32,
        "total_seconds": 5856.55
      },
      "gpu_memory_gb": 1028.87,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Intelligent-Internet/II-Medical-8B-1706": {
      "model_id": "Intelligent-Internet/II-Medical-8B-1706",
      "model_name": "Intelligent-Internet/II-Medical-8B-1706",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:20:39.535280",
      "end_time": "2026-01-22T01:54:13.164768",
      "duration_seconds": 5613.63,
      "vllm_pid": 1729662,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_013249",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 5547.12,
        "total_seconds": 5613.63
      },
      "gpu_memory_gb": 1019.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated": {
      "model_id": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
      "model_name": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T00:55:48.775910",
      "end_time": "2026-01-22T01:57:25.511981",
      "duration_seconds": 3696.74,
      "vllm_pid": 1105438,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_015408",
      "timing": {
        "server_start_seconds": 120.06,
        "test_execution_seconds": 3566.73,
        "total_seconds": 3696.74
      },
      "gpu_memory_gb": 1028.1,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Goedel-LM/Goedel-Prover-V2-8B": {
      "model_id": "Goedel-LM/Goedel-Prover-V2-8B",
      "model_name": "Goedel-LM/Goedel-Prover-V2-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:05:46.146390",
      "end_time": "2026-01-22T02:03:56.355373",
      "duration_seconds": 7090.21,
      "vllm_pid": 1008860,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_015927",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 7022.9,
        "total_seconds": 7090.21
      },
      "gpu_memory_gb": 1028.93,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Goekdeniz-Guelmez/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1": {
      "model_id": "Goekdeniz-Guelmez/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1",
      "model_name": "Goekdeniz-Guelmez/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:12:47.508969",
      "end_time": "2026-01-22T02:03:57.674726",
      "duration_seconds": 6670.17,
      "vllm_pid": 1713602,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_015512",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 6598.5,
        "total_seconds": 6670.17
      },
      "gpu_memory_gb": 1019.38,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-8B": {
      "model_id": "Qwen/Qwen3-8B",
      "model_name": "Qwen/Qwen3-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T00:35:23.574593",
      "end_time": "2026-01-22T02:08:21.131696",
      "duration_seconds": 5577.56,
      "vllm_pid": 1759189,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_020454",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 5505.91,
        "total_seconds": 5577.56
      },
      "gpu_memory_gb": 1019.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2": {
      "model_id": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2",
      "model_name": "huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T01:11:53.559246",
      "end_time": "2026-01-22T02:19:38.898726",
      "duration_seconds": 4065.34,
      "vllm_pid": 1137119,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_020504",
      "timing": {
        "server_start_seconds": 120.06,
        "test_execution_seconds": 3935.54,
        "total_seconds": 4065.34
      },
      "gpu_memory_gb": 1028.23,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ilsp/Llama-Krikri-8B-Base": {
      "model_id": "ilsp/Llama-Krikri-8B-Base",
      "model_name": "ilsp/Llama-Krikri-8B-Base",
      "size_bytes": 8202227712,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:03:57.678985",
      "end_time": "2026-01-22T02:19:42.664185",
      "duration_seconds": 944.99,
      "vllm_pid": 1936480,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_020923",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 880.6,
        "total_seconds": 944.99
      },
      "gpu_memory_gb": 1019.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mlx-community/phi-4-4bit": {
      "model_id": "mlx-community/phi-4-4bit",
      "model_name": "mlx-community/phi-4-4bit",
      "size_bytes": 8246633911,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T02:19:42.667875",
      "end_time": "2026-01-22T02:20:12.843745",
      "duration_seconds": 30.18,
      "vllm_pid": 1969290,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 640, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     return loader.load_weights(\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m ValueError: There is no module or parameter named 'lm_head.biases' in Phi3ForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1969368)\u001b[0;0m \n[rank0]:[W122 02:20:01.145105872 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1969290)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.18
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit": {
      "model_id": "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
      "model_name": "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-22T02:19:38.912559",
      "end_time": "2026-01-22T02:20:19.167258",
      "duration_seconds": 40.25,
      "vllm_pid": 1269519,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n[rank0]:[W122 02:20:09.440547345 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=1270159)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1269519)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 40.25
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit": {
      "model_id": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
      "model_name": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
      "size_bytes": 8248929342,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T02:20:12.847123",
      "end_time": "2026-01-22T02:20:43.004761",
      "duration_seconds": 30.16,
      "vllm_pid": 1970602,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.model = self._init_model(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 611, in _init_model\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     return LlamaModel(vllm_config=vllm_config, prefix=prefix, layer_type=layer_type)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 393, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 395, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     lambda prefix: layer_type(vllm_config=vllm_config, prefix=prefix),\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 302, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.self_attn = LlamaAttention(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=1970741)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W122 02:20:31.103593937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1970602)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit": {
      "model_id": "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
      "model_name": "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
      "size_bytes": 8248929356,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T02:20:43.008264",
      "end_time": "2026-01-22T02:20:43.045339",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/Seed-Coder-8B-Base": {
      "model_id": "ByteDance-Seed/Seed-Coder-8B-Base",
      "model_name": "ByteDance-Seed/Seed-Coder-8B-Base",
      "size_bytes": 8250462208,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T02:20:43.121839",
      "end_time": "2026-01-22T02:20:43.236815",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/llama-3-8b-bnb-4bit": {
      "model_id": "unsloth/llama-3-8b-bnb-4bit",
      "model_name": "unsloth/llama-3-8b-bnb-4bit",
      "size_bytes": 8248929386,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T02:20:43.054962",
      "end_time": "2026-01-22T02:20:43.091889",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ilsp/Llama-Krikri-8B-Instruct": {
      "model_id": "ilsp/Llama-Krikri-8B-Instruct",
      "model_name": "ilsp/Llama-Krikri-8B-Instruct",
      "size_bytes": 8202227712,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:08:21.136008",
      "end_time": "2026-01-22T02:26:40.460115",
      "duration_seconds": 1099.32,
      "vllm_pid": 1945574,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_022140",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1029.2,
        "total_seconds": 1099.32
      },
      "gpu_memory_gb": 1019.26,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ValiantLabs/Qwen3-8B-Esper3": {
      "model_id": "ValiantLabs/Qwen3-8B-Esper3",
      "model_name": "ValiantLabs/Qwen3-8B-Esper3",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:19:37.691135",
      "end_time": "2026-01-22T02:30:53.660185",
      "duration_seconds": 4275.97,
      "vllm_pid": 1849125,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_022754",
      "timing": {
        "server_start_seconds": 80.05,
        "test_execution_seconds": 4184.22,
        "total_seconds": 4275.97
      },
      "gpu_memory_gb": 1019.33,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Trendyol/Trendyol-LLM-8B-T1": {
      "model_id": "Trendyol/Trendyol-LLM-8B-T1",
      "model_name": "Trendyol/Trendyol-LLM-8B-T1",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:17:44.140745",
      "end_time": "2026-01-22T02:34:31.073339",
      "duration_seconds": 4606.93,
      "vllm_pid": 1844970,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_023153",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 4535.54,
        "total_seconds": 4606.93
      },
      "gpu_memory_gb": 1019.33,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "RWKV-Red-Team/ARWKV-7B-Preview-0.1": {
      "model_id": "RWKV-Red-Team/ARWKV-7B-Preview-0.1",
      "model_name": "RWKV-Red-Team/ARWKV-7B-Preview-0.1",
      "size_bytes": 8291057152,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T02:34:31.077874",
      "end_time": "2026-01-22T02:34:46.347327",
      "duration_seconds": 15.27,
      "vllm_pid": 2000344,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m   Value error, Model architectures ['RwkvHybridForCausalLM'] are not supported for now. Supported architectures: dict_keys(['AfmoeForCausalLM', 'ApertusForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'ArceeForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BailingMoeForCausalLM', 'BailingMoeV2ForCausalLM', 'BambaForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CwmForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekV32ForCausalLM', 'Dots1ForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'ExaoneForCausalLM', 'Exaone4ForCausalLM', 'Fairseq2LlamaForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FalconH1ForCausalLM', 'FlexOlmoForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3nForCausalLM', 'Qwen3NextForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GptOssForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'GritLM', 'Grok1ModelForCausalLM', 'HunYuanMoEV1ForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HCXVisionForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'InternLM2VEForCausalLM', 'InternLM3ForCausalLM', 'JAISLMHeadModel', 'Jais2ForCausalLM', 'JambaForCausalLM', 'KimiLinearForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'LLaMAForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxText01ForCausalLM', 'MiniMaxM1ForCausalLM', 'MiniMaxM2ForCausalLM', 'MistralForCausalLM', 'MistralLarge3ForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiMoForCausalLM', 'NemotronForCausalLM', 'NemotronHForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'OuroForCausalLM', 'PanguEmbeddedForCausalLM', 'PanguUltraMoEForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Plamo2ForCausalLM', 'Plamo3ForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RWForCausalLM', 'SeedOssForCausalLM', 'Step3TextForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'TeleChatForCausalLM', 'TeleChat2ForCausalLM', 'TeleFLMForCausalLM', 'XverseForCausalLM', 'Zamba2ForCausalLM', 'BertModel', 'BertSpladeSparseEmbeddingModel', 'Gemma2Model', 'Gemma3TextModel', 'GPT2ForSequenceClassification', 'GteModel', 'GteNewModel', 'InternLM2ForRewardModel', 'JambaForSequenceClassification', 'LlamaModel', 'MistralModel', 'ModernBertModel', 'NomicBertModel', 'Qwen2Model', 'Qwen2ForRewardModel', 'Qwen2ForProcessRewardModel', 'RobertaForMaskedLM', 'RobertaModel', 'XLMRobertaModel', 'CLIPModel', 'LlavaNextForConditionalGeneration', 'Phi3VForCausalLM', 'Qwen2VLForConditionalGeneration', 'SiglipModel', 'PrithviGeoSpatialMAE', 'Terratorch', 'BertForSequenceClassification', 'BertForTokenClassification', 'GteNewForSequenceClassification', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'RobertaForSequenceClassification', 'XLMRobertaForSequenceClassification', 'JinaVLForRanking', 'AriaForConditionalGeneration', 'AudioFlamingo3ForConditionalGeneration', 'AyaVisionForConditionalGeneration', 'BagelForConditionalGeneration', 'BeeForConditionalGeneration', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'Cohere2VisionForConditionalGeneration', 'DeepseekVLV2ForCausalLM', 'DeepseekOCRForCausalLM', 'DotsOCRForCausalLM', 'Ernie4_5_VLMoeForConditionalGeneration', 'FuyuForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3nForConditionalGeneration', 'GLM4VForCausalLM', 'Glm4vForConditionalGeneration', 'Glm4vMoeForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'H2OVLChatModel', 'HunYuanVLForConditionalGeneration', 'InternVLChatModel', 'NemotronH_Nano_VL_V2', 'OpenCUAForConditionalGeneration', 'InternS1ForConditionalGeneration', 'InternVLForConditionalGeneration', 'Idefics3ForConditionalGeneration', 'SmolVLMForConditionalGeneration', 'KeyeForConditionalGeneration', 'KeyeVL1_5ForConditionalGeneration', 'RForConditionalGeneration', 'KimiVLForConditionalGeneration', 'LightOnOCRForConditionalGeneration', 'Llama_Nemotron_Nano_VL', 'Llama4ForConditionalGeneration', 'LlavaForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MantisForConditionalGeneration', 'MiDashengLMModel', 'MiniMaxVL01ForConditionalGeneration', 'MiniCPMO', 'MiniCPMV', 'Mistral3ForConditionalGeneration', 'MolmoForCausalLM', 'NVLM_D', 'Ovis', 'Ovis2_5', 'PaddleOCRVLForConditionalGeneration', 'PaliGemmaForConditionalGeneration', 'Phi4MMForCausalLM', 'PixtralForConditionalGeneration', 'QwenVLForConditionalGeneration', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'Qwen2_5OmniModel', 'Qwen2_5OmniForConditionalGeneration', 'Qwen3OmniMoeForConditionalGeneration', 'Qwen3VLForConditionalGeneration', 'Qwen3VLMoeForConditionalGeneration', 'SkyworkR1VChatModel', 'Step3VLForConditionalGeneration', 'TarsierForConditionalGeneration', 'Tarsier2ForConditionalGeneration', 'UltravoxModel', 'VoxtralForConditionalGeneration', 'WhisperForConditionalGeneration', 'MiMoMTPModel', 'EagleLlamaForCausalLM', 'EagleLlama4ForCausalLM', 'EagleMiniCPMForCausalLM', 'Eagle3LlamaForCausalLM', 'LlamaForCausalLMEagle3', 'Eagle3Qwen2_5vlForCausalLM', 'Eagle3Qwen3vlForCausalLM', 'EagleMistralLarge3ForCausalLM', 'EagleDeepSeekMTPModel', 'DeepSeekMTPModel', 'ErnieMTPModel', 'LongCatFlashMTPModel', 'Glm4MoeMTPModel', 'MedusaModel', 'OpenPanguMTPModel', 'Qwen3NextMTP', 'SmolLM3ForCausalLM', 'Emu3ForConditionalGeneration', 'TransformersForCausalLM', 'TransformersMoEForCausalLM', 'TransformersMultiModalForCausalLM', 'TransformersMultiModalMoEForCausalLM', 'TransformersEmbeddingModel', 'TransformersMoEEmbeddingModel', 'TransformersMultiModalEmbeddingModel', 'TransformersForSequenceClassification', 'TransformersMoEForSequenceClassification', 'TransformersMultiModalForSequenceClassification']) [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2000344)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.27
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "gustavecortal/Beck-8B": {
      "model_id": "gustavecortal/Beck-8B",
      "model_name": "gustavecortal/Beck-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:29:55.953969",
      "end_time": "2026-01-22T02:35:25.336908",
      "duration_seconds": 3929.38,
      "vllm_pid": 1172331,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_022221",
      "timing": {
        "server_start_seconds": 60.08,
        "test_execution_seconds": 3858.37,
        "total_seconds": 3929.38
      },
      "gpu_memory_gb": 1028.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "scb10x/typhoon2-qwen2vl-7b-vision-instruct": {
      "model_id": "scb10x/typhoon2-qwen2vl-7b-vision-instruct",
      "model_name": "scb10x/typhoon2-qwen2vl-7b-vision-instruct",
      "size_bytes": 8291375616,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T02:34:46.350764",
      "end_time": "2026-01-22T02:35:36.575377",
      "duration_seconds": 50.22,
      "vllm_pid": 2000374,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.19s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:05,  2.59s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:03,  3.12s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.38s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.07s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 240, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 126, in determine_available_memory\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self.collective_rpc(\"determine_available_memory\")\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 340, in determine_available_memory\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     self.model_runner.profile_run()\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4462, in profile_run\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     dummy_encoder_outputs = self.model.embed_multimodal(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1433, in embed_multimodal\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     video_embeddings = self._process_video_input(video_input)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1386, in _process_video_input\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     video_embeds = self.visual(pixel_values_videos, grid_thw=grid_thw)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 709, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     x = blk(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m         ^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 446, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     x = x + self.attn(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m             ^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 388, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     context_layer = self.attn(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                     ^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/custom_op.py\", line 47, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._forward_method(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layers/mm_encoder_attention.py\", line 229, in forward_cuda\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._forward_fa(query, key, value, cu_seqlens, max_seqlen)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layers/mm_encoder_attention.py\", line 199, in _forward_fa\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     output = vit_flash_attn_wrapper(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/ops/vit_attn_wrappers.py\", line 80, in vit_flash_attn_wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return torch.ops.vllm.flash_attn_maxseqlen_wrapper(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/ops/vit_attn_wrappers.py\", line 37, in flash_attn_maxseqlen_wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     output = flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 253, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     out, softmax_lse = torch.ops._vllm_fa2_C.varlen_fwd(\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=2000996)\u001b[0;0m \n[rank0]:[W122 02:35:24.913232806 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2000374)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 50.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Qwen/Qwen-Audio-Chat": {
      "model_id": "Qwen/Qwen-Audio-Chat",
      "model_name": "Qwen/Qwen-Audio-Chat",
      "size_bytes": 8396222464,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T02:35:36.579684",
      "end_time": "2026-01-22T02:35:36.742402",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gpt-oss-20b-BF16": {
      "model_id": "unsloth/gpt-oss-20b-BF16",
      "model_name": "unsloth/gpt-oss-20b-BF16",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T01:57:25.597749",
      "end_time": "2026-01-22T02:36:28.763220",
      "duration_seconds": 2343.17,
      "vllm_pid": 1226057,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_022221",
      "timing": {
        "server_start_seconds": 120.06,
        "test_execution_seconds": 2213.62,
        "total_seconds": 2343.17
      },
      "gpu_memory_gb": 1028.08,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "GetSoloTech/GPT-OSS-Code-Reasoning-20B": {
      "model_id": "GetSoloTech/GPT-OSS-Code-Reasoning-20B",
      "model_name": "GetSoloTech/GPT-OSS-Code-Reasoning-20B",
      "size_bytes": 21511953984,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-22T02:36:28.799402",
      "end_time": "2026-01-22T02:37:36.393875",
      "duration_seconds": 67.59,
      "vllm_pid": 1303235,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--GetSoloTech--GPT-OSS-Code-Reasoning-20B/snapshots/a371260211403325e37bab63b774a3d394420bdf` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 20.04,
        "test_execution_seconds": null,
        "total_seconds": 67.59
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/Seed-Coder-8B-Instruct": {
      "model_id": "ByteDance-Seed/Seed-Coder-8B-Instruct",
      "model_name": "ByteDance-Seed/Seed-Coder-8B-Instruct",
      "size_bytes": 8250462208,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:20:43.242925",
      "end_time": "2026-01-22T02:40:58.831140",
      "duration_seconds": 1215.59,
      "vllm_pid": 1971396,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_023645",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 1150.98,
        "total_seconds": 1215.59
      },
      "gpu_memory_gb": 1019.42,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "WisdomShell/RewardAnything-8B-v1": {
      "model_id": "WisdomShell/RewardAnything-8B-v1",
      "model_name": "WisdomShell/RewardAnything-8B-v1",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:22:20.468431",
      "end_time": "2026-01-22T02:50:59.296944",
      "duration_seconds": 5318.83,
      "vllm_pid": 1854663,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_024151",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 5247.09,
        "total_seconds": 5318.83
      },
      "gpu_memory_gb": 1019.46,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/gemma-7b-bnb-4bit": {
      "model_id": "unsloth/gemma-7b-bnb-4bit",
      "model_name": "unsloth/gemma-7b-bnb-4bit",
      "size_bytes": 8780496240,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T02:50:59.302816",
      "end_time": "2026-01-22T02:50:59.339935",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QCRI/Fanar-1-9B-Instruct": {
      "model_id": "QCRI/Fanar-1-9B-Instruct",
      "model_name": "QCRI/Fanar-1-9B-Instruct",
      "size_bytes": 8783871488,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T02:50:59.349003",
      "end_time": "2026-01-22T02:52:09.583523",
      "duration_seconds": 70.23,
      "vllm_pid": 2033381,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.87s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.86s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.22s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.38s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.17s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m 2026-01-22 02:51:54,234 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m 2026-01-22 02:51:54,246 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:45,  1.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.30it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.72it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  8.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.37it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.47it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.45it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.34it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.44it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.49it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.16it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.68it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.30it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.90it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:02<00:01, 17.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.94it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.91it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.43it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.34it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 21.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.58it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 13.09it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2033986)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 02:51:59.445430541 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2033381)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-Rosetta-20B": {
      "model_id": "yanolja/YanoljaNEXT-Rosetta-20B",
      "model_name": "yanolja/YanoljaNEXT-Rosetta-20B",
      "size_bytes": 20914757184,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T02:20:19.215990",
      "end_time": "2026-01-22T02:55:21.999735",
      "duration_seconds": 2102.78,
      "vllm_pid": 1271500,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_023928",
      "timing": {
        "server_start_seconds": 120.06,
        "test_execution_seconds": 1973.69,
        "total_seconds": 2102.78
      },
      "gpu_memory_gb": 1028.23,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "LiquidAI/LFM2-8B-A1B": {
      "model_id": "LiquidAI/LFM2-8B-A1B",
      "model_name": "LiquidAI/LFM2-8B-A1B",
      "size_bytes": 8339929856,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:35:25.390982",
      "end_time": "2026-01-22T02:59:09.561799",
      "duration_seconds": 1424.17,
      "vllm_pid": 1301138,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_025638",
      "timing": {
        "server_start_seconds": 80.08,
        "test_execution_seconds": 1335.06,
        "total_seconds": 1424.17
      },
      "gpu_memory_gb": 1028.01,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Huihui-Qwen3-8B-abliterated-v2": {
      "model_id": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
      "model_name": "huihui-ai/Huihui-Qwen3-8B-abliterated-v2",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:31:45.516812",
      "end_time": "2026-01-22T03:00:52.799238",
      "duration_seconds": 5347.28,
      "vllm_pid": 1873096,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_025313",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 5275.92,
        "total_seconds": 5347.28
      },
      "gpu_memory_gb": 1019.71,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "RUC-DataLab/DeepAnalyze-8B": {
      "model_id": "RUC-DataLab/DeepAnalyze-8B",
      "model_name": "RUC-DataLab/DeepAnalyze-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:16:10.654051",
      "end_time": "2026-01-22T03:05:01.042060",
      "duration_seconds": 6530.39,
      "vllm_pid": 1841224,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_030155",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 6463.68,
        "total_seconds": 6530.39
      },
      "gpu_memory_gb": 1019.93,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mlabonne/Qwen3-8B-abliterated": {
      "model_id": "mlabonne/Qwen3-8B-abliterated",
      "model_name": "mlabonne/Qwen3-8B-abliterated",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:52:43.375476",
      "end_time": "2026-01-22T03:10:52.801557",
      "duration_seconds": 4689.43,
      "vllm_pid": 1216290,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_030022",
      "timing": {
        "server_start_seconds": 80.05,
        "test_execution_seconds": 4597.2,
        "total_seconds": 4689.43
      },
      "gpu_memory_gb": 1028.55,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base": {
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
      "model_name": "nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base",
      "size_bytes": 8888227328,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:10:52.871570",
      "end_time": "2026-01-22T03:10:53.010368",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openai/gpt-oss-20b": {
      "model_id": "openai/gpt-oss-20b",
      "model_name": "openai/gpt-oss-20b",
      "size_bytes": 21511953984,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T02:37:36.401239",
      "end_time": "2026-01-22T03:11:14.520044",
      "duration_seconds": 2018.12,
      "vllm_pid": 1306021,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_030022",
      "timing": {
        "server_start_seconds": 110.06,
        "test_execution_seconds": 1898.95,
        "total_seconds": 2018.12
      },
      "gpu_memory_gb": 1027.96,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "INSAIT-Institute/MamayLM-Gemma-2-9B-IT-v0.1": {
      "model_id": "INSAIT-Institute/MamayLM-Gemma-2-9B-IT-v0.1",
      "model_name": "INSAIT-Institute/MamayLM-Gemma-2-9B-IT-v0.1",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:10:53.021326",
      "end_time": "2026-01-22T03:12:08.386845",
      "duration_seconds": 75.37,
      "vllm_pid": 1378652,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.66s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.18s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.39s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.49s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.37s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m 2026-01-22 03:11:52,588 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m 2026-01-22 03:11:52,600 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:05,  8.87it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:00<00:04, 10.46it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:00<00:04, 10.92it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:00<00:04, 10.88it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:00<00:03, 11.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:00<00:03, 11.93it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 12.27it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:01<00:02, 12.74it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:01<00:02, 13.35it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:01<00:02, 14.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:01<00:01, 15.06it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:01<00:01, 15.57it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:01<00:01, 15.97it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:01<00:01, 16.46it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 17.00it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:02<00:01, 17.46it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:02<00:01, 17.87it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:02<00:00, 18.81it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 38/51 [00:02<00:00, 18.68it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 41/51 [00:02<00:00, 19.52it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [00:02<00:00, 20.52it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 47/51 [00:02<00:00, 21.16it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 50/51 [00:03<00:00, 22.32it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 16.25it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1379260)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:11:58.000277936 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1378652)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 75.37
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Mantis2024/Dirty-Muse-Writer-v01-Uncensored-Erotica-NSFW": {
      "model_id": "Mantis2024/Dirty-Muse-Writer-v01-Uncensored-Erotica-NSFW",
      "model_name": "Mantis2024/Dirty-Muse-Writer-v01-Uncensored-Erotica-NSFW",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T03:12:08.484137",
      "end_time": "2026-01-22T03:13:16.152233",
      "duration_seconds": 67.67,
      "vllm_pid": 1381740,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Mantis2024--Dirty-Muse-Writer-v01-Uncensored-Erotica-NSFW/snapshots/9f4f5d00f495d211df85d72c11048857f0db2784` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 20.04,
        "test_execution_seconds": null,
        "total_seconds": 67.67
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ClosedCharacter/Peach-9B-8k-Roleplay": {
      "model_id": "ClosedCharacter/Peach-9B-8k-Roleplay",
      "model_name": "ClosedCharacter/Peach-9B-8k-Roleplay",
      "size_bytes": 8829407232,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:00:52.807504",
      "end_time": "2026-01-22T03:14:06.661365",
      "duration_seconds": 793.85,
      "vllm_pid": 2052834,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_030622",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 724.04,
        "total_seconds": 793.85
      },
      "gpu_memory_gb": 1019.95,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "SlerpE/WoonaV1.2-9b": {
      "model_id": "SlerpE/WoonaV1.2-9b",
      "model_name": "SlerpE/WoonaV1.2-9b",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:13:16.190355",
      "end_time": "2026-01-22T03:14:21.446650",
      "duration_seconds": 65.26,
      "vllm_pid": 1384670,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.64s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.41s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.47s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.37s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m 2026-01-22 03:14:07,613 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m 2026-01-22 03:14:07,624 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:46,  1.07it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.25it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.15it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.69it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  8.00it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.38it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.49it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.43it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.54it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.52it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.74it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.39it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 17.00it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:02<00:01, 17.53it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:00, 18.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 19.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.63it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.55it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 21.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.87it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.85it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 13.12it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1384712)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:14:12.747159972 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1384670)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.26
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": {
      "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "model_name": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:25:57.633925",
      "end_time": "2026-01-22T03:15:00.617610",
      "duration_seconds": 6542.98,
      "vllm_pid": 1164288,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_031231",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 6462.52,
        "total_seconds": 6542.98
      },
      "gpu_memory_gb": 1028.2,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Unbabel/Tower-Plus-9B": {
      "model_id": "Unbabel/Tower-Plus-9B",
      "model_name": "Unbabel/Tower-Plus-9B",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:14:06.665222",
      "end_time": "2026-01-22T03:15:11.864670",
      "duration_seconds": 65.2,
      "vllm_pid": 2081214,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.58s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.36s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.46s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.34s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m 2026-01-22 03:14:56,666 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m 2026-01-22 03:14:56,677 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:46,  1.07it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.24it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.13it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.65it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  7.94it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.30it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.37it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.35it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.37it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.03it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.57it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.17it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.77it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:03<00:01, 17.25it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.78it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.76it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.27it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.16it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 20.86it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.44it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.35it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 12.96it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2081257)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:15:01.807796922 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2081214)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "WiroAI/wiroai-turkish-llm-9b": {
      "model_id": "WiroAI/wiroai-turkish-llm-9b",
      "model_name": "WiroAI/wiroai-turkish-llm-9b",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:14:21.494303",
      "end_time": "2026-01-22T03:15:46.896484",
      "duration_seconds": 85.4,
      "vllm_pid": 1386740,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  12% Completed | 1/8 [00:04<00:30,  4.34s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 2/8 [00:08<00:26,  4.36s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  38% Completed | 3/8 [00:13<00:21,  4.34s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 4/8 [00:17<00:17,  4.34s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  62% Completed | 5/8 [00:19<00:10,  3.58s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 6/8 [00:23<00:07,  3.81s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards:  88% Completed | 7/8 [00:28<00:04,  4.01s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 8/8 [00:32<00:00,  4.17s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 8/8 [00:32<00:00,  4.10s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m 2026-01-22 03:15:33,795 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m 2026-01-22 03:15:33,807 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:46,  1.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.18it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.66it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  7.96it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.32it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.42it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.32it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.42it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.60it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.22it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.81it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:02<00:01, 17.30it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.81it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.79it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.29it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.18it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 20.83it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.41it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.35it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 13.02it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1386802)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:15:38.974221837 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1386740)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 85.4
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "silma-ai/SILMA-9B-Instruct-v1.0": {
      "model_id": "silma-ai/SILMA-9B-Instruct-v1.0",
      "model_name": "silma-ai/SILMA-9B-Instruct-v1.0",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:15:00.700422",
      "end_time": "2026-01-22T03:16:10.924176",
      "duration_seconds": 70.22,
      "vllm_pid": 1387652,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:07,  1.84s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:04<00:07,  2.40s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:07<00:05,  2.58s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:10<00:02,  2.69s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:13<00:00,  2.74s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:13<00:00,  2.62s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m 2026-01-22 03:15:54,075 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m 2026-01-22 03:15:54,086 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:46,  1.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.18it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.70it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  8.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.38it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.48it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.41it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.34it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.44it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.49it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.15it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.67it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.25it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.86it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:02<00:01, 17.33it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.86it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.76it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.05it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 20.72it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.25it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 13.03it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1388240)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:15:58.242161381 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1387652)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "QCRI/Fanar-1-9B": {
      "model_id": "QCRI/Fanar-1-9B",
      "model_name": "QCRI/Fanar-1-9B",
      "size_bytes": 9243540992,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:16:11.006758",
      "end_time": "2026-01-22T03:16:11.095460",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ytu-ce-cosmos/Turkish-Gemma-9b-T1": {
      "model_id": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
      "model_name": "ytu-ce-cosmos/Turkish-Gemma-9b-T1",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:15:11.869647",
      "end_time": "2026-01-22T03:16:17.097704",
      "duration_seconds": 65.23,
      "vllm_pid": 2083206,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.59s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.38s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.50s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.37s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m 2026-01-22 03:16:04,433 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m 2026-01-22 03:16:04,445 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:45,  1.10it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.29it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.39it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  7.71it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.21it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.21it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.15it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.21it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.26it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 14.94it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.48it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.71it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:03<00:01, 17.20it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.73it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.68it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.17it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.04it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 20.70it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.15it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 21.81it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 12.85it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2083268)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:16:09.706301609 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2083206)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/glm-4-9b": {
      "model_id": "zai-org/glm-4-9b",
      "model_name": "zai-org/glm-4-9b",
      "size_bytes": 9399951392,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:16:17.101349",
      "end_time": "2026-01-22T03:16:17.207188",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ytu-ce-cosmos/Turkish-Gemma-9b-v0.1": {
      "model_id": "ytu-ce-cosmos/Turkish-Gemma-9b-v0.1",
      "model_name": "ytu-ce-cosmos/Turkish-Gemma-9b-v0.1",
      "size_bytes": 9241705984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:15:46.904619",
      "end_time": "2026-01-22T03:16:52.131492",
      "duration_seconds": 65.23,
      "vllm_pid": 1389564,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.53s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.21s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.44s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.55s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.41s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m 2026-01-22 03:16:38,088 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m 2026-01-22 03:16:38,099 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:45,  1.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:14,  3.29it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:08,  5.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.71it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  8.00it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.35it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.44it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.32it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.39it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.41it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.06it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.58it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.15it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.73it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:02<00:01, 17.11it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.64it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.61it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.13it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.02it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 20.69it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.28it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.21it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 13.00it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1389643)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:16:42.290639759 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1389564)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "zai-org/GLM-Z1-9B-0414": {
      "model_id": "zai-org/GLM-Z1-9B-0414",
      "model_name": "zai-org/GLM-Z1-9B-0414",
      "size_bytes": 9400279040,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T03:16:52.217550",
      "end_time": "2026-01-22T03:18:09.940659",
      "duration_seconds": 77.72,
      "vllm_pid": 1391751,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--zai-org--GLM-Z1-9B-0414/snapshots/b221b06fefb23ca320922cf6e68ab5f2fb82de81` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 30.09,
        "test_execution_seconds": null,
        "total_seconds": 77.72
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gemma-2-9b-bnb-4bit": {
      "model_id": "unsloth/gemma-2-9b-bnb-4bit",
      "model_name": "unsloth/gemma-2-9b-bnb-4bit",
      "size_bytes": 9502508136,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:18:10.022233",
      "end_time": "2026-01-22T03:18:10.079803",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ClosedCharacter/Peach-2.0-9B-8k-Roleplay": {
      "model_id": "ClosedCharacter/Peach-2.0-9B-8k-Roleplay",
      "model_name": "ClosedCharacter/Peach-2.0-9B-8k-Roleplay",
      "size_bytes": 8829407232,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:59:09.613433",
      "end_time": "2026-01-22T03:22:35.817357",
      "duration_seconds": 1406.2,
      "vllm_pid": 1354011,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_032055",
      "timing": {
        "server_start_seconds": 70.08,
        "test_execution_seconds": 1326.35,
        "total_seconds": 1406.2
      },
      "gpu_memory_gb": 1028.79,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ibm-ai-platform/Bamba-9B-v2": {
      "model_id": "ibm-ai-platform/Bamba-9B-v2",
      "model_name": "ibm-ai-platform/Bamba-9B-v2",
      "size_bytes": 9780235392,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:22:35.987592",
      "end_time": "2026-01-22T03:22:36.034019",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ibm-ai-platform/Bamba-9B-v1": {
      "model_id": "ibm-ai-platform/Bamba-9B-v1",
      "model_name": "ibm-ai-platform/Bamba-9B-v1",
      "size_bytes": 9780235392,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:22:35.848980",
      "end_time": "2026-01-22T03:22:35.979130",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "pytorch/SmolLM3-3B-INT8-INT4": {
      "model_id": "pytorch/SmolLM3-3B-INT8-INT4",
      "model_name": "pytorch/SmolLM3-3B-INT8-INT4",
      "size_bytes": 10053484801,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:22:36.041749",
      "end_time": "2026-01-22T03:23:01.205530",
      "duration_seconds": 25.16,
      "vllm_pid": 1404520,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/torchao.py\", line 157, in from_config\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     from torchao.core.config import config_from_dict\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m ModuleNotFoundError: No module named 'torchao'\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m \n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m \n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1699, in create_engine_config\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     config = VllmConfig(\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m              ^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/vllm.py\", line 531, in __post_init__\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     self.quant_config = VllmConfig._get_quantization_config(\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/config/vllm.py\", line 378, in _get_quantization_config\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     quant_config = get_quant_config(model_config, load_config)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line 249, in get_quant_config\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     return quant_cls.from_config(hf_quant_config)\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/torchao.py\", line 159, in from_config\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(APIServer pid=1404520)\u001b[0;0m ImportError: Please install torchao>=0.10.0 via `pip install torchao>=0.10.0` to use torchao quantization.\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 25.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ModelSpace/GemmaX2-28-9B-v0.1": {
      "model_id": "ModelSpace/GemmaX2-28-9B-v0.1",
      "model_name": "ModelSpace/GemmaX2-28-9B-v0.1",
      "size_bytes": 10159209984,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:23:01.249322",
      "end_time": "2026-01-22T03:24:11.566384",
      "duration_seconds": 70.32,
      "vllm_pid": 1405293,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.31s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:06<00:04,  2.22s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:09<00:02,  2.73s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:13<00:00,  3.02s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:13<00:00,  2.65s/it]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m 2026-01-22 03:23:55,730 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m 2026-01-22 03:23:55,742 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:00<00:47,  1.05it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:15,  3.19it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:09,  5.08it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.57it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:01<00:05,  7.89it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:04,  9.25it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 10.37it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 11.36it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:02, 12.29it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 13.38it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:02<00:02, 14.42it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:02<00:01, 15.09it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.62it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.24it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 16.86it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:03<00:01, 17.36it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 17.89it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 36/51 [00:03<00:00, 18.90it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:03<00:00, 19.40it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/51 [00:03<00:00, 20.36it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:03<00:00, 21.06it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:03<00:00, 21.65it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 22.63it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 12.96it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 256, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 116, in initialize_from_config\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 75, in collective_rpc\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     result = run_method(self.driver_worker, method, args, kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/serial_utils.py\", line 461, in run_method\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 459, in compile_or_warm_up_model\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4563, in capture_model\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self._capture_cudagraphs(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4641, in _capture_cudagraphs\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self._dummy_run(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 4198, in _dummy_run\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     outputs = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m               ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/cuda_graph.py\", line 220, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self.runnable(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 418, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     hidden_states = self.model(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                     ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 439, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 223, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._call_with_optional_nvtx_range(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/wrapper.py\", line 109, in _call_with_optional_nvtx_range\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return callable_fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma2.py\", line 292, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     def forward(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 1044, in _fn\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return fn(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/caching.py\", line 54, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self.optimized_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"<eval_with_key>.86\", line 353, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return forward_call(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_3 = unified_attention_with_output = None\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/utils/kv_transfer_utils.py\", line 39, in wrapper\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return func(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 863, in unified_attention_with_output\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     self.impl.forward(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 687, in forward\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     flash_attn_varlen_func(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 278, in flash_attn_varlen_func\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_ops.py\", line 1255, in __call__\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m     return self._op(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1405899)\u001b[0;0m RuntimeError: This flash attention build does not support tanh softcapping.\n[rank0]:[W122 03:24:00.940571634 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1405293)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.32
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "openai/gpt-oss-safeguard-20b": {
      "model_id": "openai/gpt-oss-safeguard-20b",
      "model_name": "openai/gpt-oss-safeguard-20b",
      "size_bytes": 21511953984,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T02:55:22.094716",
      "end_time": "2026-01-22T03:30:44.134122",
      "duration_seconds": 2122.04,
      "vllm_pid": 1346174,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_032519",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 2037.92,
        "total_seconds": 2122.04
      },
      "gpu_memory_gb": 1028.6,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/internlm3-8b-instruct": {
      "model_id": "internlm/internlm3-8b-instruct",
      "model_name": "internlm/internlm3-8b-instruct",
      "size_bytes": 8804241408,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:52:09.593250",
      "end_time": "2026-01-22T03:31:47.485579",
      "duration_seconds": 2377.89,
      "vllm_pid": 2036247,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_031725",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 2306.89,
        "total_seconds": 2377.89
      },
      "gpu_memory_gb": 1019.79,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "zai-org/glm-4-9b-chat-hf": {
      "model_id": "zai-org/glm-4-9b-chat-hf",
      "model_name": "zai-org/glm-4-9b-chat-hf",
      "size_bytes": 9399951360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:16:11.104261",
      "end_time": "2026-01-22T03:40:54.353601",
      "duration_seconds": 1483.25,
      "vllm_pid": 1390313,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_033246",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 1403.18,
        "total_seconds": 1483.25
      },
      "gpu_memory_gb": 1028.16,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "omkarthawakar/LlamaV-o1": {
      "model_id": "omkarthawakar/LlamaV-o1",
      "model_name": "omkarthawakar/LlamaV-o1",
      "size_bytes": 10670220835,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:40:54.447967",
      "end_time": "2026-01-22T03:41:34.686892",
      "duration_seconds": 40.24,
      "vllm_pid": 1443731,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self.driver_worker.init_device()\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py\", line 326, in init_device\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self.worker.init_device()  # type: ignore\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 278, in init_device\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     self.model_runner = GPUModelRunnerV1(self.vllm_config, self.device)\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     MultiModalBudget(\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/utils.py\", line 46, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     max_tokens_by_modality = mm_registry.get_max_tokens_per_item_by_modality(\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/multimodal/registry.py\", line 167, in get_max_tokens_per_item_by_modality\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     return profiler.get_mm_max_tokens(\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/multimodal/profiling.py\", line 339, in get_mm_max_tokens\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     max_tokens_per_item = self.processing_info.get_mm_max_tokens_per_item(\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/transformers/multimodal.py\", line 61, in get_mm_max_tokens_per_item\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     return {\"image\": self.get_max_image_tokens()}\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/transformers/multimodal.py\", line 68, in get_max_image_tokens\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m     mm_tokens = processor._get_num_multimodal_tokens(\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1444353)\u001b[0;0m AttributeError: 'MllamaProcessor' object has no attribute '_get_num_multimodal_tokens'\n[rank0]:[W122 03:41:24.465404370 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1443731)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 40.24
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KORMo-Team/KORMo-10B-base": {
      "model_id": "KORMo-Team/KORMo-10B-base",
      "model_name": "KORMo-Team/KORMo-10B-base",
      "size_bytes": 10756624384,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:41:34.750083",
      "end_time": "2026-01-22T03:41:34.860390",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/gpt-oss-20b": {
      "model_id": "unsloth/gpt-oss-20b",
      "model_name": "unsloth/gpt-oss-20b",
      "size_bytes": 21511953984,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T03:11:14.573060",
      "end_time": "2026-01-22T03:43:20.107238",
      "duration_seconds": 1925.53,
      "vllm_pid": 1379915,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_034253",
      "timing": {
        "server_start_seconds": 75.06,
        "test_execution_seconds": 1841.32,
        "total_seconds": 1925.53
      },
      "gpu_memory_gb": 1028.43,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/DeepSeek-R1-0528-Qwen3-8B": {
      "model_id": "unsloth/DeepSeek-R1-0528-Qwen3-8B",
      "model_name": "unsloth/DeepSeek-R1-0528-Qwen3-8B",
      "size_bytes": 8190735360,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T01:54:13.168954",
      "end_time": "2026-01-22T03:43:26.189392",
      "duration_seconds": 6553.02,
      "vllm_pid": 1917371,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_033249",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 6486.56,
        "total_seconds": 6553.02
      },
      "gpu_memory_gb": 1019.58,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "zai-org/GLM-4-9B-0414": {
      "model_id": "zai-org/GLM-4-9B-0414",
      "model_name": "zai-org/GLM-4-9B-0414",
      "size_bytes": 9400279040,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:16:17.216857",
      "end_time": "2026-01-22T03:47:41.310097",
      "duration_seconds": 1884.09,
      "vllm_pid": 2085265,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_034436",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1808.88,
        "total_seconds": 1884.09
      },
      "gpu_memory_gb": 1019.87,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ByteDance-Seed/Seed-Coder-8B-Reasoning": {
      "model_id": "ByteDance-Seed/Seed-Coder-8B-Reasoning",
      "model_name": "ByteDance-Seed/Seed-Coder-8B-Reasoning",
      "size_bytes": 8250462208,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:26:40.465300",
      "end_time": "2026-01-22T03:48:45.237385",
      "duration_seconds": 4924.77,
      "vllm_pid": 1983952,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_034436",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 4843.28,
        "total_seconds": 4924.77
      },
      "gpu_memory_gb": 1019.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "speakleash/Bielik-11B-v2": {
      "model_id": "speakleash/Bielik-11B-v2",
      "model_name": "speakleash/Bielik-11B-v2",
      "size_bytes": 11168796672,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:48:45.242059",
      "end_time": "2026-01-22T03:48:45.335923",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "arcee-ai/Virtuoso-Lite": {
      "model_id": "arcee-ai/Virtuoso-Lite",
      "model_name": "arcee-ai/Virtuoso-Lite",
      "size_bytes": 10305653760,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:24:11.656106",
      "end_time": "2026-01-22T03:50:57.019487",
      "duration_seconds": 1605.36,
      "vllm_pid": 1408054,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_034449",
      "timing": {
        "server_start_seconds": 65.08,
        "test_execution_seconds": 1530.37,
        "total_seconds": 1605.36
      },
      "gpu_memory_gb": 1028.59,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "kyutai/hibiki-1b-mlx-bf16": {
      "model_id": "kyutai/hibiki-1b-mlx-bf16",
      "model_name": "kyutai/hibiki-1b-mlx-bf16",
      "size_bytes": 11185560830,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:50:57.105850",
      "end_time": "2026-01-22T03:50:57.169342",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "tiiuae/Falcon3-10B-Instruct": {
      "model_id": "tiiuae/Falcon3-10B-Instruct",
      "model_name": "tiiuae/Falcon3-10B-Instruct",
      "size_bytes": 10305653760,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:31:47.492614",
      "end_time": "2026-01-22T03:52:43.591055",
      "duration_seconds": 1256.1,
      "vllm_pid": 2115816,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_034953",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 1186.39,
        "total_seconds": 1256.1
      },
      "gpu_memory_gb": 1020.36,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "kyutai/hibiki-2b-pytorch-bf16": {
      "model_id": "kyutai/hibiki-2b-pytorch-bf16",
      "model_name": "kyutai/hibiki-2b-pytorch-bf16",
      "size_bytes": 11535027654,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:52:43.599461",
      "end_time": "2026-01-22T03:52:43.626195",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16": {
      "model_id": "ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
      "model_name": "ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
      "size_bytes": 8250462208,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:30:53.669977",
      "end_time": "2026-01-22T03:52:52.848566",
      "duration_seconds": 4919.18,
      "vllm_pid": 1992250,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_034953",
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": 4852.47,
        "total_seconds": 4919.18
      },
      "gpu_memory_gb": 1019.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "scb10x/typhoon2.1-gemma3-12b": {
      "model_id": "scb10x/typhoon2.1-gemma3-12b",
      "model_name": "scb10x/typhoon2.1-gemma3-12b",
      "size_bytes": 11765792256,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:52:52.853772",
      "end_time": "2026-01-22T03:52:52.887829",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--scb10x--typhoon2.1-gemma3-12b/snapshots/c7bf9b738f7181484dcaa02df8c73666ea2f636d/model-00001-of-00005.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-Rosetta-12B": {
      "model_id": "yanolja/YanoljaNEXT-Rosetta-12B",
      "model_name": "yanolja/YanoljaNEXT-Rosetta-12B",
      "size_bytes": 11766034176,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T03:52:52.898570",
      "end_time": "2026-01-22T03:52:52.926370",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--yanolja--YanoljaNEXT-Rosetta-12B/snapshots/f403156a909d9aa73918da663065fc5ed1a47920/model-00001-of-00006.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-Rosetta-12B-2510": {
      "model_id": "yanolja/YanoljaNEXT-Rosetta-12B-2510",
      "model_name": "yanolja/YanoljaNEXT-Rosetta-12B-2510",
      "size_bytes": 11766034176,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T03:52:53.020142",
      "end_time": "2026-01-22T03:54:37.287035",
      "duration_seconds": 104.27,
      "vllm_pid": 2158675,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/qbw/models--yanolja--YanoljaNEXT-Rosetta-12B-2510/snapshots/7f3f516b1658728651630ad6da046e54e680920e` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 55.05,
        "test_execution_seconds": null,
        "total_seconds": 104.27
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mlx-community/gpt-oss-20b-MXFP4-Q8": {
      "model_id": "mlx-community/gpt-oss-20b-MXFP4-Q8",
      "model_name": "mlx-community/gpt-oss-20b-MXFP4-Q8",
      "size_bytes": 12104075742,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T03:54:37.293078",
      "end_time": "2026-01-22T03:55:12.448746",
      "duration_seconds": 35.16,
      "vllm_pid": 2163160,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gpt_oss.py\", line 744, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 288, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     yield from self._load_module(\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m ValueError: There is no module or parameter named 'lm_head.biases' in GptOssForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2163222)\u001b[0;0m \n[rank0]:[W122 03:55:00.120183123 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2163160)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 35.16
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-21B-A3B-Base-PT": {
      "model_id": "baidu/ERNIE-4.5-21B-A3B-Base-PT",
      "model_name": "baidu/ERNIE-4.5-21B-A3B-Base-PT",
      "size_bytes": 21825437888,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T03:30:44.190586",
      "end_time": "2026-01-22T04:07:01.909959",
      "duration_seconds": 2177.72,
      "vllm_pid": 1421966,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_035204",
      "timing": {
        "server_start_seconds": 120.06,
        "test_execution_seconds": 2047.92,
        "total_seconds": 2177.72
      },
      "gpu_memory_gb": 1028.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Darkknight535/OpenCrystal-12B-L3": {
      "model_id": "Darkknight535/OpenCrystal-12B-L3",
      "model_name": "Darkknight535/OpenCrystal-12B-L3",
      "size_bytes": 11520053248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:50:57.178315",
      "end_time": "2026-01-22T04:11:41.505760",
      "duration_seconds": 1244.33,
      "vllm_pid": 1464095,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_040830",
      "timing": {
        "server_start_seconds": 65.08,
        "test_execution_seconds": 1169.46,
        "total_seconds": 1244.33
      },
      "gpu_memory_gb": 1028.24,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "soob3123/Veritas-12B": {
      "model_id": "soob3123/Veritas-12B",
      "model_name": "soob3123/Veritas-12B",
      "size_bytes": 12187325040,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:11:41.603976",
      "end_time": "2026-01-22T04:12:11.889414",
      "duration_seconds": 30.29,
      "vllm_pid": 1505447,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py\", line 507, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1506064)\u001b[0;0m \n[rank0]:[W122 04:12:02.062147894 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1505447)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.29
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "soob3123/amoral-gemma3-12B-v1": {
      "model_id": "soob3123/amoral-gemma3-12B-v1",
      "model_name": "soob3123/amoral-gemma3-12B-v1",
      "size_bytes": 12187325040,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:12:11.903360",
      "end_time": "2026-01-22T04:12:42.209309",
      "duration_seconds": 30.31,
      "vllm_pid": 1506799,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 55, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     self.load_weights(model, model_config)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 305, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py\", line 507, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     return loader.load_weights(weights)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/online_quantization.py\", line 173, in patched_model_load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 335, in load_weights\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 319, in _load_module\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m     raise ValueError(msg)\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=1507388)\u001b[0;0m \n[rank0]:[W122 04:12:32.923253348 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1506799)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.31
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-EEVE-Instruct-10.8B": {
      "model_id": "yanolja/YanoljaNEXT-EEVE-Instruct-10.8B",
      "model_name": "yanolja/YanoljaNEXT-EEVE-Instruct-10.8B",
      "size_bytes": 10804924416,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:47:41.314096",
      "end_time": "2026-01-22T04:14:52.460868",
      "duration_seconds": 1631.15,
      "vllm_pid": 2148206,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_035712",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1556.38,
        "total_seconds": 1631.15
      },
      "gpu_memory_gb": 1020.71,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "soob3123/amoral-gemma3-12B-v2": {
      "model_id": "soob3123/amoral-gemma3-12B-v2",
      "model_name": "soob3123/amoral-gemma3-12B-v2",
      "size_bytes": 12187325040,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T04:12:42.308154",
      "end_time": "2026-01-22T04:15:30.038201",
      "duration_seconds": 167.73,
      "vllm_pid": 1507513,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"unexpected char '\\\\\\\\' at 418 None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 120.09,
        "test_execution_seconds": null,
        "total_seconds": 167.73
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "speakleash/Bielik-11B-v2.3-Instruct": {
      "model_id": "speakleash/Bielik-11B-v2.3-Instruct",
      "model_name": "speakleash/Bielik-11B-v2.3-Instruct",
      "size_bytes": 11168796672,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:48:45.344359",
      "end_time": "2026-01-22T04:15:47.470408",
      "duration_seconds": 1622.13,
      "vllm_pid": 2150196,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_035712",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1547.08,
        "total_seconds": 1622.13
      },
      "gpu_memory_gb": 1020.36,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Delta-Vector/Francois-PE-V2-Huali-12B": {
      "model_id": "Delta-Vector/Francois-PE-V2-Huali-12B",
      "model_name": "Delta-Vector/Francois-PE-V2-Huali-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:15:47.475073",
      "end_time": "2026-01-22T04:16:52.705239",
      "duration_seconds": 65.23,
      "vllm_pid": 2206599,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Delta-Vector--Francois-PE-V2-Huali-12B/snapshots/2ef152b08a76a74cf850ccfb9358822d22a45166' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:13,  3.38s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:06<00:10,  3.48s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:10<00:07,  3.50s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:14<00:03,  3.53s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:17<00:00,  3.52s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:17<00:00,  3.50s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2207229)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:16:41.749152990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2206599)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "soob3123/amoral-gemma3-12B-v2-qat": {
      "model_id": "soob3123/amoral-gemma3-12B-v2-qat",
      "model_name": "soob3123/amoral-gemma3-12B-v2-qat",
      "size_bytes": 12187325040,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T04:14:52.466018",
      "end_time": "2026-01-22T04:17:10.661795",
      "duration_seconds": 138.2,
      "vllm_pid": 2204433,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"unexpected char '\\\\\\\\' at 418 None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 90.06,
        "test_execution_seconds": null,
        "total_seconds": 138.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Delta-Vector/Rei-12B": {
      "model_id": "Delta-Vector/Rei-12B",
      "model_name": "Delta-Vector/Rei-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:16:52.709058",
      "end_time": "2026-01-22T04:17:12.911453",
      "duration_seconds": 20.2,
      "vllm_pid": 2208648,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 114, in __init__\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     tokenizer = cached_tokenizer_from_config(self.model_config)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/registry.py\", line 219, in cached_tokenizer_from_config\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return cached_get_tokenizer(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/registry.py\", line 202, in get_tokenizer\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     tokenizer = tokenizer_cls_.from_pretrained(tokenizer_name, *args, **kwargs)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/tokenizers/hf.py\", line 79, in from_pretrained\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 1156, in from_pretrained\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2113, in from_pretrained\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     return cls._from_pretrained(\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2359, in _from_pretrained\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     tokenizer = cls(*init_inputs, **init_kwargs)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 139, in __init__\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py\", line 1857, in convert_slow_tokenizer\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m     elif transformer_tokenizer.vocab_file.endswith(\"tekken.json\"):\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2208648)\u001b[0;0m AttributeError: 'NoneType' object has no attribute 'endswith'\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 20.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "HumanLLMs/Human-Like-Mistral-Nemo-Instruct-2407": {
      "model_id": "HumanLLMs/Human-Like-Mistral-Nemo-Instruct-2407",
      "model_name": "HumanLLMs/Human-Like-Mistral-Nemo-Instruct-2407",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:17:12.914912",
      "end_time": "2026-01-22T04:18:23.105983",
      "duration_seconds": 70.19,
      "vllm_pid": 2209798,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m [2026-01-22 04:17:22] INFO tekken.py:184: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m [2026-01-22 04:17:22] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m [2026-01-22 04:17:22] INFO tekken.py:567: Cutting non special vocabulary to first 130072 tokens.\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:06<00:24,  6.06s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:12<00:18,  6.10s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:18<00:12,  6.10s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:23<00:05,  5.90s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:28<00:00,  5.35s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:28<00:00,  5.65s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2209869)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:18:15.918201135 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2209798)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.19
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "IIEleven11/Kalypso": {
      "model_id": "IIEleven11/Kalypso",
      "model_name": "IIEleven11/Kalypso",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:18:23.109881",
      "end_time": "2026-01-22T04:18:23.207545",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "DreadPoor/Irix-12B-Model_Stock": {
      "model_id": "DreadPoor/Irix-12B-Model_Stock",
      "model_name": "DreadPoor/Irix-12B-Model_Stock",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:17:10.665188",
      "end_time": "2026-01-22T04:18:25.935817",
      "duration_seconds": 75.27,
      "vllm_pid": 2209794,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/qbw/models--DreadPoor--Irix-12B-Model_Stock/snapshots/2e61a24ee8296b195a8d9b640ecbc0cb7ea88132' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:05<00:20,  5.05s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:11<00:17,  5.68s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:17<00:11,  5.85s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:23<00:05,  5.94s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:28<00:00,  5.54s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:28<00:00,  5.62s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m Exception ignored in: <function CompiledFxGraph.__del__ at 0x7fdd38286b60>\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_inductor/output_code.py\", line 582, in __del__\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     if self.compiled_fn_runner is not None:\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m        ^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m AttributeError: 'CompiledFxGraph' object has no attribute 'compiled_fn_runner'\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m [rank0]:W0122 04:18:13.418000 2209863 site-packages/torch/_inductor/codecache.py:1021] fx graph cache unable to load compiled graph\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m [rank0]:W0122 04:18:13.418000 2209863 site-packages/torch/_inductor/codecache.py:1021] Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m [rank0]:W0122 04:18:13.418000 2209863 site-packages/torch/_inductor/codecache.py:1021]   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 1019, in iterate_over_candidates\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m [rank0]:W0122 04:18:13.418000 2209863 site-packages/torch/_inductor/codecache.py:1021]     yield pickle.loads(content), content\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m [rank0]:W0122 04:18:13.418000 2209863 site-packages/torch/_inductor/codecache.py:1021]           ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m [rank0]:W0122 04:18:13.418000 2209863 site-packages/torch/_inductor/codecache.py:1021] _pickle.UnpicklingError: pickle data was truncated\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2209863)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:18:16.648294242 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2209794)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 75.27
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "MarinaraSpaghetti/NemoMix-Unleashed-12B": {
      "model_id": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
      "model_name": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:18:25.939537",
      "end_time": "2026-01-22T04:18:26.056923",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/Captain-Eris_Violet-GRPO-v0.420": {
      "model_id": "Nitral-AI/Captain-Eris_Violet-GRPO-v0.420",
      "model_name": "Nitral-AI/Captain-Eris_Violet-GRPO-v0.420",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:18:26.085548",
      "end_time": "2026-01-22T04:19:36.447468",
      "duration_seconds": 70.36,
      "vllm_pid": 2212456,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Nitral-AI--Captain-Eris_Violet-GRPO-v0.420/snapshots/64ea423e62b491eb1419839c7079fa5323e30834' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:18,  4.72s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:09<00:14,  4.79s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:14<00:09,  4.78s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:19<00:04,  4.79s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:23<00:00,  4.80s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:23<00:00,  4.79s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2212619)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:19:26.807843832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2212456)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.36
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Luni/StarDust-12b-v2": {
      "model_id": "Luni/StarDust-12b-v2",
      "model_name": "Luni/StarDust-12b-v2",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:18:23.217342",
      "end_time": "2026-01-22T04:19:38.428305",
      "duration_seconds": 75.21,
      "vllm_pid": 2211967,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Luni--StarDust-12b-v2/snapshots/9ea011e45783bb1dabbbe4bd173f1f600f7a8edc' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:19,  4.79s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:09<00:14,  4.81s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:14<00:09,  4.83s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:19<00:04,  4.83s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:24<00:00,  4.84s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:24<00:00,  4.83s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2212614)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:19:26.931976337 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2211967)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 75.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/Captain-Eris_Violet-V0.420-12B": {
      "model_id": "Nitral-AI/Captain-Eris_Violet-V0.420-12B",
      "model_name": "Nitral-AI/Captain-Eris_Violet-V0.420-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:19:36.455187",
      "end_time": "2026-01-22T04:20:46.734354",
      "duration_seconds": 70.28,
      "vllm_pid": 2214714,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Nitral-AI--Captain-Eris_Violet-V0.420-12B/snapshots/9da28dddf4530bb0a195c73a9fe8f844245d68f5' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:19,  4.79s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:09<00:14,  4.84s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:14<00:09,  4.89s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:19<00:04,  4.88s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:24<00:00,  4.90s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:24<00:00,  4.89s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2214782)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:20:39.345865268 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2214714)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.28
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/Captain_Eris_Noctis-12B-v0.420": {
      "model_id": "Nitral-AI/Captain_Eris_Noctis-12B-v0.420",
      "model_name": "Nitral-AI/Captain_Eris_Noctis-12B-v0.420",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:19:38.432257",
      "end_time": "2026-01-22T04:20:48.661588",
      "duration_seconds": 70.23,
      "vllm_pid": 2214718,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Nitral-AI--Captain_Eris_Noctis-12B-v0.420/snapshots/7fdb4f7b18f70179c205d48cd94b89691c79de36' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:19,  4.77s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:09<00:14,  4.87s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:14<00:09,  4.89s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:19<00:04,  4.90s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:24<00:00,  4.89s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:24<00:00,  4.88s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2214788)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:20:38.259721936 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2214718)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/Wayfarer_Eris_Noctis-12B": {
      "model_id": "Nitral-AI/Wayfarer_Eris_Noctis-12B",
      "model_name": "Nitral-AI/Wayfarer_Eris_Noctis-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:20:48.665636",
      "end_time": "2026-01-22T04:22:03.877075",
      "duration_seconds": 75.21,
      "vllm_pid": 2216862,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Nitral-AI--Wayfarer_Eris_Noctis-12B/snapshots/ca47e6b0bbe1401fe9078f783011b918c23f3484' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:19,  4.77s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:11<00:17,  5.69s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:17<00:12,  6.03s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:23<00:06,  6.19s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:30<00:00,  6.19s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:30<00:00,  6.03s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2217442)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:21:54.819322220 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2216862)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 75.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Nitral-AI/Irixxed-Magcap-12B-Slerp": {
      "model_id": "Nitral-AI/Irixxed-Magcap-12B-Slerp",
      "model_name": "Nitral-AI/Irixxed-Magcap-12B-Slerp",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:20:46.741134",
      "end_time": "2026-01-22T04:22:07.021959",
      "duration_seconds": 80.28,
      "vllm_pid": 2216858,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Nitral-AI--Irixxed-Magcap-12B-Slerp/snapshots/aa2bef5837357b929804d7f221764dccdedaac5b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:18,  4.69s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:10<00:16,  5.63s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:17<00:11,  5.90s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:23<00:06,  6.10s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:29<00:00,  6.14s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:29<00:00,  5.97s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2217441)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:21:54.733614322 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2216858)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 80.28
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PocketDoc/Dans-PersonalityEngine-V1.1.0-12b": {
      "model_id": "PocketDoc/Dans-PersonalityEngine-V1.1.0-12b",
      "model_name": "PocketDoc/Dans-PersonalityEngine-V1.1.0-12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:22:03.882118",
      "end_time": "2026-01-22T04:23:14.114079",
      "duration_seconds": 70.23,
      "vllm_pid": 2220169,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/qbw/models--PocketDoc--Dans-PersonalityEngine-V1.1.0-12b/snapshots/d72c6019a5a562b307ff99a9ab4fae447ab77405' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:05<00:23,  6.00s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:10<00:16,  5.36s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:15<00:10,  5.14s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:20<00:05,  5.05s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:25<00:00,  4.99s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:25<00:00,  5.12s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2220219)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:23:05.755166402 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2220169)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "PygmalionAI/Eleusis-12B": {
      "model_id": "PygmalionAI/Eleusis-12B",
      "model_name": "PygmalionAI/Eleusis-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T04:23:14.118308",
      "end_time": "2026-01-22T04:24:07.019557",
      "duration_seconds": 52.9,
      "vllm_pid": 2222437,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--PygmalionAI--Eleusis-12B/snapshots/5f4c5e7cfb8f8e574c5c8fcfb84faab8dce4e72c` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 5.04,
        "test_execution_seconds": null,
        "total_seconds": 52.9
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "voidful/Llama-3.1-TAIDE-R1-8B-Chat": {
      "model_id": "voidful/Llama-3.1-TAIDE-R1-8B-Chat",
      "model_name": "voidful/Llama-3.1-TAIDE-R1-8B-Chat",
      "size_bytes": 8521781248,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:35:36.790284",
      "end_time": "2026-01-22T04:24:12.489324",
      "duration_seconds": 6515.7,
      "vllm_pid": 2002560,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_042319",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 6442.47,
        "total_seconds": 6515.7
      },
      "gpu_memory_gb": 1019.42,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PygmalionAI/Pygmalion-3-12B": {
      "model_id": "PygmalionAI/Pygmalion-3-12B",
      "model_name": "PygmalionAI/Pygmalion-3-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:24:07.027470",
      "end_time": "2026-01-22T04:25:17.322394",
      "duration_seconds": 70.29,
      "vllm_pid": 2224712,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/qbw/models--PygmalionAI--Pygmalion-3-12B/snapshots/5f1eaa1de605656caa5cece8f640d14d4b058ad5' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:14,  3.56s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:08<00:12,  4.29s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:13<00:09,  4.54s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:18<00:04,  4.66s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:22<00:00,  4.72s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:22<00:00,  4.57s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2224758)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:25:07.292017846 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2224712)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.29
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Retreatcost/KansenSakura-Eclipse-RP-12b": {
      "model_id": "Retreatcost/KansenSakura-Eclipse-RP-12b",
      "model_name": "Retreatcost/KansenSakura-Eclipse-RP-12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T04:25:17.335182",
      "end_time": "2026-01-22T04:26:10.053643",
      "duration_seconds": 52.72,
      "vllm_pid": 2226944,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--Retreatcost--KansenSakura-Eclipse-RP-12b/snapshots/03210ad6957cff2ef735851bfda9a0ed71c5defc` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 5.03,
        "test_execution_seconds": null,
        "total_seconds": 52.72
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Retreatcost/KansenSakura-Erosion-RP-12b": {
      "model_id": "Retreatcost/KansenSakura-Erosion-RP-12b",
      "model_name": "Retreatcost/KansenSakura-Erosion-RP-12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:26:10.057145",
      "end_time": "2026-01-22T04:27:15.268352",
      "duration_seconds": 65.21,
      "vllm_pid": 2229248,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Retreatcost--KansenSakura-Erosion-RP-12b/snapshots/f166c3e125f83e07be483b58178a5bdffeda1f50' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:13,  3.33s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:06<00:10,  3.43s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:10<00:06,  3.46s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:13<00:03,  3.52s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:17<00:00,  3.52s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:17<00:00,  3.49s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2229310)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:27:04.174920580 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2229248)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "K-intelligence/Midm-2.0-Base-Instruct": {
      "model_id": "K-intelligence/Midm-2.0-Base-Instruct",
      "model_name": "K-intelligence/Midm-2.0-Base-Instruct",
      "size_bytes": 11545677824,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:52:43.637671",
      "end_time": "2026-01-22T04:28:06.039220",
      "duration_seconds": 2122.4,
      "vllm_pid": 2158635,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_042524",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 2047.38,
        "total_seconds": 2122.4
      },
      "gpu_memory_gb": 916.78,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Retreatcost/KansenSakura-Radiance-RP-12b": {
      "model_id": "Retreatcost/KansenSakura-Radiance-RP-12b",
      "model_name": "Retreatcost/KansenSakura-Radiance-RP-12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:27:15.276771",
      "end_time": "2026-01-22T04:28:15.497479",
      "duration_seconds": 60.22,
      "vllm_pid": 2231391,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--Retreatcost--KansenSakura-Radiance-RP-12b/snapshots/ad45ce0ec6610eb5a44e55ff7b8df5de6875a65e' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:13,  3.36s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:06<00:10,  3.51s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:10<00:07,  3.61s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:14<00:03,  3.57s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:18<00:00,  3.66s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:18<00:00,  3.61s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2231484)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:28:08.553180672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2231391)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 60.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "inflatebot/MN-12B-Mag-Mell-R1": {
      "model_id": "inflatebot/MN-12B-Mag-Mell-R1",
      "model_name": "inflatebot/MN-12B-Mag-Mell-R1",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:28:06.047051",
      "end_time": "2026-01-22T04:29:11.280037",
      "duration_seconds": 65.23,
      "vllm_pid": 2233428,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/qbw/models--inflatebot--MN-12B-Mag-Mell-R1/snapshots/b5f9f348c2e43dc8c862b2d4ab1521e3256696fc' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:13,  3.39s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:06<00:10,  3.45s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:10<00:07,  3.67s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:14<00:03,  3.81s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:18<00:00,  3.87s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:18<00:00,  3.77s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2233497)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:29:02.824675288 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2233428)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "sleepdeprived3/Christian-Bible-Expert-v2.0-12B": {
      "model_id": "sleepdeprived3/Christian-Bible-Expert-v2.0-12B",
      "model_name": "sleepdeprived3/Christian-Bible-Expert-v2.0-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T04:29:11.287734",
      "end_time": "2026-01-22T04:30:08.899878",
      "duration_seconds": 57.61,
      "vllm_pid": 2235630,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--sleepdeprived3--Christian-Bible-Expert-v2.0-12B/snapshots/c5c2b6aba3f8c7096b0828b0996a4b210f3e3884` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 10.04,
        "test_execution_seconds": null,
        "total_seconds": 57.61
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yamatazen/EtherealAurora-12B-v2": {
      "model_id": "yamatazen/EtherealAurora-12B-v2",
      "model_name": "yamatazen/EtherealAurora-12B-v2",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:30:08.904062",
      "end_time": "2026-01-22T04:31:14.116636",
      "duration_seconds": 65.21,
      "vllm_pid": 2237789,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--yamatazen--EtherealAurora-12B-v2/snapshots/f218cef001312aa9b547b992e12dc9e330c3fff8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:03<00:14,  3.52s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:07<00:10,  3.58s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:10<00:07,  3.59s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:14<00:03,  3.60s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:18<00:00,  3.61s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:18<00:00,  3.60s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2237849)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640672. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:31:02.634706525 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2237789)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 65.21
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yamatazen/LorablatedStock-12B": {
      "model_id": "yamatazen/LorablatedStock-12B",
      "model_name": "yamatazen/LorablatedStock-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:31:14.120329",
      "end_time": "2026-01-22T04:31:14.216854",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "yanolja/YanoljaNEXT-EEVE-10.8B": {
      "model_id": "yanolja/YanoljaNEXT-EEVE-10.8B",
      "model_name": "yanolja/YanoljaNEXT-EEVE-10.8B",
      "size_bytes": 10804924416,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:43:26.193971",
      "end_time": "2026-01-22T04:36:30.564740",
      "duration_seconds": 3184.37,
      "vllm_pid": 2140052,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_043222",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 3106.98,
        "total_seconds": 3184.37
      },
      "gpu_memory_gb": 1020.28,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Delta-Vector/Archaeo-12B": {
      "model_id": "Delta-Vector/Archaeo-12B",
      "model_name": "Delta-Vector/Archaeo-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:15:30.119854",
      "end_time": "2026-01-22T04:44:24.601850",
      "duration_seconds": 1734.48,
      "vllm_pid": 1514227,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "score": 0.6716
        },
        "mmlu_pro": {
          "score": 0.4279
        },
        "math_500": {
          "score": 0.4111
        },
        "gsm8k": {
          "score": 0.85
        },
        "gpqa_diamond": {
          "score": 0.35
        }
      },
      "eval_output_dir": "outputs/20260122_041637",
      "timing": {
        "server_start_seconds": 65.08,
        "test_execution_seconds": 1659.62,
        "total_seconds": 1734.48
      },
      "gpu_memory_gb": 1028.41,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PocketDoc/Dans-PersonalityEngine-V1.3.0-12b": {
      "model_id": "PocketDoc/Dans-PersonalityEngine-V1.3.0-12b",
      "model_name": "PocketDoc/Dans-PersonalityEngine-V1.3.0-12b",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:22:07.025598",
      "end_time": "2026-01-22T04:46:38.129816",
      "duration_seconds": 1471.1,
      "vllm_pid": 2220191,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_043743",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 1391.27,
        "total_seconds": 1471.1
      },
      "gpu_memory_gb": 1020.03,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "soob3123/Veiled-Calla-12B": {
      "model_id": "soob3123/Veiled-Calla-12B",
      "model_name": "soob3123/Veiled-Calla-12B",
      "size_bytes": 12187325040,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:55:12.451956",
      "end_time": "2026-01-22T04:46:38.357512",
      "duration_seconds": 3085.91,
      "vllm_pid": 2164537,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_043743",
      "timing": {
        "server_start_seconds": 115.06,
        "test_execution_seconds": 2958.45,
        "total_seconds": 3085.91
      },
      "gpu_memory_gb": 1020.71,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3": {
      "model_id": "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3",
      "model_name": "redrix/AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3",
      "size_bytes": 12247802880,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:46:38.138218",
      "end_time": "2026-01-22T04:47:48.385373",
      "duration_seconds": 70.25,
      "vllm_pid": 2272510,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--redrix--AngelSlayer-12B-Unslop-Mell-RPMax-DARKNESS-v3/snapshots/0203bb0e78258307a1d69ebd652ef61621baa54b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:04<00:18,  4.75s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:09<00:14,  4.85s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:14<00:09,  4.88s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:19<00:04,  4.87s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:23<00:00,  4.72s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:23<00:00,  4.78s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 109, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 248, in _initialize_kv_caches\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1340, in get_kv_cache_configs\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     check_enough_kv_cache_memory(\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 710, in check_enough_kv_cache_memory\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(EngineCore_DP0 pid=2272601)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (1024000), (156.25 GiB KV cache is needed, which is larger than the available KV cache memory (97.76 GiB). Based on the available memory, the estimated maximum model length is 640656. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n[rank0]:[W122 04:47:38.986083659 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2272510)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 70.25
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-21B-A3B-PT": {
      "model_id": "baidu/ERNIE-4.5-21B-A3B-PT",
      "model_name": "baidu/ERNIE-4.5-21B-A3B-PT",
      "size_bytes": 21948655808,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T04:07:01.980687",
      "end_time": "2026-01-22T04:48:25.518990",
      "duration_seconds": 2483.54,
      "vllm_pid": 1496212,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_044531",
      "timing": {
        "server_start_seconds": 85.05,
        "test_execution_seconds": 2387.5,
        "total_seconds": 2483.54
      },
      "gpu_memory_gb": 1028.35,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "redrix/GodSlayer-12B-ABYSS": {
      "model_id": "redrix/GodSlayer-12B-ABYSS",
      "model_name": "redrix/GodSlayer-12B-ABYSS",
      "size_bytes": 12247813120,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T04:47:48.391400",
      "end_time": "2026-01-22T04:48:41.151078",
      "duration_seconds": 52.76,
      "vllm_pid": 2274768,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--redrix--GodSlayer-12B-ABYSS/snapshots/b602ebc4db04d4f785be63f64f9c554bf21b9469` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 5.03,
        "test_execution_seconds": null,
        "total_seconds": 52.76
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "dreamgen/lucid-v1-nemo": {
      "model_id": "dreamgen/lucid-v1-nemo",
      "model_name": "dreamgen/lucid-v1-nemo",
      "size_bytes": 12247833600,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:48:41.155411",
      "end_time": "2026-01-22T04:48:41.341434",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ReadyArt/Omega-Darker_The-Final-Directive-12B": {
      "model_id": "ReadyArt/Omega-Darker_The-Final-Directive-12B",
      "model_name": "ReadyArt/Omega-Darker_The-Final-Directive-12B",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:24:12.494378",
      "end_time": "2026-01-22T04:50:52.357550",
      "duration_seconds": 1599.86,
      "vllm_pid": 2224741,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_045003",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 1520.2,
        "total_seconds": 1599.86
      },
      "gpu_memory_gb": 1019.97,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base": {
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
      "model_name": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
      "size_bytes": 12310001152,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:50:52.362308",
      "end_time": "2026-01-22T04:50:52.453927",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "mistralai/Mistral-Nemo-Instruct-2407": {
      "model_id": "mistralai/Mistral-Nemo-Instruct-2407",
      "model_name": "mistralai/Mistral-Nemo-Instruct-2407",
      "size_bytes": 12247782400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:28:15.501098",
      "end_time": "2026-01-22T04:53:00.803911",
      "duration_seconds": 1485.3,
      "vllm_pid": 2233498,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_045158",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1410.85,
        "total_seconds": 1485.3
      },
      "gpu_memory_gb": 1019.95,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit": {
      "model_id": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
      "model_name": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
      "size_bytes": 12589408254,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:53:00.811240",
      "end_time": "2026-01-22T04:53:31.007697",
      "duration_seconds": 30.2,
      "vllm_pid": 2287093,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m [2026-01-22 04:53:10] INFO tekken.py:184: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m [2026-01-22 04:53:10] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m [2026-01-22 04:53:10] INFO tekken.py:567: Cutting non special vocabulary to first 130072 tokens.\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     import bitsandbytes\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m ModuleNotFoundError: No module named 'bitsandbytes'\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m The above exception was the direct cause of the following exception:\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 566, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.model = self._init_model(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 611, in _init_model\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     return LlamaModel(vllm_config=vllm_config, prefix=prefix, layer_type=layer_type)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 393, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 395, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     lambda prefix: layer_type(vllm_config=vllm_config, prefix=prefix),\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 302, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.self_attn = LlamaAttention(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 935, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 467, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 283, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     self.quant_method = quant_config.get_quant_method(self, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 145, in get_quant_method\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     return BitsAndBytesLinearMethod(self)\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 192, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m     raise ImportError(\n\u001b[0;36m(EngineCore_DP0 pid=2287665)\u001b[0;0m ImportError: Please install bitsandbytes>=0.46.1 via `pip install bitsandbytes>=0.46.1` to use bitsandbytes quantizer.\n[rank0]:[W122 04:53:21.586297215 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2287093)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.2
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Motif-Technologies/Motif-2-12.7B-Base": {
      "model_id": "Motif-Technologies/Motif-2-12.7B-Base",
      "model_name": "Motif-Technologies/Motif-2-12.7B-Base",
      "size_bytes": 12703860896,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:53:31.011130",
      "end_time": "2026-01-22T04:53:31.147550",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Motif-Technologies/Motif-2-12.7B-Instruct": {
      "model_id": "Motif-Technologies/Motif-2-12.7B-Instruct",
      "model_name": "Motif-Technologies/Motif-2-12.7B-Instruct",
      "size_bytes": 12703860896,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T04:53:31.178520",
      "end_time": "2026-01-22T04:53:46.407178",
      "duration_seconds": 15.23,
      "vllm_pid": 2288502,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m [2026-01-22 04:53:39] INFO configuration_motif.py:165:  kwargs : {'absolute_position_embedding': False, 'architectures': ['MotifForCausalLM'], 'attn_rms_norm_eps': 1e-05, 'auto_map': {'AutoConfig': 'configuration_motif.MotifConfig', 'AutoModelForCausalLM': 'modeling_motif.MotifForCausalLM'}, 'bfloat16': True, 'bos_token_id': 219396, 'eos_token_id': 219395, 'expanded': True, 'fused_rope': False, 'head_dim': 128, 'load_pretrained': None, 'loss_reduction': 'mean', 'model_type': 'Motif', 'num_noise_heads': 8, 'tensor_parallel': True, 'torch_dtype': 'float32', 'transformers_version': '4.55.0', 'use_bias': False, 'k_ratio': 1, '_commit_hash': '2d6630091f4601144f2b19230cd2a1208cf12a57', 'attn_implementation': None}\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m [2026-01-22 04:53:39] INFO configuration_motif.py:165:  kwargs : {}\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m Encountered exception while importing kernels: No module named 'kernels'\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m Encountered exception while importing kernels: No module named 'kernels'\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m Encountered exception while importing kernels: No module named 'kernels'\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m Encountered exception while importing kernels: No module named 'kernels'\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1332, in create_engine_config\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     model_config = self.create_model_config()\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1189, in create_model_config\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     return ModelConfig(\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m            ^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m   Value error, Model architecture MotifForCausalLM was supported in vLLM until v0.10.2, and is not supported anymore. Please use an older version of vLLM if you want to use this model architecture. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n\u001b[0;36m(APIServer pid=2288502)\u001b[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 15.23
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "FPHam/Sydney_Overthinker_13b_HF": {
      "model_id": "FPHam/Sydney_Overthinker_13b_HF",
      "model_name": "FPHam/Sydney_Overthinker_13b_HF",
      "size_bytes": 13015864320,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:53:46.414231",
      "end_time": "2026-01-22T04:53:46.497277",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "OpenDFM/ChemDFM-v1.0-13B": {
      "model_id": "OpenDFM/ChemDFM-v1.0-13B",
      "model_name": "OpenDFM/ChemDFM-v1.0-13B",
      "size_bytes": 13091732480,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T04:53:46.614805",
      "end_time": "2026-01-22T04:53:46.722618",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "wave-on-discord/silly-v0.2": {
      "model_id": "wave-on-discord/silly-v0.2",
      "model_name": "wave-on-discord/silly-v0.2",
      "size_bytes": 12247802880,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:46:38.361884",
      "end_time": "2026-01-22T04:59:00.356940",
      "duration_seconds": 742.0,
      "vllm_pid": 2272512,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_045459",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 658.18,
        "total_seconds": 742.0
      },
      "gpu_memory_gb": 1020.05,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-8B-FP8": {
      "model_id": "Qwen/Qwen3-8B-FP8",
      "model_name": "Qwen/Qwen3-8B-FP8",
      "size_bytes": 8191159296,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T02:03:56.435043",
      "end_time": "2026-01-22T05:04:39.971616",
      "duration_seconds": 10843.54,
      "vllm_pid": 1238726,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_044957",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 10767.89,
        "total_seconds": 10843.54
      },
      "gpu_memory_gb": 1028.23,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "LatitudeGames/Wayfarer-2-12B": {
      "model_id": "LatitudeGames/Wayfarer-2-12B",
      "model_name": "LatitudeGames/Wayfarer-2-12B",
      "size_bytes": 12247802880,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:44:24.666454",
      "end_time": "2026-01-22T05:06:20.756858",
      "duration_seconds": 1316.09,
      "vllm_pid": 1571142,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_050552",
      "timing": {
        "server_start_seconds": 65.08,
        "test_execution_seconds": 1242.05,
        "total_seconds": 1316.09
      },
      "gpu_memory_gb": 1028.34,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "LatitudeGames/Muse-12B": {
      "model_id": "LatitudeGames/Muse-12B",
      "model_name": "LatitudeGames/Muse-12B",
      "size_bytes": 12247802880,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:31:14.241726",
      "end_time": "2026-01-22T05:06:59.167167",
      "duration_seconds": 2144.93,
      "vllm_pid": 2239967,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_050008",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 2069.36,
        "total_seconds": 2144.93
      },
      "gpu_memory_gb": 1020.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "LatitudeGames/Wayfarer-12B": {
      "model_id": "LatitudeGames/Wayfarer-12B",
      "model_name": "LatitudeGames/Wayfarer-12B",
      "size_bytes": 12247802880,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:36:30.571564",
      "end_time": "2026-01-22T05:10:22.650734",
      "duration_seconds": 2032.08,
      "vllm_pid": 2251399,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_050806",
      "timing": {
        "server_start_seconds": 70.07,
        "test_execution_seconds": 1952.04,
        "total_seconds": 2032.08
      },
      "gpu_memory_gb": 1020.27,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "CausalLM/miniG": {
      "model_id": "CausalLM/miniG",
      "model_name": "CausalLM/miniG",
      "size_bytes": 13990227488,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T05:10:22.655681",
      "end_time": "2026-01-22T05:10:22.692443",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/nekomata-14b": {
      "model_id": "rinna/nekomata-14b",
      "model_name": "rinna/nekomata-14b",
      "size_bytes": 14167290880,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T05:10:22.702694",
      "end_time": "2026-01-22T05:10:22.767995",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ByteDance-Seed/Seed-X-RM-7B": {
      "model_id": "ByteDance-Seed/Seed-X-RM-7B",
      "model_name": "ByteDance-Seed/Seed-X-RM-7B",
      "size_bytes": 14494291969,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T05:10:22.857344",
      "end_time": "2026-01-22T05:10:22.896842",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "rinna/nekomata-14b-instruction": {
      "model_id": "rinna/nekomata-14b-instruction",
      "model_name": "rinna/nekomata-14b-instruction",
      "size_bytes": 14167290880,
      "tp_size": 1,
      "status": "skipped",
      "start_time": "2026-01-22T05:10:22.798840",
      "end_time": "2026-01-22T05:10:22.830777",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Base model not supported (no chat_template in tokenizer_config.json)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "allenai/OLMo-2-1124-13B-Instruct-preview": {
      "model_id": "allenai/OLMo-2-1124-13B-Instruct-preview",
      "model_name": "allenai/OLMo-2-1124-13B-Instruct-preview",
      "size_bytes": 13716198400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:59:00.365619",
      "end_time": "2026-01-22T05:15:27.244208",
      "duration_seconds": 986.88,
      "vllm_pid": 2299538,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_051135",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 911.56,
        "total_seconds": 986.88
      },
      "gpu_memory_gb": 1020.41,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond": {
      "model_id": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
      "model_name": "Doctor-Shotgun/MS3.2-24B-Magnum-Diamond",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T04:48:25.587386",
      "end_time": "2026-01-22T05:16:24.416192",
      "duration_seconds": 1678.83,
      "vllm_pid": 1579163,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_050733",
      "timing": {
        "server_start_seconds": 90.06,
        "test_execution_seconds": 1579.23,
        "total_seconds": 1678.83
      },
      "gpu_memory_gb": 1027.78,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "FlareRebellion/WeirdCompound-v1.7-24b": {
      "model_id": "FlareRebellion/WeirdCompound-v1.7-24b",
      "model_name": "FlareRebellion/WeirdCompound-v1.7-24b",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "skipped",
      "start_time": "2026-01-22T05:16:24.430874",
      "end_time": "2026-01-22T05:16:24.476583",
      "duration_seconds": null,
      "vllm_pid": null,
      "vllm_version": null,
      "vllm_error": "Corrupted safetensors file: /mnt/baai_cp_perf/hf_models/models--FlareRebellion--WeirdCompound-v1.7-24b/snapshots/6bb1d0d2b3d350f063b2d967aee32252fb399f24/model-00001-of-00010.safetensors (SafetensorError: Error while deserializing header: incomplete metadata, file not fully covered)",
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": null,
      "gpu_memory_gb": null,
      "throughput": null
    },
    "baidu/ERNIE-4.5-21B-A3B-Thinking": {
      "model_id": "baidu/ERNIE-4.5-21B-A3B-Thinking",
      "model_name": "baidu/ERNIE-4.5-21B-A3B-Thinking",
      "size_bytes": 21825437888,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T03:43:20.165035",
      "end_time": "2026-01-22T05:22:54.925418",
      "duration_seconds": 5974.76,
      "vllm_pid": 1449362,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_051804",
      "timing": {
        "server_start_seconds": 85.06,
        "test_execution_seconds": 5877.89,
        "total_seconds": 5974.76
      },
      "gpu_memory_gb": 1027.98,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "allenai/OLMo-2-1124-13B-Instruct": {
      "model_id": "allenai/OLMo-2-1124-13B-Instruct",
      "model_name": "allenai/OLMo-2-1124-13B-Instruct",
      "size_bytes": 13716198400,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:53:46.835930",
      "end_time": "2026-01-22T05:31:34.142234",
      "duration_seconds": 2267.31,
      "vllm_pid": 2288584,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_051641",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2187.19,
        "total_seconds": 2267.31
      },
      "gpu_memory_gb": 1020.84,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "microsoft/Phi-3-medium-128k-instruct": {
      "model_id": "microsoft/Phi-3-medium-128k-instruct",
      "model_name": "microsoft/Phi-3-medium-128k-instruct",
      "size_bytes": 13960238080,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:04:40.072478",
      "end_time": "2026-01-22T05:36:54.910712",
      "duration_seconds": 1934.84,
      "vllm_pid": 1611332,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_052423",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 1854.87,
        "total_seconds": 1934.84
      },
      "gpu_memory_gb": 1028.01,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "microsoft/Phi-3-medium-4k-instruct": {
      "model_id": "microsoft/Phi-3-medium-4k-instruct",
      "model_name": "microsoft/Phi-3-medium-4k-instruct",
      "size_bytes": 13960238080,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:06:20.781991",
      "end_time": "2026-01-22T05:39:03.197776",
      "duration_seconds": 1962.42,
      "vllm_pid": 1615572,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_053807",
      "timing": {
        "server_start_seconds": 70.08,
        "test_execution_seconds": 1882.37,
        "total_seconds": 1962.42
      },
      "gpu_memory_gb": 1027.64,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/Phi-3-medium-4k-instruct": {
      "model_id": "unsloth/Phi-3-medium-4k-instruct",
      "model_name": "unsloth/Phi-3-medium-4k-instruct",
      "size_bytes": 13960238080,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:06:59.173760",
      "end_time": "2026-01-22T05:39:29.067567",
      "duration_seconds": 1949.89,
      "vllm_pid": 2316389,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_053252",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 1875.0,
        "total_seconds": 1949.89
      },
      "gpu_memory_gb": 1020.82,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b": {
      "model_id": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
      "model_name": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T05:22:54.961538",
      "end_time": "2026-01-22T05:48:34.413618",
      "duration_seconds": 1539.45,
      "vllm_pid": 1648628,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_054019",
      "timing": {
        "server_start_seconds": 85.05,
        "test_execution_seconds": 1443.81,
        "total_seconds": 1539.45
      },
      "gpu_memory_gb": 1027.29,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "AXCXEPT/phi-4-deepseek-R1K-RL-EZO": {
      "model_id": "AXCXEPT/phi-4-deepseek-R1K-RL-EZO",
      "model_name": "AXCXEPT/phi-4-deepseek-R1K-RL-EZO",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:10:22.912308",
      "end_time": "2026-01-22T05:51:37.269610",
      "duration_seconds": 2474.36,
      "vllm_pid": 2323373,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_054045",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2394.02,
        "total_seconds": 2474.36
      },
      "gpu_memory_gb": 1020.46,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "AXCXEPT/phi-4-open-R1-Distill-EZOv1": {
      "model_id": "AXCXEPT/phi-4-open-R1-Distill-EZOv1",
      "model_name": "AXCXEPT/phi-4-open-R1-Distill-EZOv1",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:15:27.253450",
      "end_time": "2026-01-22T05:52:16.618437",
      "duration_seconds": 2209.36,
      "vllm_pid": 2334134,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_054045",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2128.09,
        "total_seconds": 2209.36
      },
      "gpu_memory_gb": 1020.39,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/NVIDIA-Nemotron-Nano-9B-v2": {
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "model_name": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "size_bytes": 8888227328,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:05:01.046932",
      "end_time": "2026-01-22T06:07:47.734111",
      "duration_seconds": 10966.69,
      "vllm_pid": 2061819,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_055337",
      "timing": {
        "server_start_seconds": 55.04,
        "test_execution_seconds": 10878.47,
        "total_seconds": 10966.69
      },
      "gpu_memory_gb": 1020.48,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "pfnet/plamo-2-translate": {
      "model_id": "pfnet/plamo-2-translate",
      "model_name": "pfnet/plamo-2-translate",
      "size_bytes": 9528235008,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:18:10.089399",
      "end_time": "2026-01-22T06:10:48.103958",
      "duration_seconds": 10358.01,
      "vllm_pid": 1395091,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_055002",
      "timing": {
        "server_start_seconds": 135.06,
        "test_execution_seconds": 10185.36,
        "total_seconds": 10358.01
      },
      "gpu_memory_gb": 1028.78,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Orion-zhen/phi-4-abliterated": {
      "model_id": "Orion-zhen/phi-4-abliterated",
      "model_name": "Orion-zhen/phi-4-abliterated",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:31:34.147703",
      "end_time": "2026-01-22T06:12:37.194540",
      "duration_seconds": 2463.05,
      "vllm_pid": 2366503,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_060900",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 2377.67,
        "total_seconds": 2463.05
      },
      "gpu_memory_gb": 1020.82,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/phi-4-abliterated": {
      "model_id": "huihui-ai/phi-4-abliterated",
      "model_name": "huihui-ai/phi-4-abliterated",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:36:54.933731",
      "end_time": "2026-01-22T06:17:40.574764",
      "duration_seconds": 2445.64,
      "vllm_pid": 1677117,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_061208",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2365.52,
        "total_seconds": 2445.64
      },
      "gpu_memory_gb": 1027.64,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ReadyArt/Broken-Tutu-24B-Transgression-v2.0": {
      "model_id": "ReadyArt/Broken-Tutu-24B-Transgression-v2.0",
      "model_name": "ReadyArt/Broken-Tutu-24B-Transgression-v2.0",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T05:48:34.498278",
      "end_time": "2026-01-22T06:21:01.381601",
      "duration_seconds": 1946.88,
      "vllm_pid": 1700878,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_061859",
      "timing": {
        "server_start_seconds": 85.05,
        "test_execution_seconds": 1851.89,
        "total_seconds": 1946.88
      },
      "gpu_memory_gb": 1027.66,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "ReadyArt/Broken-Tutu-24B-Unslop-v2.0": {
      "model_id": "ReadyArt/Broken-Tutu-24B-Unslop-v2.0",
      "model_name": "ReadyArt/Broken-Tutu-24B-Unslop-v2.0",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-22T06:21:01.468720",
      "end_time": "2026-01-22T06:21:46.762550",
      "duration_seconds": 45.29,
      "vllm_pid": 1764028,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m The tokenizer you are loading from '/mnt/baai_cp_perf/hf_models/models--ReadyArt--Broken-Tutu-24B-Unslop-v2.0/snapshots/864aeaae1a4f6b482e68913009a4c3315d0f1373' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n\u001b[0;36m(Worker_TP0 pid=1764904)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=1764904)\u001b[0;0m \nLoading safetensors checkpoint shards:  10% Completed | 1/10 [00:02<00:25,  2.86s/it]\n\u001b[0;36m(Worker_TP0 pid=1764904)\u001b[0;0m \nLoading safetensors checkpoint shards:  10% Completed | 1/10 [00:02<00:25,  2.87s/it]\n\u001b[0;36m(Worker_TP0 pid=1764904)\u001b[0;0m \n[rank0]:[W122 06:21:34.069941990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=1764321)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=1764028)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 45.29
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.0": {
      "model_id": "ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.0",
      "model_name": "ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.0",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "test_error",
      "start_time": "2026-01-22T06:21:46.770861",
      "end_time": "2026-01-22T06:23:59.495146",
      "duration_seconds": 132.72,
      "vllm_pid": 1765656,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Quick sanity test failed: Quick sanity test FAILED: Error code: 400 - {'error': {'message': \"unexpected char '\\\\\\\\' at 51 None\", 'type': 'BadRequestError', 'param': None, 'code': 400}}",
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 85.06,
        "test_execution_seconds": null,
        "total_seconds": 132.72
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/phi-4": {
      "model_id": "microsoft/phi-4",
      "model_name": "microsoft/phi-4",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:51:37.275054",
      "end_time": "2026-01-22T06:33:00.176850",
      "duration_seconds": 2482.9,
      "vllm_pid": 2406126,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_061448",
      "timing": {
        "server_start_seconds": 65.05,
        "test_execution_seconds": 2407.88,
        "total_seconds": 2482.9
      },
      "gpu_memory_gb": 1020.74,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "unsloth/phi-4": {
      "model_id": "unsloth/phi-4",
      "model_name": "unsloth/phi-4",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:07:47.738677",
      "end_time": "2026-01-22T06:49:13.700676",
      "duration_seconds": 2485.96,
      "vllm_pid": 2437856,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_063419",
      "timing": {
        "server_start_seconds": 70.06,
        "test_execution_seconds": 2405.62,
        "total_seconds": 2485.96
      },
      "gpu_memory_gb": 1020.85,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "prithivMLmods/Phi-4-o1": {
      "model_id": "prithivMLmods/Phi-4-o1",
      "model_name": "prithivMLmods/Phi-4-o1",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:52:16.625077",
      "end_time": "2026-01-22T06:53:29.795917",
      "duration_seconds": 3673.17,
      "vllm_pid": 2407443,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_065033",
      "timing": {
        "server_start_seconds": 75.06,
        "test_execution_seconds": 3584.66,
        "total_seconds": 3673.17
      },
      "gpu_memory_gb": 1020.95,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "arcee-ai/Virtuoso-Small-v2": {
      "model_id": "arcee-ai/Virtuoso-Small-v2",
      "model_name": "arcee-ai/Virtuoso-Small-v2",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:17:40.652309",
      "end_time": "2026-01-22T06:57:17.144793",
      "duration_seconds": 2376.49,
      "vllm_pid": 1756930,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_062529",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 2290.57,
        "total_seconds": 2376.49
      },
      "gpu_memory_gb": 1027.66,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "deepcogito/cogito-v1-preview-qwen-14B": {
      "model_id": "deepcogito/cogito-v1-preview-qwen-14B",
      "model_name": "deepcogito/cogito-v1-preview-qwen-14B",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:33:00.181298",
      "end_time": "2026-01-22T07:02:31.061123",
      "duration_seconds": 1770.88,
      "vllm_pid": 2487988,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_065443",
      "timing": {
        "server_start_seconds": 75.06,
        "test_execution_seconds": 1684.75,
        "total_seconds": 1770.88
      },
      "gpu_memory_gb": 1021.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "arcee-ai/Arcee-Blitz": {
      "model_id": "arcee-ai/Arcee-Blitz",
      "model_name": "arcee-ai/Arcee-Blitz",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T06:23:59.587612",
      "end_time": "2026-01-22T07:05:44.785622",
      "duration_seconds": 2505.2,
      "vllm_pid": 1769665,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_065831",
      "timing": {
        "server_start_seconds": 85.05,
        "test_execution_seconds": 2408.0,
        "total_seconds": 2505.2
      },
      "gpu_memory_gb": 1028.02,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "arcee-ai/Homunculus": {
      "model_id": "arcee-ai/Homunculus",
      "model_name": "arcee-ai/Homunculus",
      "size_bytes": 12458808320,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:50:52.485048",
      "end_time": "2026-01-22T07:08:20.142067",
      "duration_seconds": 8247.66,
      "vllm_pid": 2281776,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_070350",
      "timing": {
        "server_start_seconds": 60.05,
        "test_execution_seconds": 8174.81,
        "total_seconds": 8247.66
      },
      "gpu_memory_gb": 1021.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "AIDC-AI/Marco-MT-Algharb": {
      "model_id": "AIDC-AI/Marco-MT-Algharb",
      "model_name": "AIDC-AI/Marco-MT-Algharb",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:08:20.147795",
      "end_time": "2026-01-22T07:22:51.490670",
      "duration_seconds": 871.34,
      "vllm_pid": 2557585,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "score": 0.2689
        },
        "math_500": {
          "score": 0.0069
        },
        "gsm8k": {
          "score": 0.0
        },
        "mmlu_pro": {
          "score": 0.0614
        },
        "gpqa_diamond": {
          "score": 0.19
        }
      },
      "eval_output_dir": "outputs/20260122_070932",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 791.8,
        "total_seconds": 871.34
      },
      "gpu_memory_gb": 1021.16,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "sometimesanotion/Lamarck-14B-v0.7": {
      "model_id": "sometimesanotion/Lamarck-14B-v0.7",
      "model_name": "sometimesanotion/Lamarck-14B-v0.7",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:49:13.708865",
      "end_time": "2026-01-22T07:27:58.229851",
      "duration_seconds": 2324.52,
      "vllm_pid": 2519744,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_072437",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 2237.98,
        "total_seconds": 2324.52
      },
      "gpu_memory_gb": 1021.18,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "sthenno-com/miscii-14b-0218": {
      "model_id": "sthenno-com/miscii-14b-0218",
      "model_name": "sthenno-com/miscii-14b-0218",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:53:29.801520",
      "end_time": "2026-01-22T07:32:30.789223",
      "duration_seconds": 2340.99,
      "vllm_pid": 2528321,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_072913",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2259.93,
        "total_seconds": 2340.99
      },
      "gpu_memory_gb": 1021.1,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "sthenno-com/miscii-14b-1225": {
      "model_id": "sthenno-com/miscii-14b-1225",
      "model_name": "sthenno-com/miscii-14b-1225",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:57:17.235147",
      "end_time": "2026-01-22T07:33:08.481279",
      "duration_seconds": 2151.25,
      "vllm_pid": 1834087,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_070714",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2070.05,
        "total_seconds": 2151.25
      },
      "gpu_memory_gb": 1028.02,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "dphn/Dolphin-Mistral-24B-Venice-Edition": {
      "model_id": "dphn/Dolphin-Mistral-24B-Venice-Edition",
      "model_name": "dphn/Dolphin-Mistral-24B-Venice-Edition",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T07:05:44.862441",
      "end_time": "2026-01-22T07:36:50.619243",
      "duration_seconds": 1865.76,
      "vllm_pid": 1850154,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_073434",
      "timing": {
        "server_start_seconds": 85.06,
        "test_execution_seconds": 1769.25,
        "total_seconds": 1865.76
      },
      "gpu_memory_gb": 1028.02,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "suayptalha/Lamarckvergence-14B": {
      "model_id": "suayptalha/Lamarckvergence-14B",
      "model_name": "suayptalha/Lamarckvergence-14B",
      "size_bytes": 14765947904,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:02:31.065996",
      "end_time": "2026-01-22T07:40:39.158906",
      "duration_seconds": 2288.09,
      "vllm_pid": 2546163,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_073351",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 2201.76,
        "total_seconds": 2288.09
      },
      "gpu_memory_gb": 1021.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated": {
      "model_id": "huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated",
      "model_name": "huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T07:36:50.679959",
      "end_time": "2026-01-22T08:07:33.248079",
      "duration_seconds": 1842.57,
      "vllm_pid": 1910486,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "score": 0.8279
        },
        "mmlu_pro": {
          "score": 0.6657
        },
        "math_500": {
          "score": 0.739
        },
        "gpqa_diamond": {
          "score": 0.47
        },
        "gsm8k": {
          "score": 0.96
        }
      },
      "eval_output_dir": "outputs/20260122_073818",
      "timing": {
        "server_start_seconds": 85.05,
        "test_execution_seconds": 1748.01,
        "total_seconds": 1842.57
      },
      "gpu_memory_gb": 1027.53,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HelpingAI/Dhanishtha-2.0-preview-0825": {
      "model_id": "HelpingAI/Dhanishtha-2.0-preview-0825",
      "model_name": "HelpingAI/Dhanishtha-2.0-preview-0825",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:32:30.795902",
      "end_time": "2026-01-22T08:09:11.524710",
      "duration_seconds": 2200.73,
      "vllm_pid": 2607039,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_074155",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 2113.03,
        "total_seconds": 2200.73
      },
      "gpu_memory_gb": 1021.02,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-14B-AWQ": {
      "model_id": "Qwen/Qwen3-14B-AWQ",
      "model_name": "Qwen/Qwen3-14B-AWQ",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T08:09:11.530305",
      "end_time": "2026-01-22T08:09:51.711835",
      "duration_seconds": 40.18,
      "vllm_pid": 2678234,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.80s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.89s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:09<00:00,  4.87s/it]\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m \n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 56, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     process_weights_after_loading(model, model_config, target_device)\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 108, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     quant_method.process_weights_after_loading(module)\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py\", line 385, in process_weights_after_loading\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     marlin_scales = marlin_permute_scales(\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 302, in marlin_permute_scales\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m     s = s.reshape((-1, len(scale_perm)))[:, scale_perm]\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m Search for `cudaErrorUnsupportedPtxVersion' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\u001b[0;36m(EngineCore_DP0 pid=2678259)\u001b[0;0m \n[rank0]:[W122 08:09:41.025849189 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2678234)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 40.18
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "KORMo-Team/KORMo-10B-sft": {
      "model_id": "KORMo-Team/KORMo-10B-sft",
      "model_name": "KORMo-Team/KORMo-10B-sft",
      "size_bytes": 10756624384,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T03:41:34.869316",
      "end_time": "2026-01-22T08:13:46.552691",
      "duration_seconds": 16331.68,
      "vllm_pid": 1445035,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_080909",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 16245.46,
        "total_seconds": 16331.68
      },
      "gpu_memory_gb": 1028.43,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HelpingAI/Dhanishtha-2.0-preview": {
      "model_id": "HelpingAI/Dhanishtha-2.0-preview",
      "model_name": "HelpingAI/Dhanishtha-2.0-preview",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:27:58.239902",
      "end_time": "2026-01-22T08:22:07.751488",
      "duration_seconds": 3249.51,
      "vllm_pid": 2597926,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_081104",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 3167.45,
        "total_seconds": 3249.51
      },
      "gpu_memory_gb": 1021.1,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "nvidia/NVIDIA-Nemotron-Nano-12B-v2": {
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "model_name": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "size_bytes": 12310001152,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T04:48:41.355495",
      "end_time": "2026-01-22T08:29:39.553985",
      "duration_seconds": 13258.2,
      "vllm_pid": 2277094,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_082320",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 13168.96,
        "total_seconds": 13258.2
      },
      "gpu_memory_gb": 1020.91,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-14B-Base": {
      "model_id": "Qwen/Qwen3-14B-Base",
      "model_name": "Qwen/Qwen3-14B-Base",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T08:09:51.715737",
      "end_time": "2026-01-22T08:48:28.058071",
      "duration_seconds": 2316.34,
      "vllm_pid": 2679046,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_083133",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2235.98,
        "total_seconds": 2316.34
      },
      "gpu_memory_gb": 1021.07,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "internlm/JanusCoder-14B": {
      "model_id": "internlm/JanusCoder-14B",
      "model_name": "internlm/JanusCoder-14B",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T08:22:07.755586",
      "end_time": "2026-01-22T09:01:17.366229",
      "duration_seconds": 2349.61,
      "vllm_pid": 2703491,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_084940",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2269.25,
        "total_seconds": 2349.61
      },
      "gpu_memory_gb": 1021.08,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Goekdeniz-Guelmez/Josiefied-Qwen3-14B-abliterated-v3": {
      "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-14B-abliterated-v3",
      "model_name": "Goekdeniz-Guelmez/Josiefied-Qwen3-14B-abliterated-v3",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:22:51.498129",
      "end_time": "2026-01-22T09:04:58.019312",
      "duration_seconds": 6126.52,
      "vllm_pid": 2587394,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_090233",
      "timing": {
        "server_start_seconds": 100.06,
        "test_execution_seconds": 6013.06,
        "total_seconds": 6126.52
      },
      "gpu_memory_gb": 1021.18,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "HelpingAI/Dhanishtha-nsfw": {
      "model_id": "HelpingAI/Dhanishtha-nsfw",
      "model_name": "HelpingAI/Dhanishtha-nsfw",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:33:08.567116",
      "end_time": "2026-01-22T09:06:28.542704",
      "duration_seconds": 5599.98,
      "vllm_pid": 1902966,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_081504",
      "timing": {
        "server_start_seconds": 80.05,
        "test_execution_seconds": 5506.98,
        "total_seconds": 5599.98
      },
      "gpu_memory_gb": 1027.53,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "dnotitia/DNA-R1": {
      "model_id": "dnotitia/DNA-R1",
      "model_name": "dnotitia/DNA-R1",
      "size_bytes": 14659548160,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:10:48.209982",
      "end_time": "2026-01-22T09:18:10.969472",
      "duration_seconds": 11242.76,
      "vllm_pid": 1744001,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_090750",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 11155.73,
        "total_seconds": 11242.76
      },
      "gpu_memory_gb": 1027.52,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen3-14B": {
      "model_id": "Qwen/Qwen3-14B",
      "model_name": "Qwen/Qwen3-14B",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T07:40:39.163390",
      "end_time": "2026-01-22T09:31:19.847170",
      "duration_seconds": 6640.68,
      "vllm_pid": 2622853,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_090614",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 6557.28,
        "total_seconds": 6640.68
      },
      "gpu_memory_gb": 1021.08,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "puwaer/Doujinshi-14b-chat": {
      "model_id": "puwaer/Doujinshi-14b-chat",
      "model_name": "puwaer/Doujinshi-14b-chat",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T08:48:28.067088",
      "end_time": "2026-01-22T09:31:25.167756",
      "duration_seconds": 2577.1,
      "vllm_pid": 2755193,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_090614",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2497.27,
        "total_seconds": 2577.1
      },
      "gpu_memory_gb": 1021.08,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "DatarusAI/Datarus-R1-14B-preview": {
      "model_id": "DatarusAI/Datarus-R1-14B-preview",
      "model_name": "DatarusAI/Datarus-R1-14B-preview",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T09:31:19.853292",
      "end_time": "2026-01-22T09:35:23.848787",
      "duration_seconds": 244.0,
      "vllm_pid": 2837893,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--DatarusAI--Datarus-R1-14B-preview/snapshots/60b4cb859cdcdc323702d4898f8916e207df6191` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--DatarusAI--Datarus-R1-14B-preview/snapshots/60b4cb859cdcdc323702d4898f8916e207df6191` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 80.06,
        "test_execution_seconds": 140.86,
        "total_seconds": 244.0
      },
      "gpu_memory_gb": 1021.28,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "FractalAIResearch/Fathom-R1-14B": {
      "model_id": "FractalAIResearch/Fathom-R1-14B",
      "model_name": "FractalAIResearch/Fathom-R1-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "test_error",
      "start_time": "2026-01-22T09:31:25.171652",
      "end_time": "2026-01-22T09:35:31.155048",
      "duration_seconds": 245.98,
      "vllm_pid": 2837920,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": "Test failed with exit code: 1",
      "test_stderr": "Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--FractalAIResearch--Fathom-R1-14B/snapshots/785c629b2f4a032233576f711669b5c470ab4c76` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\nTraceback (most recent call last):\n  File \"/mnt/baai_cp_perf/qbw/evalscope-experiment-runner/run_experiment.py\", line 869, in run_test\n    run_task(task_cfg=task_cfg)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 28, in run_task\n    return run_single_task(task_cfg, run_time)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 42, in run_single_task\n    result = evaluate_model(task_cfg, outputs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/run.py\", line 147, in evaluate_model\n    res_dict = evaluator.eval()\n               ^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 109, in eval\n    subset_score = self.evaluate_subset(subset, dataset)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 139, in evaluate_subset\n    task_states = self.get_answers(subset, dataset)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 197, in get_answers\n    finished_task_states = run_in_threads_with_progress(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 265, in run_in_threads_with_progress\n    on_error(items[index], exc)\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 195, in on_error\n    raise exc\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 257, in run_in_threads_with_progress\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 183, in worker\n    return self._predict_sample(sample, model_prediction_dir)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py\", line 225, in _predict_sample\n    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 453, in run_inference\n    model_output = self._on_inference(model, sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py\", line 404, in _on_inference\n    model_output = model.generate(input=sample.input, tools=sample.tools)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/api/model/model.py\", line 198, in generate\n    output = self.api.generate(\n             ^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/models/openai_compatible.py\", line 93, in generate\n    completion = retry_call(\n                 ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/evalscope/utils/function_utils.py\", line 106, in retry_call\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `/mnt/baai_cp_perf/hf_models/models--FractalAIResearch--Fathom-R1-14B/snapshots/785c629b2f4a032233576f711669b5c470ab4c76` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}}\n",
      "test_stdout": "",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 147.61,
        "total_seconds": 245.98
      },
      "gpu_memory_gb": 1021.28,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "JungZoona/T3Q-qwen2.5-14b-v1.0-e3": {
      "model_id": "JungZoona/T3Q-qwen2.5-14b-v1.0-e3",
      "model_name": "JungZoona/T3Q-qwen2.5-14b-v1.0-e3",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T09:35:31.158478",
      "end_time": "2026-01-22T09:36:01.378944",
      "duration_seconds": 30.22,
      "vllm_pid": 2846579,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.attn = attn_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 230, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m     self.impl = impl_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2846656)\u001b[0;0m TypeError: FlashAttentionImpl.__init__() got an unexpected keyword argument 'layer_idx'\n[rank0]:[W122 09:35:53.642831670 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2846579)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-reasoning-plus": {
      "model_id": "microsoft/Phi-4-reasoning-plus",
      "model_name": "microsoft/Phi-4-reasoning-plus",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:39:29.071883",
      "end_time": "2026-01-22T09:40:31.934162",
      "duration_seconds": 14462.86,
      "vllm_pid": 2382515,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_093717",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 14378.97,
        "total_seconds": 14462.86
      },
      "gpu_memory_gb": 1021.19,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "sarvamai/sarvam-m": {
      "model_id": "sarvamai/sarvam-m",
      "model_name": "sarvamai/sarvam-m",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T08:07:33.298030",
      "end_time": "2026-01-22T09:44:34.384568",
      "duration_seconds": 5821.09,
      "vllm_pid": 1968905,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_091933",
      "timing": {
        "server_start_seconds": 90.06,
        "test_execution_seconds": 5717.76,
        "total_seconds": 5821.09
      },
      "gpu_memory_gb": 1027.53,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "stelterlab/Mistral-Small-24B-Instruct-2501-AWQ": {
      "model_id": "stelterlab/Mistral-Small-24B-Instruct-2501-AWQ",
      "model_name": "stelterlab/Mistral-Small-24B-Instruct-2501-AWQ",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-22T09:44:34.467468",
      "end_time": "2026-01-22T09:45:24.690211",
      "duration_seconds": 50.22,
      "vllm_pid": 2151896,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m [2026-01-22 09:44:43] INFO tekken.py:184: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m [2026-01-22 09:44:43] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m [2026-01-22 09:44:43] INFO tekken.py:567: Cutting non special vocabulary to first 130072 tokens.\n\u001b[0;36m(Worker_TP0 pid=2152507)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=2152507)\u001b[0;0m \nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:03<00:06,  3.46s/it]\n\u001b[0;36m(Worker_TP0 pid=2152507)\u001b[0;0m \nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:07<00:03,  3.73s/it]\n\u001b[0;36m(Worker_TP0 pid=2152507)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:10<00:00,  3.43s/it]\n\u001b[0;36m(Worker_TP0 pid=2152507)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:10<00:00,  3.49s/it]\n\u001b[0;36m(Worker_TP0 pid=2152507)\u001b[0;0m \n[rank0]:[W122 09:45:16.427899246 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 97, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     super().__init__(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 172, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 660, in wait_for_ready\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m     raise e from None\n\u001b[0;36m(EngineCore_DP0 pid=2152373)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2151896)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 50.22
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "unsloth/Devstral-Small-2505": {
      "model_id": "unsloth/Devstral-Small-2505",
      "model_name": "unsloth/Devstral-Small-2505",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "vllm_crash",
      "start_time": "2026-01-22T09:45:24.772075",
      "end_time": "2026-01-22T09:47:00.064130",
      "duration_seconds": 95.29,
      "vllm_pid": 2153934,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m [2026-01-22 09:45:33] INFO tekken.py:184: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m [2026-01-22 09:45:33] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m [2026-01-22 09:45:33] INFO tekken.py:567: Cutting non special vocabulary to first 130072 tokens.\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  10% Completed | 1/10 [00:02<00:26,  2.93s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  20% Completed | 2/10 [00:05<00:23,  2.92s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  30% Completed | 3/10 [00:08<00:20,  2.88s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  40% Completed | 4/10 [00:11<00:17,  2.86s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  50% Completed | 5/10 [00:14<00:14,  2.87s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  60% Completed | 6/10 [00:17<00:11,  2.87s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  70% Completed | 7/10 [00:20<00:08,  2.88s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  80% Completed | 8/10 [00:23<00:05,  2.93s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards:  90% Completed | 9/10 [00:26<00:02,  2.96s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 10/10 [00:28<00:00,  2.74s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nLoading safetensors checkpoint shards: 100% Completed | 10/10 [00:28<00:00,  2.85s/it]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m 2026-01-22 09:46:40,673 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(Worker_TP1 pid=2153997)\u001b[0;0m 2026-01-22 09:46:40,673 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n\u001b[0;36m(Worker_TP1 pid=2153997)\u001b[0;0m 2026-01-22 09:46:40,690 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m 2026-01-22 09:46:40,690 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|\u258f         | 1/51 [00:01<01:01,  1.23s/it]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|\u258d         | 2/51 [00:01<00:27,  1.75it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|\u258c         | 3/51 [00:01<00:17,  2.71it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|\u258a         | 4/51 [00:01<00:12,  3.76it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|\u2589         | 5/51 [00:01<00:09,  4.79it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|\u2588\u258f        | 6/51 [00:01<00:07,  5.74it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|\u2588\u258e        | 7/51 [00:01<00:06,  6.59it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|\u2588\u258c        | 8/51 [00:01<00:05,  7.36it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|\u2588\u258a        | 9/51 [00:02<00:05,  7.97it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|\u2588\u2588\u258f       | 11/51 [00:02<00:04,  9.06it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|\u2588\u2588\u258c       | 13/51 [00:02<00:03,  9.77it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|\u2588\u2588\u2589       | 15/51 [00:02<00:03, 10.44it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:02<00:03, 11.10it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:02<00:02, 11.77it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:03<00:02, 12.48it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:03<00:02, 12.87it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:03<00:01, 13.12it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:03<00:01, 13.77it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:03<00:01, 14.10it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 31/51 [00:03<00:01, 14.46it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 33/51 [00:03<00:01, 15.00it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 35/51 [00:04<00:01, 15.74it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 37/51 [00:04<00:00, 16.16it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 39/51 [00:04<00:00, 16.73it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 41/51 [00:04<00:00, 17.49it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 44/51 [00:04<00:00, 18.51it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 46/51 [00:04<00:00, 18.83it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 48/51 [00:04<00:00, 19.07it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:04<00:00, 19.78it/s]\nCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:04<00:00, 10.58it/s]\n\u001b[0;36m(Worker_TP0 pid=2153996)\u001b[0;0m \nCapturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]\nCapturing CUDA graphs (decode, FULL):   2%|\u258f         | 1/51 [00:00<00:06,  8.04it/s]\nCapturing CUDA graphs (decode, FULL):   4%|\u258d         | 2/51 [00:00<00:05,  8.60it/s]\nCapturing CUDA graphs (decode, FULL):   6%|\u258c         | 3/51 [00:00<00:05,  8.97it/s]\nCapturing CUDA graphs (decode, FULL):   8%|\u258a         | 4/51 [00:00<00:05,  9.20it/s]\nCapturing CUDA graphs (decode, FULL):  10%|\u2589         | 5/51 [00:00<00:04,  9.39it/s]\nCapturing CUDA graphs (decode, FULL):  14%|\u2588\u258e        | 7/51 [00:00<00:04,  9.72it/s]\nCapturing CUDA graphs (decode, FULL):  18%|\u2588\u258a        | 9/51 [00:00<00:04, 10.18it/s]\nCapturing CUDA graphs (decode, FULL):  22%|\u2588\u2588\u258f       | 11/51 [00:01<00:03, 10.58it/s]\nCapturing CUDA graphs (decode, FULL):  25%|\u2588\u2588\u258c       | 13/51 [00:01<00:03, 11.22it/s]\nCapturing CUDA graphs (decode, FULL):  29%|\u2588\u2588\u2589       | 15/51 [00:01<00:03, 11.90it/s]\nCapturing CUDA graphs (decode, FULL):  33%|\u2588\u2588\u2588\u258e      | 17/51 [00:01<00:02, 12.67it/s]\nCapturing CUDA graphs (decode, FULL):  37%|\u2588\u2588\u2588\u258b      | 19/51 [00:01<00:02, 13.49it/s]\nCapturing CUDA graphs (decode, FULL):  41%|\u2588\u2588\u2588\u2588      | 21/51 [00:01<00:02, 14.19it/s]\nCapturing CUDA graphs (decode, FULL):  45%|\u2588\u2588\u2588\u2588\u258c     | 23/51 [00:01<00:01, 14.94it/s]\nCapturing CUDA graphs (decode, FULL):  49%|\u2588\u2588\u2588\u2588\u2589     | 25/51 [00:02<00:01, 15.61it/s]\nCapturing CUDA graphs (decode, FULL):  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 27/51 [00:02<00:01, 16.44it/s]\nCapturing CUDA graphs (decode, FULL):  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 29/51 [00:02<00:01, 17.31it/s]\nCapturing CUDA graphs (decode, FULL):  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 32/51 [00:02<00:01, 18.66it/s]\nCapturing CUDA graphs (decode, FULL):  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 35/51 [00:02<00:00, 20.12it/s]\nCapturing CUDA graphs (decode, FULL):  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 38/51 [00:02<00:00, 21.72it/s]\nCapturing CUDA graphs (decode, FULL):  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 41/51 [00:02<00:00, 23.70it/s]\nCapturing CUDA graphs (decode, FULL):  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 45/51 [00:02<00:00, 26.26it/s]\nCapturing CUDA graphs (decode, FULL):  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 49/51 [00:02<00:00, 28.04it/s]\nCapturing CUDA graphs (decode, FULL): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:03<00:00, 16.75it/s]\n\u001b[0;36m(EngineCore_DP0 pid=2153959)\u001b[0;0m [2026-01-22 09:46:49] INFO tekken.py:184: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n\u001b[0;36m(EngineCore_DP0 pid=2153959)\u001b[0;0m [2026-01-22 09:46:49] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.\n\u001b[0;36m(EngineCore_DP0 pid=2153959)\u001b[0;0m [2026-01-22 09:46:49] INFO tekken.py:567: Cutting non special vocabulary to first 130072 tokens.\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1423, in run_server_worker\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     await init_app_state(engine_client, app.state, args)\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1101, in init_app_state\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     resolved_chat_template = await process_chat_template(\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/utils.py\", line 300, in process_chat_template\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     resolved_chat_template = resolve_mistral_chat_template(\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/chat_utils.py\", line 452, in resolve_mistral_chat_template\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m     raise ValueError(\n\u001b[0;36m(APIServer pid=2153934)\u001b[0;0m ValueError: 'chat_template' or 'chat_template_kwargs' cannot be overridden for mistral tokenizer.\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 95.29
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "microsoft/Phi-4-reasoning": {
      "model_id": "microsoft/Phi-4-reasoning",
      "model_name": "microsoft/Phi-4-reasoning",
      "size_bytes": 14659507200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T05:39:03.244414",
      "end_time": "2026-01-22T09:48:38.683000",
      "duration_seconds": 14975.44,
      "vllm_pid": 1682201,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_094831",
      "timing": {
        "server_start_seconds": 70.08,
        "test_execution_seconds": 14891.58,
        "total_seconds": 14975.44
      },
      "gpu_memory_gb": 1027.37,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen2.5-14B-Instruct-1M": {
      "model_id": "Qwen/Qwen2.5-14B-Instruct-1M",
      "model_name": "Qwen/Qwen2.5-14B-Instruct-1M",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T09:48:38.688825",
      "end_time": "2026-01-22T09:49:08.958496",
      "duration_seconds": 30.27,
      "vllm_pid": 2160344,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.attn = attn_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 230, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m     self.impl = impl_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2160917)\u001b[0;0m TypeError: FlashAttentionImpl.__init__() got an unexpected keyword argument 'layer_idx'\n[rank0]:[W122 09:49:00.951289240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2160344)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.27
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Tesslate/UIGEN-T3-14B-Preview": {
      "model_id": "Tesslate/UIGEN-T3-14B-Preview",
      "model_name": "Tesslate/UIGEN-T3-14B-Preview",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T08:13:46.622592",
      "end_time": "2026-01-22T09:52:19.631627",
      "duration_seconds": 5913.01,
      "vllm_pid": 1981095,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_095027",
      "timing": {
        "server_start_seconds": 70.21,
        "test_execution_seconds": 5828.07,
        "total_seconds": 5913.01
      },
      "gpu_memory_gb": 1027.46,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "naver-hyperclovax/HyperCLOVAX-SEED-Think-14B": {
      "model_id": "naver-hyperclovax/HyperCLOVAX-SEED-Think-14B",
      "model_name": "naver-hyperclovax/HyperCLOVAX-SEED-Think-14B",
      "size_bytes": 14748112896,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T06:12:37.198769",
      "end_time": "2026-01-22T10:07:54.289337",
      "duration_seconds": 14117.09,
      "vllm_pid": 2447890,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_094148",
      "timing": {
        "server_start_seconds": 125.06,
        "test_execution_seconds": 13978.16,
        "total_seconds": 14117.09
      },
      "gpu_memory_gb": 1021.32,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Goekdeniz-Guelmez/Josiefied-Qwen2.5-14B-Instruct-abliterated-v4": {
      "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-14B-Instruct-abliterated-v4",
      "model_name": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-14B-Instruct-abliterated-v4",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:35:23.853479",
      "end_time": "2026-01-22T10:14:36.285334",
      "duration_seconds": 2352.43,
      "vllm_pid": 2846020,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_100920",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 2270.93,
        "total_seconds": 2352.43
      },
      "gpu_memory_gb": 1021.35,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mlabonne/Qwen3-14B-abliterated": {
      "model_id": "mlabonne/Qwen3-14B-abliterated",
      "model_name": "mlabonne/Qwen3-14B-abliterated",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T08:29:39.558859",
      "end_time": "2026-01-22T10:17:31.364135",
      "duration_seconds": 6471.81,
      "vllm_pid": 2718233,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_101622",
      "timing": {
        "server_start_seconds": 105.06,
        "test_execution_seconds": 6350.35,
        "total_seconds": 6471.81
      },
      "gpu_memory_gb": 1021.39,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "rstar2-reproduce/rStar2-Agent-14B": {
      "model_id": "rstar2-reproduce/rStar2-Agent-14B",
      "model_name": "rstar2-reproduce/rStar2-Agent-14B",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:01:17.375547",
      "end_time": "2026-01-22T10:23:50.127614",
      "duration_seconds": 4952.75,
      "vllm_pid": 2779790,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_101847",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 4869.6,
        "total_seconds": 4952.75
      },
      "gpu_memory_gb": 1021.49,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Qwen/Qwen2.5-Coder-14B-Instruct": {
      "model_id": "Qwen/Qwen2.5-Coder-14B-Instruct",
      "model_name": "Qwen/Qwen2.5-Coder-14B-Instruct",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:49:08.990482",
      "end_time": "2026-01-22T10:29:11.554294",
      "duration_seconds": 2402.56,
      "vllm_pid": 2161574,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_095337",
      "timing": {
        "server_start_seconds": 75.09,
        "test_execution_seconds": 2316.54,
        "total_seconds": 2402.56
      },
      "gpu_memory_gb": 1027.52,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "soob3123/amoral-qwen3-14B": {
      "model_id": "soob3123/amoral-qwen3-14B",
      "model_name": "soob3123/amoral-qwen3-14B",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:06:28.579841",
      "end_time": "2026-01-22T10:31:07.669545",
      "duration_seconds": 5079.09,
      "vllm_pid": 2080166,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_103028",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 4989.92,
        "total_seconds": 5079.09
      },
      "gpu_memory_gb": 1027.96,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "Tesslate/UIGEN-T1.1-Qwen-14B": {
      "model_id": "Tesslate/UIGEN-T1.1-Qwen-14B",
      "model_name": "Tesslate/UIGEN-T1.1-Qwen-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:52:19.706259",
      "end_time": "2026-01-22T10:37:18.513793",
      "duration_seconds": 2698.81,
      "vllm_pid": 2168291,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_103235",
      "timing": {
        "server_start_seconds": 70.2,
        "test_execution_seconds": 2613.48,
        "total_seconds": 2698.81
      },
      "gpu_memory_gb": 1028.06,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated": {
      "model_id": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
      "model_name": "huihui-ai/Qwen2.5-14B-Instruct-1M-abliterated",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T10:37:18.524887",
      "end_time": "2026-01-22T10:37:48.958342",
      "duration_seconds": 30.43,
      "vllm_pid": 2255077,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.attn = attn_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 230, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m     self.impl = impl_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2255101)\u001b[0;0m TypeError: FlashAttentionImpl.__init__() got an unexpected keyword argument 'layer_idx'\n[rank0]:[W122 10:37:40.548800537 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2255077)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 30.43
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "Krystalan/DRT-14B": {
      "model_id": "Krystalan/DRT-14B",
      "model_name": "Krystalan/DRT-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:36:01.384427",
      "end_time": "2026-01-22T10:39:54.955549",
      "duration_seconds": 3833.57,
      "vllm_pid": 2847436,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_102512",
      "timing": {
        "server_start_seconds": 70.06,
        "test_execution_seconds": 3749.93,
        "total_seconds": 3833.57
      },
      "gpu_memory_gb": 1021.61,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "mkurman/Qwen2.5-14B-DeepSeek-R1-1M": {
      "model_id": "mkurman/Qwen2.5-14B-DeepSeek-R1-1M",
      "model_name": "mkurman/Qwen2.5-14B-DeepSeek-R1-1M",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "vllm_crash",
      "start_time": "2026-01-22T10:39:54.960906",
      "end_time": "2026-01-22T10:40:30.537357",
      "duration_seconds": 35.58,
      "vllm_pid": 2972482,
      "vllm_version": null,
      "vllm_error": "vLLM process crashed with exit code: 1",
      "vllm_stderr": "\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m Process EngineCore_DP0:\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.run()\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/multiprocessing/process.py\", line 108, in run\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     raise e\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     super().__init__(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self._init_executor()\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/executor/uniproc_executor.py\", line 48, in _init_executor\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.driver_worker.load_model()\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 289, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3581, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.model = model_loader.load_model(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 49, in load_model\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     model = initialize_model(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m             ^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", line 48, in initialize_model\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 543, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.model = Qwen2Model(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                  ^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 291, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     old_init(self, **kwargs)\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 394, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                                                     ^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 605, in make_layers\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     + [\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m       ^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 606, in <listcomp>\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 396, in <lambda>\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     lambda prefix: decoder_layer_type(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 258, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.self_attn = Qwen2Attention(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 184, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.attn = attn_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/attention/layer.py\", line 230, in __init__\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m     self.impl = impl_cls(\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m                 ^^^^^^^^^\n\u001b[0;36m(EngineCore_DP0 pid=2973601)\u001b[0;0m TypeError: FlashAttentionImpl.__init__() got an unexpected keyword argument 'layer_idx'\n[rank0]:[W122 10:40:21.360922837 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m Traceback (most recent call last):\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/bin/vllm\", line 7, in <module>\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     sys.exit(main())\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m              ^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     args.dispatch_function(args)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 60, in cmd\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     uvloop.run(run_server(args))\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 92, in run\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return runner.run(wrapper())\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/asyncio/runners.py\", line 118, in run\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return self._loop.run_until_complete(task)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/uvloop/__init__.py\", line 48, in wrapper\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return await main\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     async with build_async_engine_client(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 210, in __aenter__\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return await anext(self.gen)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 215, in from_vllm_config\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return cls(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 121, in make_async_mp_client\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     return AsyncMPClient(*client_args)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 820, in __init__\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     super().__init__(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\", line 477, in __init__\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/contextlib.py\", line 144, in __exit__\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     next(self.gen)\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 903, in launch_core_engines\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     wait_for_engine_startup(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m   File \"/mnt/baai_cp_perf/qbw/conda/envs/dschat/lib/python3.11/site-packages/vllm/v1/engine/utils.py\", line 960, in wait_for_engine_startup\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m     raise RuntimeError(\n\u001b[0;36m(APIServer pid=2972482)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "test_error": null,
      "test_stderr": null,
      "test_stdout": null,
      "traceback": null,
      "datasets_tested": null,
      "eval_scores": null,
      "eval_output_dir": null,
      "timing": {
        "server_start_seconds": null,
        "test_execution_seconds": null,
        "total_seconds": 35.58
      },
      "gpu_memory_gb": null,
      "throughput": null
    },
    "soob3123/GrayLine-Qwen3-14B": {
      "model_id": "soob3123/GrayLine-Qwen3-14B",
      "model_name": "soob3123/GrayLine-Qwen3-14B",
      "size_bytes": 14768307200,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:04:58.025087",
      "end_time": "2026-01-22T10:45:47.371717",
      "duration_seconds": 6049.35,
      "vllm_pid": 2787052,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_104156",
      "timing": {
        "server_start_seconds": 70.06,
        "test_execution_seconds": 5965.65,
        "total_seconds": 6049.35
      },
      "gpu_memory_gb": 1021.61,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/Qwen2.5-14B-Instruct-abliterated-v2": {
      "model_id": "huihui-ai/Qwen2.5-14B-Instruct-abliterated-v2",
      "model_name": "huihui-ai/Qwen2.5-14B-Instruct-abliterated-v2",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T10:37:49.026329",
      "end_time": "2026-01-22T11:17:16.753326",
      "duration_seconds": 2367.73,
      "vllm_pid": 2255819,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {
        "mmlu": {
          "score": 0.8077
        },
        "mmlu_pro": {
          "score": 0.6286
        },
        "math_500": {
          "score": 0.8152
        },
        "gsm8k": {
          "score": 0.95
        },
        "gpqa_diamond": {
          "score": 0.46
        }
      },
      "eval_output_dir": "outputs/20260122_103906",
      "timing": {
        "server_start_seconds": 70.19,
        "test_execution_seconds": 2282.82,
        "total_seconds": 2367.73
      },
      "gpu_memory_gb": 1028.06,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "yentinglin/Mistral-Small-24B-Instruct-2501-reasoning": {
      "model_id": "yentinglin/Mistral-Small-24B-Instruct-2501-reasoning",
      "model_name": "yentinglin/Mistral-Small-24B-Instruct-2501-reasoning",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T09:47:00.081999",
      "end_time": "2026-01-22T11:57:39.153948",
      "duration_seconds": 7839.07,
      "vllm_pid": 2156807,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_111843",
      "timing": {
        "server_start_seconds": 85.06,
        "test_execution_seconds": 7740.6,
        "total_seconds": 7839.07
      },
      "gpu_memory_gb": 1028.06,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "netease-youdao/Confucius-o1-14B": {
      "model_id": "netease-youdao/Confucius-o1-14B",
      "model_name": "netease-youdao/Confucius-o1-14B",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T10:40:30.540593",
      "end_time": "2026-01-22T12:05:37.142196",
      "duration_seconds": 5106.6,
      "vllm_pid": 2974349,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_104704",
      "timing": {
        "server_start_seconds": 80.06,
        "test_execution_seconds": 5012.97,
        "total_seconds": 5106.6
      },
      "gpu_memory_gb": 1021.71,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated": {
      "model_id": "huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated",
      "model_name": "huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T10:29:11.584077",
      "end_time": "2026-01-22T12:28:00.818487",
      "duration_seconds": 7129.23,
      "vllm_pid": 2238837,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_115912",
      "timing": {
        "server_start_seconds": 70.09,
        "test_execution_seconds": 7045.43,
        "total_seconds": 7129.23
      },
      "gpu_memory_gb": 1028.05,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated-v2": {
      "model_id": "huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated-v2",
      "model_name": "huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated-v2",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T10:31:07.695191",
      "end_time": "2026-01-22T12:30:06.598848",
      "duration_seconds": 7138.9,
      "vllm_pid": 2242862,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_122925",
      "timing": {
        "server_start_seconds": 75.05,
        "test_execution_seconds": 7043.59,
        "total_seconds": 7138.9
      },
      "gpu_memory_gb": 1028.06,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "zerofata/MS3.2-PaintedFantasy-24B": {
      "model_id": "zerofata/MS3.2-PaintedFantasy-24B",
      "model_name": "zerofata/MS3.2-PaintedFantasy-24B",
      "size_bytes": 23572403200,
      "tp_size": 2,
      "status": "success",
      "start_time": "2026-01-22T11:57:39.206247",
      "end_time": "2026-01-22T12:34:47.306138",
      "duration_seconds": 2228.1,
      "vllm_pid": 2406651,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_123121",
      "timing": {
        "server_start_seconds": 90.06,
        "test_execution_seconds": 2127.54,
        "total_seconds": 2228.1
      },
      "gpu_memory_gb": 1028.04,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese": {
      "model_id": "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese",
      "model_name": "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T10:17:31.370344",
      "end_time": "2026-01-22T12:41:20.275598",
      "duration_seconds": 8628.91,
      "vllm_pid": 2928752,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_120654",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 8545.13,
        "total_seconds": 8628.91
      },
      "gpu_memory_gb": 1021.76,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    },
    "PKU-DS-LAB/FairyR1-14B-Preview": {
      "model_id": "PKU-DS-LAB/FairyR1-14B-Preview",
      "model_name": "PKU-DS-LAB/FairyR1-14B-Preview",
      "size_bytes": 14770033664,
      "tp_size": 1,
      "status": "success",
      "start_time": "2026-01-22T09:40:31.943164",
      "end_time": "2026-01-22T12:45:16.360710",
      "duration_seconds": 11084.42,
      "vllm_pid": 2856877,
      "vllm_version": "0.13.0",
      "vllm_error": null,
      "vllm_stderr": null,
      "test_error": null,
      "test_stderr": "",
      "test_stdout": "TEST_COMPLETED_SUCCESSFULLY",
      "traceback": null,
      "datasets_tested": [
        "mmlu",
        "mmlu_pro",
        "math_500",
        "gsm8k",
        "gpqa_diamond"
      ],
      "eval_scores": {},
      "eval_output_dir": "outputs/20260122_124239",
      "timing": {
        "server_start_seconds": 70.05,
        "test_execution_seconds": 11000.61,
        "total_seconds": 11084.42
      },
      "gpu_memory_gb": 1021.71,
      "throughput": {
        "samples_per_second": null,
        "total_samples": null
      }
    }
  }
}